chatgpt says I should know
https://tutorialsdojo.com/aws-certified-solutions-architect-associate-saa-c03/

AWS_CLI_AUTO_PROMPT=on-partial

    (JMESPath is a query language for JSON)
    
aws s3api list-buckets --query "Buckets[?Name == 'my-exmplae-bucket-ab2']"
aws s3api list-buckets --query "Contents[].Key"
aws s3api list-buckets --ouput yaml
aws s3api delete-bucket my-deleteme-bucket
aws s3api create-bucket --bucker my-new-bucket --create-bucket-configuration="LocationConstraint=ca-central-1"
aws s3api put-object --bucket my-cool-buccket --body customerData.csv --key customers/customerData.csv
aws s3api delete-objects --bucket my-cool-bucket --delte file:///path/to/file_objects.json
aws s3api head-object --bucket <bucket-name> --key <object-key> // returns Metadata such as Content-Type, Content-Length, ETag, and LastModified.
aws s3api put-object \
    --bucket="checksums-examples-ab-2342" \
    --key="myfilesha1.txt"
    --body="myfile. txt"\
    --checksum-algorithm="SHA1" \
    --checksum-sha1="c28ccc2c5e214036806014df9fb43634f3e770b2" # verifies data integrity, hash at client then hash on aws side. Fails if no match
aws s3 sync local_folder/ s3://my-example-bucket-ab2
aws s3 cp myfile.txt s3://my-unique-bucket123 --storage-class STANDARD_IA
aws s3api put-bucket-policy --bucket bucket-policy-example-ab-1234 --policy file://policy.json

## Create bucket
aws s3api create-bucket -- bucket acl-example-ab-5235 -- region us-east-1

## Turn off Block Public Access for ACLs
# a
aws s3api put-public-access-block \
    -- bucket acl-example-ab-5235 \
    -- public-access-block-configuration "BlockPublicAcls=false, IgnorePublicAcls=false, BlockPublicPolicy=true, RestrictPublicBuckets=true"
# b
aws s3api get-public-access-block -- bucket acl-example-ab-5235



#change  bucket ownership controls (acl stuff)
aws s3api put-bucket-ownership-controls
    -- bucket acl-example-ab-5235
    -- ownership-controls="Rules=[{ObjectOwnership=BucketOwnerPreferred}]"

## Change ACLs to allow for a user in another AWS Account
aws s3api put-bucket-acl \
    -- bucket acl-example-ab-5235 \
    -- access-control-policy file:///workspace/AWS-Examples/s3/acls/policy.json

## Other account can now run
 aws cp somefile.txt s3://mybigbucket-123123
 aws ls s3://mybigbucket-123123
 
aws s3 rm s3://mybigbucker-123123/somefile.txt
aws s3 rb s3://mybigbucker-123123
 
 
AWS Cloudformation
    - AWS's IaC tool
    - sets up your AWS resources 
    - does provisioning and configuring those resources (EC2, RDS DB, DynamoDB) for you
    - Quickly replicate your infrastructure
    - Simplify infrastructure management
    - Easily control and track changes to your infrastructure
    - Rollback
        - go to previous infrastructure
    - Drift Detection
        - track unintended changes to AWS resources that were manually modified outside of CloudFormation.
        - compares current AWS resources with the configuration in the CloudFormation template.
            - different resources are marked "Drifted". (eg, security group rules changed manually)
    - Stack Management:
        Stacks  
            - Groups of AWS resources (kinda like .tfstate file but not).
            - Create Stack, Update Stack, Delete Stack
        StackSets
            - Deploy stacks across multiple AWS accounts and regions.
        ChangeSet
            - $ terraform plan
            - preview what will change in your stack before applying updates.
            - deletes or replaces any critical resources
            - you decide when to execute the change set
            - Change sets don't indicate whether CloudFormation will successfully update a stack. For example
        
    How AWS CloudFormation Works
        1 Create a Template
            - json or yaml
        2 Deploy the Stack
            - console, CLI, or SDK to create a stack.
        3. Update the Stack
            - Modify the template and update the stack with a change set.
            
    Note
        - required flag w/ 3 options:
        --capabilities <CAPABILITY_IAM | CAPABILITY_NAMED_IAM | CAPABILITY_AUTO_EXPAND>
        CAPABILITY_IAM
            - when you create IAM roles BUT you do not give them a name
                - sometimes it's easier for AWS to name it (auto prevent same name collision)
            - AWS auto names IAM roles (if you don’t specify RoleName)
            - whenever you have `AWS::IAM::Role` in your template
            - required if the role has a policy or permissions. 
                - not required if the role is defined but doesn’t include a policy.
            - will automatically create and modify IAM roles, policies, and users specified in your template.
            - you (running the command) must have permisisons prior like: iam:CreateRole, iam:AttachRolePolicy, and iam:PassRole
        CAPABILITY_NAMED_IAM
            - when you create IAM roles AND you name them manually
            - whenever you have AWS::IAM::Role BUT ALSO a property like "RoleName": "MyCustomRole"
                - or if the stack tries to modify an existing named IAM role
        CAPABILITY_AUTO_EXPAND
            - macro-based templates 
            - ie AWS::CloudFormation::Macro, (AWS CloudFormation macros)
                - AWS CloudFormation Macros allow you to dynamically transform your template before deployment.
            - sometimes requried if you're using another template that relies on one
            - advanced, high skill stuff

    Terraform & CloudFormation:
        State File 
            Terraform: Local or remote (terraform.tfstate)	
            CF: Managed within AWS (no external state file), but close to `--stack-name <MY_WHATEVER_IDENDIFIER>`
        Drift Detection	
            Terraform: Requires terraform refresh	
            CF: Built-in drift detection
        Change Previews	
            Terraform: terraform plan	
            CF: CloudFormation Change Sets
        State Locking	
            Terraform: Uses Terraform backend (e.g., S3 + DynamoDB)	
            CF: AWS manages state automatically

                                               |   Like a reference to whatever in aws 
                                               |       manages the .tfstate file 
                                               |       Arbitrary name
                                              \|/
                                               V                                               
aws cloudformation create-stack --stack-name my-stack --template-body file://template.yaml
aws cloudformation delete-stack --stack-name my-stack  # Deletes all resources
aws cloudformation delete-stack --stack-name my-stack
aws cloudformation create-stack --stack-name my-NEW-stack --template-body file://NEW_template2.yaml
# get all stacks
aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE                                               |  
# View changeSet/terraform plan                                                                                    V
aws cloudformation create-change-set --stack-name my-stack --template-body file://template.yaml --change-set-name add-s3
aws cloudformation describe-change-set --stack-name my-stack --change-set-name add-s3




aws cloudformation deploy \ 
     --template-file template.yaml
     --no-execute-changeset 
     --region us-west-2
     --stack-name sfn-s3-simple
 #no-execute-changeset = you must confirm it in aws console
 #stack-name = you can see this 'stack' in aws console under CloudFormation
 
aws cloudformation delete-stack --stack-name my-cool-stack --region us-west-2

Cloud Development Kit
$ cdk boostrap # runs cdk-stacks.ts, deploys to cloudformation
$ cdk destroy # delets stacks

s3api = low level, full api
s3 = high level, less

IaC tools
    Terraform
    Cloudformation
    Pulumi
    aws sdk/cdk in java, javascript, ruby, python, ect

############################
############################
############################
###                      ###
###         S3           ###
###                      ###
############################
############################
############################

1. Bucket Types
2. Object & metadata
3. Object lock
4. Bucket URI
5. CLI
6. Request Styles
7. DualStack  
8. Storage Classes
    - Standard
    - Intelligent Tiering
       -  Frequent access
       - infrequent
       - archive 
       - archive instant
       - deep archive
    - Infrequent Access
    - One-Zone
    - One-Zone-IA
    - One-Zone Express
    - Glacier Instant Retrieval
    - glacier flexible
        - expident
        - standard
        - bulk
    - glacier deep archive
9. Encryption
10. Security
11. ACLS
12. Access Grants
13. IAM Access Analyzer for S3
14. Private Link
15. Data Consistency
16. Object Replication
17. Versioning
18. Lifecycle
19. Transfer Acceleration
20. Presigned URLs
21. Access points
22. Multi Region Access points
23. Object Lambda Access point
24. Mountpoint for s3
25. Requester Pays
26. Batch Operation
27. Inventory
28. Select
29. Event Notification
30. Storage Class Analysis
31. Storage Lense
32. Static Website Hosting
33. Multipart Uploads
34. Byte-Range Fetches
35. Interoperability



S3 Bucket Types
    General Purpose bucket
        - original bucket
        - recommended for most castes
        - ** flat hierachy **
        - max 100 buckets / account

    Directory Buckets
        - new type
        - ** folder hierarchy **
        - only to be used with s3 express one zone storage
        - max 10 buckets / account
        Use case:
            - single digit milliseconds for PUT or GET
        
S3 Objects
    Etags 
        - kinda like a hash but not (b/c multi part uploads)
        - a way to detech when the object has changed (w/o downloading)
        - reflects content changes, NOT metadata
        - respresents a specific version
        - (also used for caching systems)
    Checksum
        - optional and explict hash
        - recomputed and validated 
        - "a hardcore etag"
        - strong to verify integrety
        - preferable way to verify data integrity if in-transit data is lost
    Prefix
        - in object key, it's everything before the filename, "/assets/images/" <-- prefix
        - NOT FOLDERS
        - Standard = FLAT
        - 1024 max characters
    Metadata (general)
        - all metadata be part of the S3 Object will be in the headers when you http GET it.
        - "should provide info about the cloud resoruces not the contents of the object" (?)
        System-defined: 
            - a set from AWS, eg Content-Type, Content-Length, Cache-Control, Expires, Last-Modified, Etags, ect
            - some could be modified
        User-defined: 
            x-amz-meta-* eg (x-amz-meta-version = 0.2)
        
    Note:
        WORM
            - Write Once Read Many (WORM)
            - a storage compliance feature that makes data immutable
            - once written, the file can never be modified or delted
            - CD, DVD
            Use Case:
                - healthcare or financial, where files need to be audited
        ROM 
            - Read Only Memory (ROM)
            - data is permanently written during manufacturing and is read-only during regular operation.
            - to store firmware that doesn’t need to change (eg BIOS in computers).
            - Video game cartridges (Zelda Ocarina of Time) are ROM but include additional stuff to save games (Battery-backed SRAM, EEPROM or Flash memory)
            
    S3 Object Locks
        - WORM
        - use on the bucket or object-level
        - it can be enabled when creating a new bucket or on existing buckets. 
        - you must first enable versioning for the bucket, as you won’t be able to turn versioning on later
        
        - protects only the version specified in the request. 
            -> It doesn't prevent new versions of the object from being created.

        - if bucket has 1 year lock, you put object at 11th month, you still wait full year to modify object.
        - fixed or indefinetly time
        - must use CLI for individual obect locks
        - GOVERNANCE or COMPLIANCE (governmance = special IAM can delete, compliance = no one)
            aws s3api put-object \
                -- bucket your-bucket-name
                -- key your-object-key \
                -- body file-to-upload \
                -- object-lock-mode GOVERNANCE \
                -- object-lock-retain-until-date "2025-01-01T00:00:00Z"

    S3 Bucket URI
        - s3://mycoolbucket/photo.jpg
        - Not a valid web-accessible URL
        - it's used by AWS CLI, SDKs, internal references
        this is prob valid: 
            https://mycoolbucket.s3.amazonaws.com/photo.jpg
            https://mycoolbucket.s3.us-east-1.amazonaws.com/photo.jpg

    S3 CLI
        $ aws s3
            - A high-level way to interact with S3 buckets and objects
        $ aws s3api
            - A low level way to interact with S3 buckets and objects
        $ aws s3control
            - Managing s3 access points, S3 outposts buckets, S3 batch operations, storage lens.
        $ aws s3outposts
            - Manage endpoints for S3 outposts

    S3 Request Styles
        Virtual Hosted style
            - Prefered, new
            - bucket name becomes part of the domain name.
            https://<bucket-name>.s3.<region>.amazonaws.com/<object-key> ------> https://mycoolbucket.s3.us-west-2.amazonaws.com/photo.jpg
         Path-Style URLs 
            ❌ Legacy / Deprecated
            - doest work in new regions
            - bucket name  is part of path
            https://s3.<region>.amazonaws.com/<bucket-name>/<object-key> ---> https://s3.us-west-2.amazonaws.com/mycoolbucket/photo.jpg
        - both have region in the sub-domain.

    S3 DualStack
                        |
                        V
        - https://s3.dualstack.us-west-2.amazonaws.com/photos/dog.jpg
        - IPv6
            - DualStack endpoint handles IPv6
        - https://s3.us-west-2.amazonaws.com/photos/dog.jpg
        - IPv4
            - Standard endpoint handles only IPv4 traffic
        - You dont enable or configure anything, its on there by default if you need it for w/e reason.


################################################
##########     storage classes     #############
################################################
    S3 Storage Classes
        - Buckets can contain objects of mixed storage classes.
        - S3 doesn’t support setting a default storage class at the bucket level.
            - except fot intelligent tiering
        - Use lifecycle policies or control it per-upload.
        - you can change the storage class of any object in your S3 bucket
            - (excluding locked-in/compliance, ect)
        - Options that effect:
            Retrieval Time
            Accessibility
            Durability 
            Price

    S3 Reduced Redundancy Storage (RRS) 
        - DEAD
        - legacy storage class
        - Sucks, dont use
        
    1)
    Standard (default) 
        Durabilyt:    Very High - 11 9s 99.999999999
        Availabiilty: Very High -  4 9s 99.99%
        Redundancy: 3 AZs, data is stored
        Retrieval: miliseconds (low latency, fast)
        Pricing:
            Storage / GB
            Per request
            0.0x / GB (many conditions effect it but 0.01-0.09 $)
        Use Case:
            - General. Does everything well
        
        Overview:
            - Fast, Available and Durable.
            - High throughput: optimized for frequent access
        

    2)
    Standard-IA (Infrequent Access) 
        Durability:    Very  High - 11 9s 
        Availability:  Med  High - 3 9s -> 99.9%
        Redundancy: 3 AZs, data is stored
        Retrieval:  miliseconds (low latency, fast)
        
        Pricing:
            Storage / GB ~= 0.5x Standard  = half as much as Standard
            Per request ~=  2x Standard    = double as much as Standard
            * Charged per GB ($ 0.01 / GB, yet Standard is free)
            * has a min storage duration charge of 30 days)
            
        Use Case:
            - Data that is accessed less often than standard
                but still needs speed when needed. (disaster recovery, logs, archives.)
        Overview:
            - Cheaper if you access less than once a month.
            - Extra fee to retrieve. 
            - 30 day min charge
            - 50% less $ than Standard (reduced availability)        
            - fast
            - High throughput: but less than Standard (b/c less avail.)

    3)
    S3 One-Zone-IA        
        Durability:  Very High - 11 9s
        Availablity  Low High - 2.5 9s -> 99.5%
        Redundancy: 1 AZ
        Retrieval: miliseconds (fast)
        
        Pricing
            Storage / GB ~= 0.5x = half as Standard
            Per request ~= 2x = double as Standard 
            * Charged per GB ($ 0.01 / GB)
            * has a min storage duration charge of 30 days
        Use Case:
                Non-critical, reproducible,
        Overview:
            - for data that is less frequtnly accessed 
            - 20% less than Infrequent Access = 60% less than stardard
            - Everything is the same as Infrequent Access (IA) but 1 AZ vs 3 AZ
            - 30 day min charge

    4)
    S3 Intelligent Tiering 
        Durability: 11-9s, 99.999999999%
        Availability: 99.9%
        Latency: Milliseconds
        Tiers:
            Frequent Access 
                - active items
            Infrequent Access
                - 30 days
            Archive Instant Access
                - 90 days
            Archive Access (optional)
                - 90 days, 3-5 hours data retrieval
            Deep Archive Access (optional)
                - 180 days, ~12 hours data retrieval
        
        Overview:
            - uses ML to analyze object usage and determine storage class. 
            - Automatically moves data to the most cost-effective tier based on usage
            
            - Extra fee to analyze
                $0.0025 per 1,000 objects per month + storage
            - Objects under 128 KB are not eligible for tiering
            - 30 day min charge
            - Only Int teiring can move storage classes by access time
                -  b/c bucket Lifecycle policies are based on object age, not access frequency. 
        Use Case: 
            - Unknown or changing access patterns        
        how to use Int Tiering:
            - Option 1: At Upload Time
            - Option 2: bucket policy - set S3 lifecycle configs to auto transition objects to Intelligent-Tiering 
            - Option 3: as bucket default storage (2023 new!)
    5)
    S3 Express One-Zone 
        Pricing: 
            Storage = $$$ = Expensive = $0.16 / GB
            Per Reqeust = 0.5x = half as Standard
            Per GB Transfer = 0.5x = half as "
                - 512kb min charged
            - ??? its wordy and confusing
        Use Case: 
            Real-time analytics, ML training, high-performance computing (HPC), temp files, scratch data
            
        Overview:
            - fastest! ZOOOM 10x faster than s3
            - single-digit ms performance
            - one AZ
            - S3 Directory Bucket
            - 50% less than Standard cost
            - Durability: 1 AZ
            - applies a flat per request charge for request sizes up to 512kb
        
    6)
    S3 Glacier Instand Retrieval
        Durability: 11 9s
        Availabily: 3 9s
        Redundancy: 3 Az
        Retrieval: milisceonds (fast)
        
        Pricing:
            Storage / GB ~= 68% lower than Standard IA
            Per Request~= 4x double as Standard
            * Charged per GB ($ 0.03 / GB)
        Overview:
            - For long-term cold storage. Get data instantly
            - for rarely accessed data but is needed immediate when called
            - long storage + performance
            
    7)
    S3 Glacier Flexible Retrieval
        Durability: 11 9s
        Availabily: 3 9s
        Redundancy: 3 Az
        Retrieval: mins-hours
        
        Three retrieval tiers:
            Expediant Tier 
                - 1-5 min. 
                - Limited to 250 MB (least cheap)
            Standard Tier 
                - 3-5 hours. 
                - No limit. (default)
            Bulk Tier 
                - 5-12 hours. 
                - No limit, even petabytes (cheapest)
        Pricing:
            Storage / GB ~= variable, but close to 68%
            Per Request~= 0.2
            * Charged per GB ($ 0.03 / GB)
            - charged for a minimum storage duration of 90 days,
        Overview:
            - takes minutes to hours get data (Standard, Expediated, Bulk Retrieval)
            - formerly "S3 Glacier"
        Trick: You should store few large files, instead of many small.
            - b/c archived objects will have +40KBs of metadata attached (can add up)

    8)
    S3 Glacier Deep Archive 
        Durability: 11 9s
        Availabily: 3 9s
        Redundancy: 3 Az
        Retrieval: hours
        - SAME 
        - lowest cost 
        - Data retrieval time is 12 hours.
        - Is nearly the exact same ast S3 Glacier Flexibl 
            - but NO Expediate Tier
            - but Storage / GB is cheaper -> 1/3rd of Flexible Ret. 
            - but Retrieval is more expensive
        -  has a minimum storage duration of  180 days.

    NOTE
        Glacier Vault = legacy, separate service (it's in older setups, some using it via direct Glacier APIs).
    NOTE 2
        Archived Objects = objects stored at a reduced cost (Glacier, ect)
                        - cannot be accessed in real-time, you wait 


S3 Encryption
    - by default, all data is encrypted
    (SSE = Server Side Encryption)
    SSE-S3
        - default (even when not specified)
        - AWS does the encryption
        $ Free
    SSE-KMS
        - you choose KMS key
            - (keys can be auto rotated)
            - regulatory compliance
            -  keys => money $
        - Either supply KMS key with each object upload
            OR
        - S3 Bucket Keys = set a KMS key at the bucket level to encrypt all
            -> Save money with Bucket Keys
        $ 0.03 / 10,000 enc-dec requests
        $ 1 Key = $1 / month
            
    SSE-C (Customer keys)
        - you give the key to AWS
            - they encrypt
            - they dont save key
        - You PROVIDE the key when uploading or getting a object.
            - if you lose key, gg
        - OFFLOAD encryption/decryption compute to AWS
        $ Free
        
    DSSE-KMS
        Dual-layer (SSE)
        - data is twice encrypted 
            - (client & AWS)
        1. get a KMS key (safe "child" key) -> encrypt -> toss that key
        2. upload -> aws encrypts again
        - ^ decrypt=reverse
        - ^ decrypt=reverse
    
    Client-Side Encryption
        - not a s3 option
        - you encrypt your data (client side)
        - outside AWS

S3 Security:
    - policies (json) that defines who can access what
    Control user permissions with:
        - IAM Policy 
            - provide permissions to multiple buckets (and resources in general)
            - the principle (who) is the entity the IAM policy is attached to
        - Bucket Policy
            - a Resource Policy
            - access to this specific bucket
            - can name multiple principals
        - ACLs (Legacy)
            - Access Control Lists
        
s3 ACLs
    Features of S3 ACLs:
    1. Allow/Deny
    2. Grantee Types:
        - Specific AWS accounts.
        - Predefined groups (eg "Everyone" "Group" "Authenticated Users").  
            - Authenticated Users = any AWS account whether they are part of your AWS account or not
    3. Permissions:
        - READ
        - WRITE
        - READ_ACP: Permission to read the ACL itself.
        - WRITE_ACP: Permission to modify the ACL.
        - FULL_CONTROL
    - An S3 bucket can have both ACLs and bucket policies at the same time. 
        - They are separate access control mechanisms. 
        - Permissions are evaluated together
        - if either allows it, then access granted 
        - (unless blocked by the Public Access Settings we discussed earlier).
    - bucket policy applies to the whole bucket
    - ACLs are at both the object level AND bucket level

S3 Access Grants 
    - [directory, s3, grants, permissions, large-scale?]
    - control data access across multiple accounts
    - "map identities in directories to datasets in S3"
        - directores =  Active Directory, or IAM Principals, 
    - instead of managing permissions to resources (S3, DynamoDB) in each account
        - you define grants
            grants = Who can access what under what conditions
    - you Register resources (S3s & key-path).
    - you choose the Grantee (IAM user or group)
    - you define Access Level and Conditions


    
S3 IAM access Analyzer for S3
    - [alert, exposed, s3]
    - alert you when your s3 buck are public & exposed to internet or cross-acounts aws 
    - free
    - reports for you


S3 Data Consistency
    - after putting new data:
    - data kept across 2+ servers might not be in perfect sync
    - S3 is STRONGLY consistent
    Strongly Consistent
        - IMMEDIATELY, data will be same/consistent (Andrew Brown say sub 1 sec ?)
        - all read/write/delete
    Eventually Consistent
        - not same instantly
        - <2 sec?

S3 Object Replication 
    - copy objects in two buckets
        - automatically replicates new objects 
        - does not retroactively replicate existing objects
    Replication Rule:
        - Are defined at the bucket level (not object level)
            - objects replicated are based on prefixes or object tags
            -> prefixes or object tags
    - Replicate objects to a different:
        - storage class
        - region
        - account and transfer ownership
    - Replicate while retaining object metadata
    
    Advantages:
        Data protection 
            - from accidents or attacks
        Data sovereignty requirements
            - multiple copies
        Compliance 
        Latency
            - closer to user
            - closer to compute

    - metrics (3) of the replication operation are visible in cloudwatch and S3 management console
    
    
    
    1. Same-Region Replication (SRR)
        - replication buckets in same region
        - us-east -> us-east
    2. Cross-Region Replication (CRR)
        - buckets different region
        - us-east -> EU-west
    3. Multi-destination replication
        - Replicate to multiple AWS Regions
        - eu-west <--- us-east ---> ap-asia
    4. Two-way replication
        - Sync replica and metadata changes
        - us-east <---> ap-asia
    5. S3 Replication Time Control (S3 RTC)
        - Ensures that 99.99% of new objects are replicated within 15 minutes.
        - Applies to: Cross-Region Replication (CRR) or Same-Region Replication (SRR).
        - Predictable replication backed by SLA
    
        
    
S3 Versioning
    - store multiple version of S3 objects
    - recover from accidents
    - stores all versions at the same key-path/adrress
    - once enabled cannot turn off
        -> BUT can be "Suspended"
    NOTE
        - MFA Delete: enable this, requires MFA to delete operation
    Bucket states:
    1. Unversioned (default)
    2. Versioned
    3. Versioned-suspended
    
    
S3 Lifecycle Policies
    - automate Storage Type and deletion of objects
    Rules
        = set of Filtes & Actions
    Filter
        = prefix OR tag OR Size (min/max GB)
    Actions
        = to do after x days.
        Action types
        1. Transition:
            - Storage Class (Standard -> Glacier)
            - Move current version or previous object version (ie Non-Concurrent)
        2. Expiriation:
            - Delete
    
    
S3 Transfer Acceleration
    [FAST, upload/download, special endpoint, 1 bucket]
    - FAST upload/downloads
        - 50% to 500%
    - you get a special endpoint:
        eg) 
            https://bucketname.S3-ACCELERATE.amazonaws.com
    - bucket level feature (not indivual objects)
    - Uses CloudFront's global edge location
        - then to AWS's "private backbone network"
        - instead of directly to S3
    Use case:
        - speed (centralized bucket)
        - dont want to use all your bandwidth 
        
    1. Turn on Transfer Accelerate (prob CLI)
    2. Upload 
        $ aws  s3  cp  ./file.jpg   s3://my-bucket/file.jpg   --endpoint-url  https://s3-accelerate.amazonaws.com

AWS S3 Presigned URLs
    [temp-url, temp-access, upload/download, expires, s3]
    - a temporary URL to upload/download from S3
    - give URL to someone WITHOUT you AWS credentials
    
    - Generated by AWS user 
    - expiration (1 hour)
    - GET, PUT, ect
    
    - You can use the same URL multiple times, until expiration
    Use Case:
        Uploading:
            Why not upload to server then upload to S3?
                - Scalability
                    - Offload Bandwidth & CPU from your server to the client
        Downloading:
            - keep bucket private
            - no aws credentials exposed
        Not use case:
            - You need to validate/scan the file 
        
        
S3 Access Points
        [magic-url, s3, user groups, permissions]
        - manage user permissions to data in S3 buckets.
        - is a "named network endpoint" attached to a S3 bucket.
        - access points have own IAM permissions, and VPC configuration.
        Problem solver:
            - bucket policies is hard when many users/apps
                -> Giant policies
        - 1st Access Point for app A @ /app-a-data/
        - 2nd Access Point for app B @ /app2-b-data/
        Features!
            Example 
                Access Point endpoint = https://my-access-point-123456789012.s3-accesspoint.us-east-1.amazonaws.com
                it points to --> mybucket-example
                policies restrict so only acces --> s3://mybucket-example/app-a-data/*                |
                                                                                                      |
                You Download:                                                                         V
                    GET https://my-access-point-123456789012.s3-accesspoint.us-east-1.amazonaws.com/app-a-data/file.txt
                Your Upload:aws 
                    $ s3api put-object 
                          --bucket  arn:aws:s3:us-east-1:123456789012:accesspoint/my-access-point 
                          --key     app-a-data/myfile.txt 
                          --body    ./localfile.txt
            - optionally: make only resources in your VPC can access the s3 (private networking)
            - can keep the same Endpoint and change the prefix:
                From app-a-data/ → to app-a-new-2nd-data/

        Note:
            - it's just an organization feature
            - it's possible to enforce this all with IAM policies to scope object keys/paths right 
        
S3 Multi-Region Access Point
    [optimal region, s3, download/upload, 1-url to rule them all, many-buckets]
    Problem:
        - You have 5 buckets around the world, each have data replication on
        - How to download/upload to cloest bucket?
        ---> Multi-Region Access Point
    - a magical global URL that auto goes to closest/fastest bucket
    - Automaticaly resolves failover, ie if a region/bucket is down
    How to:
        1. Few S3 buckets globally.
        2. Enable data replication (same data across all).
        3. ✔️ Create the endpoint
            https://my-thingy-endpoint.mrap.accesspoint.s3-global.amazonaws.com
    - Similar to Transfer Acceleration but TA is for 1 bucket, and $
        
S3 Object Lambda Access Point
    [lambda, s3, intercept, transform, request]
    - get objects and do custom transformations on the fly. 
    - Instead of returning the raw object:
        - you call Object via Lambda Access Point
        - Object goes from S3 -> Lambda -> (transforms) -> OG Requester (you)
        - original object is unmodified.
    Uses:
        Redact PII (and detect with Amazon Comprehend)
        Resize images
        Add watermark
        Change file format	Convert JSON to CSV on the fly
    eg)
        https://my-object-lambda-ap-whatever-bro . s3-object-lambda . us-east-1 . amazonaws . com /logs/file.json

Mountpoint for S3
    [linux CLI, mount, api-translate, s3]
    - "mount" your S3 bucket as a local file system.
    - app can accesses objects in S3
    - translates linux operations into S3 API calls
        ls, cp, mv, and cat
    - It's an open source thing you install on your Linux OS
        - Installable: EC2, On-premises servers.
        - NOT Lambda

    - Can do something but not all
        - YES list, read, 
        - NO modify, delete directories, symbolic links.

    Use cases   
        - Large-scale media processing
        - reading huge files
        - Machine learning pipelines
            - data science/ML and want to skip writing code for S3 APIs—just cat your training data directly.
        
S3 Requester Pays
    [offload-costs, no-checkout, api-driven, s3]
    - S3 bucket ownder offsets the transfer cost to another AWS account
    - Request pays for bandwidth
    - Ownder pays for storage
    Use Case: 
        - Make large datasets available 
            - eg, satellite imagery, genomics, weather data
            - Governments, research institutions, universities.
        - team Collabs
    How to:
        - Owner toggles "Requester Pays" at bucket level (not object)
        - Owner IAM uses "s3:RequestPayer: requester" Condition
        - Owner IAM uses (optionally) certain AWS account access
        - Sender make http request header ="x-amz-request-payer: requester"
          OR
          $ aws s3 cp s3://bucket-name/object-key ./local-file --request-payer requester
    Note:
        - There’s no "purchase agreement" process - you’re just billed like with any AWS usage.
        

        
Amazon S3 Batch Operations 
    [billions, batch, simple-job, s3]
    - perform large-scale operations on millions or billions of s3 objects
    - you provide a CSV or a "S3 Inventory rerpot via manifest.json"
    - Operations:
        - Copy objects between buckets
        - Replace tags
        - Modify access control lists (ACLs).
        - Invoke Lambda functions
        - more
    - NO CLI just clicking UI
    
Amazon S3 Inventory
    [1-bucket-overview, metadata, report, s3, CSV]
    - a report of your objects metadata
        - can prefix filter 
        - daily, weekly, ect
    For: 
        - compliance and regulatory
        - business stuff
    Metadata:
        - Object Key (name)
        - Last Modified Date
        - Size
        - Storage Class (Standard, Glacier)
        - ETag
        - IsMultipartUploaded
        - Server-Side Encryption
        - IsObjectLocked
        - Replication Status 
        - Object Versioning data
        
Amazon S3 Select 
    [deprecated, SQL query, s3]
    !!! is no longer available to new customers. 
    - Existing customers of Amazon S3 Select can continue to use the feature as usual
    
    - query data via SQL expressions.
        
S3 Event Notification
    [s3, events, sqs/sns/lambda trigger, update/delete/create/restore/ect]
    - receive notifications when certain events happen in your S3 bucket.
    - event notifications are delivered in seconds but can sometimes take a minute or longer.
        
    events:
        - New object created
        - Object removal
        - Restore object
        - Reduced Redundancy Storage (RRS) object lost 
        - Replication 
        - S3 Lifecycle expiration 
        - S3 Lifecycle transition 
        - S3 Intelligent-Tiering automatic archival 
        - Object tagging 
        - Object ACL PUT 
    notifications reciever:
        SNS topic
        SQS queue
        EventBridge
        AWS Lambda function 
    How to:
        - AWS Console → S3 → Select your bucket.
        -> "Event notifications" 
        -> "Create event notification"



S3 Storage Class Analysis
    [access-patterns, storage class, recommendations]
    - view access patterns in your S3 buckets
    - helps find right storage class
    - recommend object classes
    - optimize costs
    - output is a CSV report:
        Date
        Preflix
        StorageClass
        AccessCount
        TransitionCandidateFlag

S3 Storage Lens
    [account-wide, s3, metric-overview, dashboard, free/premium]
    - Storage metrics/analytics with optimzation recommendations 
        - lots of metrics
    - ALL S3 buckets account wide
    - " organization-wide visibility"
    - recommends cost optimization
    - free, but premium tier too
    - pretty UI Dashboard
        - CSV too
    
    
S3 Static Website Hosting
    - host a static website from an S3 bucket.
    - static = HTML, CSS, JavaScript, and assets 
    - no server-side processing.
    - http ONLY
        - must use cloudfront for https
    how to:
        - upload your stuff
        - enable it
    For w/e reason website looks like either (dot or dash . or -)
                                       |
                                       V
        A)http://bucket-name.s3-website-region.amazonaws.com
        B)http://bucket-name.s3-website.region.amazonaws.com
    
    
S3 Multipart Upload
    - upload large files in chunks
    - required for 100MB+ files
    Features
        - resume from broken uploads
            - upload the missing parts
            - can resume at any time (no expiry)
        - fast b/c parallel
    - easy with SDK
    - low level API is complex
        $ s3api create-multipart-upload --bucket DOC-EXAMPLE-BUCKET --key large_test_file 
         ...
         
         
S3 Byte-Range Fetches
    - request a start-end point of bytes 
    - Use the "Range" HTTP header
    - You need to manually do it
        - store chunks
        - concat them together
    
    Tip:
        - use concurrent connections to fetch different byte ranges for min/max fast
        - 8 MB or 16 MB, typically.
    

S3 Interoperability  
    [other cloud providers storage, s3]
    - Other cloud providers can easily use the S3 API.
    - Many cloud storage solutions (like MinIO, Wasabi, Backblaze B2, Ceph, etc.) offer S3-compatible APIs.
        => They mimic the S3 API behavior.
    - Apps that use S3 can change to those cloud services without changes to the codebase (or minimal changes).
        - b/c how they made their API

        
Random bucket shit:
    BlockPublicAcls	
        false	
        -> Allows objects/bucket to have public ACLs. You can set public permissions via ACL.
            [true
            -> Blocks new public ACLs on buckets or objects.]
    IgnorePublicAcls	
        false	
        - ACLs will not be ignored — public ACLs will be honored.
            [true
            -> Ignores all ACLs, public or private.]
    BlockPublicPolicy	
        true	
        - Prevents applying any bucket policy that would make the bucket public. Even if you try to apply one, it will be rejected.
    RestrictPublicBuckets	
        true
        - the bucket only be accessed by authenticated users using IAM.
        - Even if a bucket policy allows public access, only AWS account principals can access the bucket. 
        - Blocks anonymous access.
        
    --> this is enabling ACLs, but keeps the bucket from being fully public by restricting public policies and general bucket exposure.
        Setting	                Affects	        What it blocks
        ----------------------------------------------------------
        BlockPublicAcls	        ACLs	        Setting public ACLs
        IgnorePublicAcls	    ACLs	        All ACLs (even private) — ACLs ignored
        BlockPublicPolicy	    Bucket Policy	Attaching public bucket policies
        RestrictPublicBuckets	Bucket Policy	Anonymous access via policy (forces IAM only)

    -  ACLs and bucket policies can grant access to objects or buckets — including public access — but they work differently.
    a.
    - To "Block Public ACLs" on an S3 bucket, you use the Public Access Block settings feature. 
        - This setting prevents users from making objects or buckets public using ACLs.
    If someone tries this:
        $ aws s3api put-object-acl --bucket your-bucket-name --key somefile.txt --acl public-read
    It will fail with an error like:
        "An error occurred (AccessDenied) when calling the PutObjectAcl operation: Public access is blocked for this bucket"
    - IgnorePublicAcls=true: S3 will disregard all public (and private) ACLs.
    - BlockPublicAcls=false: S3 will still allow setting ACLs — it just won’t honor them.
    -> nice combo... syntax
    b.
    - To "Block Public Policy" on an S3 bucket means:
        - You are preventing the bucket from being made public via a bucket policy, even if someone tries to write one
    - ignore does same thing
    
    
###########################################
###########################################
#####                              ########
#####          CLI STUFF           ########
#####                              ########
###########################################
###########################################

RECALL:
    APIs
        - Application Programming Interface
        - two apps/services can talk to each other
        - usually HTTP
    - For AWS these are all using the AWS API:
        - AWS Management Console
        - AWS SDK
        - AWS CLI
    Terminal
        - text only interface
    Console
        - physical computer to interact w/ terminal
    Shell
        - a CLI program where users input commands

Tip:
    AWS CLI 
        - When interacting with AWS API, networking issues will occur
        - It's recommended to use exponential backoff before trying again
        - Try again in 1,2,4,8,16,32 econds
        - most SDKs have expo backoff built in
        
    
AWS Access Keys 
    - credentials used to authenticate and authorize programmatic access to AWS services. 
    - can have 2 active Access Keys per account
        - TOML
    = aka AWS Credentials
    Two parts:
        1. Access Key ID 
            – a public identifier.
            - not sensitive by itself (but dont share)
        2. Secret Access Key 
            – a private key, similar to a password.
            
    Profile
        - a named set of credentials
        - "default" is used when no other profile is explicitly given
        - ~/.aws/credentials
            
            [default]
            aws_access_key_id = AKIA...
            aws_secret_access_key = ...

            [dev]
            aws_access_key_id = AKIA...
            aws_secret_access_key = ...
            region = us-west-2
    
        - dev profile
            $ aws s3 ls --profile dev
            OR
            $ export AWS_PROFILE=dev
            
        - use the CLI to configure your profile
            $ aws configure
            <Enter Access Key ID>
            <Enter Secret>
            <Enter Name>
            <Enter Region>

    
AWS Config File
    - stores generic configurations 
    - Doesnt not have the super-sensitive access_key or secret_access_key
        - but you can do some weird mix and matching?
    eg)
        ~/.aws/config
        
        [default]
            region=us-west-2
            output=json
        [profile development]
        s3 =
            max_concurrent_requests = 20
            max_queue_size = 10000
            multipart_threshold = 64MB
            multipart_chunksize = 16MB
            max_bandwidth = 50MB/s
            use_accelerate_endpoint = true
            addressing_style = path

AWS CLI Environment Variables
    - You can set Configs and Credentials
    Priority:
        Most
            1. CLI Paramets
            2. Enviornment Variables
            3. Configuration Files
        Least
    Common env vars:
       aws_cli_auto_prompt <---- Nice
       aws_access_key_id
       aws_secret_access_key
       aws_default_region
       aws_region
       aws_profile
       aws_confif_file (has path)
       aws_session_token
       aws_role_arn

    Auto-completion Options
        - provides autocomplete dropdown like intellisense
            - "interactive shell" built into aws
            - helps assist writing CLI
        - suggests commands
        - suggests sub-commands
        - suggests flags
        - suggests parameters (shows required flags)
        - suggests YOUR resources 
            eg) Blog-nwqueasdf5frq
            eg) example-my-app-ddecdafi89u
        - suggests from ~/.aws/credentials
        - fuzzy search
        
        Turn on via
            $ aws --cli-auto-prompt
            $ export AWS_CLI_AUTO_PROMPT=on-partial  <--- recommended
            ~/.aws/config -> cli_auto_prompt = on
            
        
    Documentation Panel
        - Press F3 -> doc pane opens
        

AWS Security Token Service (STS)
    - you request temporary credentials from a IAM users 
    - STS returns:
        AccessKeyID
        SecretAccessKey
        SessionToken
        Expiration
    - STS returns a user's quasi-credentials, then login with those credentials
    - fun fact: is a global service at https://sts.amazonaws.com
    - can get credential from IdP, SAML, OpenID Connect, or a custom identity broker

AWS Signing API Requests
    - adding authentication info to HTTP requests sent to AWS services. 
    -> This is required 
    - additional authentication information on your API requests. 
    - helps verify the identity and authenticity
    - automatically via CLI & SDKs
    Required credentials:
        Access Key ID
        Secret Access Key
        -> hashes: HMAC-SHA256.
    - "x-amz-signature" header
         Signature Version 4 (SigV4).
    
AWS IP Address Ranges
    - AWS Publishes all its IP ranges
    - if you want whitelist, blacklist, ect
    https://ip-ranges.amazonaws.com/ip-ranges.json

Service Endpoint
    - AWS services have a pattern:
        protocol://service-code.region-code.amazonaws.com
    - often "https://"
    

AWS CLI Input Flags
    - use a yaml/json file to populate CLI commands 
        --cli-input-json
        --cli-input-yaml
    eg)
        $ aws ec2 run-instance --cli-input-json file://launch-config.json
        launch-config.json
            {
                "ImageId": "ami-12345678",
                "InstanceType": "t2.micro",
                "KeyName": "my-key-pair",
                "SecurityGroupIds": ["sg-12345678"],
                "SubnetId": "subnet-12345678",
                "MaxCount": 1,
                "MinCount": 1,
                "TagSpecifications": [{
                    "ResourceType": "instance",
                    "Tags": [
                        {
                        "Key": "Name",
                        "Value": "MyInstance"
                        }
                    ]}
                ]
            }


AWS Resource Access Manager (RAM) 
    - you share resources across AWS Accounts
    - VPC can be shared with other AWS Account
    - is a service that enables you to share AWS resources with other AWS accounts, organizational units (OUs)
    - Benefits:
        - centralized management
        - security
        - scalable infrastructure for some reason
    Principals:
        - entities that can be granted access to shared resources. These entities can be other AWS 
            accounts, organizational units (OUs), or the entire AWS Organization. Principals in AWS RAM are 
            essentially the recipients of shared resources, and they are the ones who can use or manage those resources.
        

            
#######################################    
#######################################    
#######################################    
#######################################
####                               ####
####       AWS NETWORKING          ####
####                               ####
#######################################    
#######################################    
#######################################    
#######################################    
    
    
NAT v Switch v Router
    - Router: Does NAT. Routes data packets between networks using IP addresses
    - Switch: Connects multiple devices within the same network. Forwarding traffic based on MAC addresses. (MAC addresses)
    - NAT: Translates private IPs to public IPs 
             (when going outbound, and handles the response from external internet. 
        but w/o initial outbound, the public internet cant talk to your network unless rules ect are established
    
             [Internet]  
                 |  
           [Public IP]  
           [ Router ]  (Performs NAT)  
                 |  
           [ Switch ]  
          /    |    \  
     [PC1]   [PC2]  [PC3]  
 (192.168.1.2) (192.168.1.3) (192.168.1.4)  





VPC (Virtual Private Cloud)
    - a virtual network dedicated to your AWS account
    - an isolated network where you can launch AWS resources.
    - region specific
        - do not span regions
        - (have to use VPC Peering to connect across regions)
        
    Default VPC        
        - every region comes with a default VPC (and components of it)
        - So you can immediately deploy instances
        Default things:
            VPC
            subnets x4
            IGW
            Security Groups (SG)
            NACL
            DHCP option set
            Route Table
        - you can delete the "default VPC"
        - you can recreate a new VPC (but not restore) via CLI `$ aws sec create-default-vpc --region ca-central-1`
    Note
        Default route (aka catch-all-route)
            - 0.0.0.0/0 or ::/0
        
                
Internet Gateway (IGW)
    - a VPC component that connects you and the internet. 
    - supports IPv4 and IPv6 traffic
    - free, no charge
    - only data charge 
    - the Route Table will probably have something like
    Desintation 0.0.0.0/0 ---> Target igw-xxxxxxx
    
    - not much to configure. It just exists being an IGW
    - IGW not tied to AZs.

Subnets

    - a range of IP addresses in your VPC. 
    - create things (EC2s), in specific subnets.
    - is in only 1 AZ
        - cannot span multi AZs
    Dual stack = both IPv4 & IPv6
    
    
        VPC
        ├── Public Subnet
        │   ├── NAT Gateway (Elastic IP)
        │   └── Internet Gateway attached to VPC
        ├── Private Subnet
        │   └── EC2 instance (no public IP)

    
    Types:
        Public subnet
            - has a direct route to an IGW
            - resources (ECs) can access public internet
            - outside internet can communication to your resources, 
                - (if the security group and NACLs allow it)
        Private subnet
            - does NOT have a route to an IGW
            - resources (ECs) need a NAT to access the public internet
        VPN-only subnet
            - does NOT have a route to an IGW
            - has a route to a "Site-to-Site VPN connection" with a "virtual private gateway"
        Isolated subnet
            - no routes outside the VPC
            - communication is only with other resources in the same VPC
    - subnets must be associated with a route table.
        - RT names routes for outbound traffic leaving the subnet
    - subnets must be associated with a NACL
    

NAT Gateway 
    Recall:
        actual NAT = mapping IP addresses from one to another
            = private IPs to public IPs 
            - done in the IP headers while they still in traffic
        eg)     (public)                    (private GW)
            10.10.10.24/24 ----> NAT ----> 172.16.131.254/24
    AWS NAT GW:
        - allows instances in my private subnet to access the outside world 
            but the outside world cant access me
          - (another words: that allows resources in private subnets to access the internet while remaining private themselves)
        - thats it.
        - secure, scales, ect.
        - requires public IP
        - created in 1 AZ
        - Cost money $$ (expensive relatively)
        
        - supports IPv6 to IPv4 Translations
        - public subnets do not need a NAT GW
    
    NAT Gateway @ AWS console
        - In aws, you dont do a private -> public NATing map
            - doesnt have a table
        - instead we create EC2s (or w/e) and attached it to subnets.
        - then in Route Table we map "Desination: <CIDR Block>" ---> "Target: <NAT Gateway>"'
        - or similar with certain resources
            
        1. Public NAT Gateway
            - "standard" NAT
            - Instances in private subnets can connect to the internet through a public NAT gateway
                - cant wont receive unsolicited inbount connections from internet
            - public subnet must have a route to a Internet Gateway (IGW) (possibly through NAT or w/e).
                Route Table: 0.0.0.0/0 → Internet Gateway.
        2. Private NAT Gateway
            - instances in private subnets can connect to other VPCs (or your on-premises) through a private NAT gateway. 
                Route Table: 10.0.0.0/16 → local
            - if you want a private network, you MUST NEED a NAT gateway
            - instances in private subnets (with no public IPs) can outbound to internet, but blocks inbound connections.
        Note:
            Internet Gateway IG
                - Every EC2 needs a public IP
                - bidirectional internet access for each EC2
            Public NAT:
                - only NAT needs IP
    

NACLs
    - Network Access Control List
    - a security layer like a firewall
    - subnets are associated with NACLs
    - 1 subnet belongs to 1 NACL
    NACLs:
        - at subnet level
        - stateless:  you need rules for both inbound/outbound traffic
        - Rules evaluated in numerical order (lowest to highest)
           - allow/deny
        - Does not support DNS allow/deny
            - SG also does not support DNS allow/deny
            - you'd have to use AWS Route 53 Resolver DNS Firewall

    Differences from Security Groups:
        - NACLs are stateless while Security Groups are stateful
        - NACLs apply to all instances in a subnet, while Security Groups apply to specific instances
        - NACLs allow you to explicitly deny traffic, while Security Groups only allow traffic
        - once a rule matches, stops further evaluations.
        - Starts are loweest
            Rule # | Type    | Protocol | Port Range | Source/Dest | Allow/Deny
            100    | Inbound | TCP      | 80         | 0.0.0.0/0  | ALLOW
            200    | Inbound | TCP      | 443        | 0.0.0.0/0  | ALLOW
            300    | Inbound | TCP      | 22         | 10.0.0.0/8 | ALLOW
            *      | Inbound | All      | All        | 0.0.0.0/0  | DENY

    - inbound = ingress
    - oubound = egress
    Trick 101:
        - block an attackers IP at NACL level
        -> DENY 23.248.68.34/32

    
    
AWS Security Groups
    - virtual firewall on the instance level (EC2)
    - instance level: specific resources/instances, not to subnets
    - Stateful: 
        - If you allow inbound traffic, the returning outbound response is automatically allowed (and vice versa)
            - b/c stateful magic
    - Allow rules only
        - you cannot explicitly deny traffic
    - rules evaluated together. 
        (no priority #, and since "Deny" doesnt exist there is no risk of conflict)
        
    - Deny by default: All inbound traffic is denied and all outbound traffic is allowed by default
    
    Features:
        - SG can allow another SG. (eg, all source traffic from SG-1 is allowed)
        - Instance can have multiple SGs attached.
        INBOUND
            Type	        Protocol	Port	Source
            -------------------------------------------------
             MySQL/Aurora	TCP	        3306	SG-1 (ID)
             SSH	        TCP	        22    	203.0.113.10/32 (your IP)
            HTTP	        TCP	        80    	0.0.0.0/0
        OUTBOUND
            Type	        Protocol	Port    Destination
            -----------------------------------------------------
            All traffic	    All	        All	    0.0.0.0/0

    Security Group VS NACL
        Level
            SG: instance level.
            NACL: subnet level.
        Statefulness:
            SG: Stateful — return traffic is automatically allowed.
            NACL: Stateless — rules must be defined for both inbound and outbound.
        Rule Type:
            SG: allow 
            NACL: allow and deny
        Evaluation:
            SG: rules evaluated together. (no priority #, and since "Deny" doesnt exist there is no risk of conflict)
            NACL: Rules evaluated in order -> lowest to highest -> 1st match breaks the evaluation/iteration.
    
    CLI Create:
        1. Create the security groups
        aws ec2 create-security-group 
            -- group-name MySecurityGroup 
            -- description "My security group" 
            -- vpc-id vpc-xxxxxxxx
    
        2. Add rules to the security group
        aws ec2 authorize-security-group-ingress 
            -- group-id sg-xxxxxxxx 
            -- protocol tcp 
            -- port 80 
            -- cidr 0.0.0.0/0
            
        3. Associate EC2 instance to the security group.
        aws ec2 modify-instance-attribute 
            -- instance-id i-XXXXXXXXXXXXXXXXX
            -- groups sg-xxxxxXXXXXXXXXXXX


    
Route Table
    - is a set of rules (routes) that direct network traffic within a VPC
        - is a table of IP CIDRs (Desinations) with Targets
    - sits behind the Internet Gateway within the VPC.
    - has 1+ subnet
    - Every VPCs must have at least one route table.
    - 1 subnet can only be associated with 1 route table at a time.
    eg)
            Destination	    Target	        Status	    Propagated
            ------------------------------------------------------------
            10.0.0.0/16	    local	        active	    No
            10.20.0.0/16	nat-04abc567    blackhole	No
            0.0.0.0/0	    igw-0123abcd	active	    No
                   
       -blackhole = gg, the route exists, but its target is no longer valid 
    Usage:
        - "Local" target = the default route = the subnets
        - "IGW" target = ingress/egress connections
        - "Virtual Private Gateway = connections to on-premise 
        - "NAT Gateway" = IPv4 egress connections 
        - "Egress Only GW" = IPv6
        - "Instance" = out to a specific EC2
         ....

    Route Table Fun Facts:
        - Allows instances in different subnets of the same VPC to communicate without 
            needing explicit routes or an internet gateway.
        - Virtual Private Gateway (VGW) is an AWS-managed VPN gateway that enables secure communication between a 
            VPC and an on-premises network (via a Site-to-Site VPN).
            - (can be target) for route table
        - Egress-Only IGW : allows outbound traffic from IPv6 instances to the internet but blocks inbound traffic.
            Since IPv6 does not use NAT, this acts as a security layer preventing unsolicited inbound connections.
        - NAT Gateway : allows instances in a private subnet to access the internet (e.g., for software updates) without exposing them to incoming internet traffi
        - Carrier Gateway : enables connectivity between AWS VPCs and telecom networks (e.g., mobile networks using AWS Wavelength).
            - Primarily used for AWS Outposts in carrier networks.
        - Core Network is part of AWS Cloud WAN, providing centralized routing and security controls for managing large-scale global networks across multiple AWS regions and on-premises locations.
        - (every vpc comes w/ main route table (cannot delete) & you make custom route tables)
            - b/c every subnet must have a route table

Jumpboxes 
    - security hardened VMs to provide *secure access to private subnets* 
    - via SSH or RCP
    - you use Security Groups, Route Tables, and private/public subnets

    
Elastic Network Interface ENI
    - NOT A SERVICE
        - is a component
    - is a "virtual network interface"
    - attaches to EC2s or NAT Gateways or w/e
    - has 1 primary IPv4 Address*
    - has 1 or more secondary IPv4 addresses (optional)
        * primary = main IP IPv4 address
        * secondary = an additional IP
        
    Like a NIC/network-card on you computer, but for EC2s:
        - like how RAM is a hardware component inside a computer.
        - It’s not a separate AWS service you subscribe to, but rather a component that comes with every EC2 instance.
        - you manage ENIs in the AWS Web Console 
            EC2 → Network Interfaces.
        - Every EC2 instance must have at least one ENI
        - It’s responsible for IP addresses, security groups, and network connections of the instance.
    - If an EC2 instance fails, you can:
        Detach the ENI from the failed instance.
        Attach it to a standby EC2 instance.
        The new instance instantly gets the same IP addresses and network settings.
    CLI
        create:
            $ aws ec2 create-network-interface --subnet-id subnet-12345678 --description "My ENI" --groups sg-12345678
        attach:
            $ aws ec2 attach-network-interface --network-interface-id eni-12345678 --instance-id i-12345678 --device-index 1
        detech 
            $ ...
        delete 
            $ ...



Recap:
    1. VPC (Virtual Private Cloud)
        A virtual network for AWS resources, isolated from others.
    2. Subnets
        Divides VPC. Public subnets for internet; private subnets don't.
    3. Internet Gateway (IGW)
        Enables public internet
    4. Route Table
        direct traffic in VPC. Has CIDR blocks and/or aws service maps
    5. NAT Gateway
        - private subnet can reach internet while preventing inbound traffic (eg download dependencies)
    6. Security Groups
        Instance-level security. Stateful
    7. NACLs
        Subnet-level security. Stateless
    8. EC2s
        In VPCs
    9. Things that are EC2
        - RDS
        - ECS (EC2 launch type)
        - Elastic Load Balancing (ELB)
        - ElastiCache
        - Redshift
        - OpenSearch
        - PrivateLink endpoints

CLI Route-table 
    `aws ec2 describe-route-tables` gets info route tables.
        eg:
        aws ec2 describe-route-tables
        aws ec2 describe-route-tables --filters "Name=vpc-id,Values=vpc-0abc123456def7890"
        aws ec2 describe-route-tables --route-table-ids rtb-0123456789abcdef0


    `aws ec2 associate-route-table`  associates a route table with a subnet, gateway, or other resource. 
        aws ec2 associate-route-table --route-table-id <value> --subnet-id <value>
        aws ec2 associate-route-table --route-table-id rtb-0123456789abcdef0 --subnet-id subnet-0abc123456def7890
        aws ec2 associate-route-table --route-table-id rtb-0123456789abcdef0 --gateway-id igw-0abc123456def7890
        # must associate a subnet with a particular route table. 
        # a subnet only associated with 1 route table, but 2+ subnets with the same subnet route table.

    --enable-dns-hostnames
        `aws ec2 modify-vpc-attribute`  --enable-dns-hostnames "{\"Value\":true} --vpc-id <your-vpc-id>"
        # EC2s in the VPC will auto receive DNS names 
        # disabled = no public DNS names, only a private IP or internal DNS names within the VPC.
        
    --map-public-ip-on-launch
        `aws ec2 modify-subnet-attribute --map-public-ip-on-launch --subnet-id subnet-12345678`
        # EC2s in the subnet will auto receive a public IP.

        
JMESPath
--query
    `aws ec2 describe-instances --query \
        "Reservations[*].Instances[*].Tags[?Key == 'Name'].Value" --output text`
        {
          "Reservations": [
            {
              "Instances": [
                {
                  "InstanceId": "i-1234567890abcdef0",
                  "Tags": [
                    {
                      "Key": "Name",
                      "Value": "my-ram-ec2-original"
                    }
                  ]
                }
              ]
            }
          ]
        }
        
aws ec2 create-network-acl-entry \ 
    --network-acl-id acl-02def3052778d5ce2 \  # Specifies the ID of the NACL to modify
    --ingress \                               # Indicates the rule applies to incoming traffic (Ingress)
    --rule-number 90 \                        # Rule number (determines the order of the rule)
    --protocol -1 \                           # -1 means all protocols (TCP, UDP, ICMP, etc.)
    --port-range From=0,To=65535 \            # Applies to all ports (0-65535)
    --cidr-block 174.5.108.3/32 \             # The IP address that the rule applies to (specific IP)
    --rule-action deny                        # Denies traffic matching the rule

https://github.com/ExamProCo/AWS-Examples/blob/main/vpc/nacl/template.yml
    https://chatgpt.com/c/67a4ae83-5af8-8001-a292-18e29c14e689

!Ref SSMRole resolves to the ARN of the IAM role created here. eg: arn:aws:iam::ACCOUNT_ID:role/SSMRole


GATEWAY
    - thing that sits between 2 networks
    - a networking service
    - acts as: reverse proxies, firewalls, and load balancers
    - some cloud providers call their GWs "Load Balancers"
    eg)
        Internet GW
            - inbount & outbound public traffic for ipv4 and ipv6
        NAT GW
            - outbound private traffic for ipv4
        Egress Only Internet GW
            - outbount private traffic for ipv6
        Carrier GW
            - connecting to AWS partnered telecom network
        Virtual Private GW (VPG)
            - the endpoint to your AWS account for VPN connections
        Customer GW
            - the endpoint into your on-premise account for VPN connections
        GW Load Balancer (GWLB)
            - Layer 3 (network layer) load blancer (for Firewalls IDS/IPS)
        Directed Connect GW
            - the endpoint for fiber optic connections at data center
        AWS Backup GW
            - the endpoint for AWS managed backups
        IoT Devce GW
            - the endpoint to send IoT data in both directions
        AWS Transit GW
            - "Hub and Spoke model" to simplify VPC peering
        Amazon ApI GW
            - abstracts API endpoints to services
        AWS Storage GW
            - syncing, caching, or extending local storage to cloud stoarge
    
Egress-Only Internet Gateway (EO-IGW)
    - ipv4 traffic going out, but PREVENT coming in
    - ipv6 address are all public so they dont requring NAT-ing
    - since IGW would not resistrict inbound traffic
    -> an EO-IGW denies inbound traffic (keeping you private)

Elastic IPs
    - a static IPv4 address for you
    - attach it to EC2s
    - attach it to ENIs
    - region specific
    - 1$ for unassociated IPs
    
Fun fact:
    - not all AWS services support ipv6, but most do
    
AWS Direct Connect
    - physical conection from on-premise to AWS
    - "Dedicated network connection"
    - helps reduce network costs
    - more consistent network than typical internet
    - has to be in a "co-located" AWS direct connect location
        - (b/c physical)
    Pricing;
        1 Port size (1, 10, 100 GBs, ect)
        2 Pay / hour (regardless of use
            - Dedicated 
                - AWS themselfs suppy the connection
            - Hosted
                - a AWS partner supplys the connection
        3 Data Transfer Out (DTO)
            - total GBs
            - OUT of aws
            - inside of aws is free
    
Note:
    Internetwork traffic privacy = keeping data private as it travels across diff networks
    

AWS PrivateLink (1/2)
    - "privately connect" between VPCs and:
        AWS services, 
        or other AWS accounts's services, 
        or AWS Marketplace services, 
    - it’s an umbrella technology that enables private connectivity between:
        - AWS services (e.g., S3, SQS, SNS, DynamoDB)
        - VPCs in different accounts
        - Third-party applications in AWS Marketplace

    - not required: IGW, NAT device, VPN, AWS Direct Connect
    - is a Endpoint type in AWS VPC
        - Interface Endpoint
        - Gateway Endpoint
    - is NOT a single service (like EC2 or S3). 
    
    - You cannot create AWS PrivateLink using AWS CLI directly, 
        -but you can create services that use PrivateLink, such as Interface Endpoint:
        $ aws ec2 create-vpc-endpoint 
            --vpc-endpoint-type Interface
            --vpc-id vpc-12345678 
            --<boiler plate> ...

AWS Private Link (2/2)
    [VPC, private, no GAT/IGW]
    - sometimes called "VPC Endpoint"
    - Traffic stays in AWS network
    - no public internet. 
    - Creates a route in your VPC
    - No NAT or IGW

    1. VPC Interface Endpoint
        Use Case: private network for wide range AWS services or 3rd party (via Marketplace)
        - An ENI (private IP) 
        - money $$
            (Charged per hour and per GB of data)
        - private connection between most AWS & custom services
        - cross-account 
        - permissions fine-grained

    2. VPC Gateway Endpoint
        Use Case:  simple, free, private access to S3 or DynamoDB.
        - Routes traffic via VPC route table
        - free
        - single account
        - NO permissions
        - s3 DynamoDB only

    AWS Interface Endpoints (AWS PrivateLink)
        - allows you to privately access AWS services over Amazon's private network 
          without using the public internet.
        - is a ENI with a private IP
        - is an  entry point for incoming traffic to your VPC
            - VPC resources (EC2, Lambda-in-VPC, ECS, etc.) can privately reach AWS services.
        - ONLY a VPC feature.
        - AWS provides private URLs (DNS names) per Interface Endpoint, mapped to private ENIs.
        - They ensure traffic stays off the public internet.
        - good if you need fine-grained security control with security groups.
        - secure data transfer
        Price:
            $ 0.01 / hour ~= $7.5 per month
            + 0.01 / GB
            
    VPC Gateway Endpoint  (AWS PrivateLink)
        - I guess this is just a very vanilla endpoint?
        - connect to S3s and DynamoDBs w/o a IGW or NAT
        - between your VPC instances and the S3 or DynamoDB
        - doesnt use PrivateLink 
        - FREE 
        Use Case:
            - like private link but for s3/dynamo
            - you have EC2 instances in a private subnet that need to access an S3 bucket. 
                With a VPC gateway endpoint they can do so **without going through the public internet**.


NOTE
    - Difference between PrivateLink and AWS Virtual Private Network (VPN)
        Private Link:
            - AWS to AWS
            - Links a VPC to some AWS resource
            - Traffic stays within AWS's network.
        AWS VPN:
            - Client to AWS
            - Links on-premise to AWS
            - Links a dude to AWS
            - encryption your traffic over the public internet.
        AWS Peering
            - AWS to AWS
            - VPC to VPC
            - A -> B
            - Not Transitive
            - If A <---> B <---> C 
              CANNOT A <---> C
            - entire VPC to entire VPC
        Transit Gateway
            - AWS to AWS
            - VPC to VPC
                - many VPCs connect via a Gateway
            - hub-and-spoke topology
            - YES A <---> B <---> C 
            - money, very very slight slower than peering

    
AWS Virtual Private Network
    - is a secure private tunnel to the AWS network
    - pay per hour
    Two types:
        1. AWS Site to Site VPN
            - connect your on-premise network or office branch to AWS
            
            Virtual Private Gateway (VPN Gateway)
                - the thing that connects your VPC with your on-site network
                - on the AWS side
                - 'is the VPN "concentrator" on the Amazon side of the Site-to-Site VPN connection.'
                - You create a Virtual Private Gateway and attach it to a VPC

        2. AWS Client VPN
            - connect individuals (remote workiers, laptops) to securely connect to AWS resources over the internet
            - "a fully managed client-based VPN" that enables you to 
                securecly access AWS resources & your on-premises network"
            - for developers and employees
            
            AWS Customer Gateway
                - the thing that connects your on-prem network with your AWS VPC
                - on the client side
                - its created in AWS that represents the customer gateway device in your on-premis network
                - aws provides sample config files from many venders for you to get started

        - You'll have a:
                (on prem)                           (your AWS VPC)
            Customer Gateway <--(VPN Connection) --> VPN Gateway

VPC Peering
    - connect 1 VPC with another VPC
        - direct route, private IPs/subnets
    - AWS VPC Peering is a network connection between two VPCs that enables them to communicate 
        privately as if they were part of the same network. 
    - in AWS's internal network, avoiding the need for internet gateways, VPNs, or NAT devices.
    - across AWS Accounts and/or across regions
    - VPCs must not have overlapping CIDR
    - is not a gateway
    - is not a VPN conection
    - does not use physical hardware
    - A "Peering Connection" in AWS is a networking link between two VPCs
    - VPC Peering is not a standalone AWS service like EC2 or S3. 
        - Instead, it is a feature of VPC allows you to establish private network connectivity between two VPCs.
    Key Features of VPC Peering
        - Private Connectivity: Traffic within AWS
        - Low Latency & High Throughput
        - Inter-Region & Intra-Region Peering: VPCs same region or different regions.
        - No Single Point of Failure: does not rely on a single device, like VPN does.

    Limitation:
    - Scaling Issue, Many-to-Many VPC Connectivity
        - Alternative: Use AWS Transit Gateway, which simplifies network management at scale.
    - Transitive Routing
        - If VPC A is peered with VPC B, 
        - B is peered with C, 
        - A cannot communicate with C through B.
            - Alternative: AWS Transit Gateway allows transitive routing.
    - Overlapping CIDR Blocks
        - Fails if VPCs have overlapping IP ranges.
        - Alternative: Reconfigure CIDR ranges or use AWS PrivateLink.


AWS Transit Gateway
    - allows you to connect multiple AWS VPCs and on-premises data centers
    - through a single, centrally managed gateway. 
    - It simplifies network management by acting as a router between these networks
    - private, no public internet
    - is a logical, cloud-based routing hub for connecting multiple VPCs and networks.
        - opposed to AWS Direct Connect, that has dedicated, private physical connection

    - Transit Gateway does practically everything VPC Peering does + more
        - but TGW uses routing to connect the two.
        - but cost money (VPC Peering is free-ish, you pay for data)
    Transitive 
        - in TGW, transitive routing IS supported
        - in VPC Peering Transitive routing is not supported 
            -  if VPC A is peered with VPC B, and VPC B with VPC C, A cannot talk to C.
    


VPC Flow Logs
    - log IP traffic info in your vpc
    - logs stored in s3, CloudWatch logs, or kinesis data  firehose

    
AWS Lattice
    - easily turn AWS resources into services for micro-serivces architecture
    - "fully managed application networking service that simplifies service-to-service 
        communication across VPCs and AWS accounts. It enables users to securely connect, observe, 
        and manage service communications without requiring complex networking configurations."
    - Good for:
        - You have multiple AWS accounts and need a simple way to connect services.
        - services deployed in different VPCs (e.g., per environment: Dev, Staging, Prod).
        - IAM-based access control
        - built-in observability
        - You don’t want to manage load balancers, PrivateLink, or complex networking.
        - MICROSERVICES
        - good for across VPCs or AWS accounts
        
    Example:
        - Problem: Architecture Today (Without Lattice)
            - Each team has its own AWS account and VPC:
                Order Service (AWS Account A, VPC A)
                Payment Service (AWS Account B, VPC B)
                Inventory Service (AWS Account C, VPC C)
                Customer Profile Service (AWS Account D, VPC D)
        Solution
        - Register Services in Lattice
            You define services like:
                orders-service
                payments-service
                inventory-service
                customer-profile-service
            AWS automatically provides service discovery - no need for manual DNS management.
            
    
AWS Traffic Mirroring
    - mirrors inbound and outbound traffic at the Elastic Network Interface (ENI) level to a target destination
    - copies the data deeply (packet duplication)
        - and pipe the duplicated data to your target destiantion 
        - optionally filter
    - sends duplicated data to a security appliance or monitoring tool.
    - VPC-specific
      - ENIs within a VPC, means:
        - cannot mirror traffic from non-ENI AWS services such as:
            - Lambda functions (unless they are in a VPC)
            - Amazon RDS/Aurora (managed databases don’t expose ENIs for direct mirroring)
            - AWS Fargate (without VPC networking) only works within the same VPC. 
            - You cannot mirror traffic across VPCs or regions directly.
        Things that use ENI:
            - EC2s
            - Elastic Load Balancers (NLBs only)
            - AWS Lambda (in a VPC)
            - ECS Tasks (with awsvpc networking mode)
            - AWS App Mesh Envoy Proxies
            - Elastic File System (EFS) Mount Targets
            - AWS RDS/Aurora (Limited)
                Not directly accessible via ENI, but can indirectly route through EC2 for capture.
            - VPN and Direct Connect Gateways

AWS Network Firewall
    - security service on VPCs.
    - does deep packet inspection (DPI) and intrusion prevention system (IPS) 
    - "a stateful, managed, network firewall and IDS/IPS for VPCs"
    - filters traffic based on customizable firewall rules, threat intelligence feeds, and AWS-managed rule groups. 
    -> Protects entire VPC networks at the subnet level.
    -> vs WAF, which protects Layer 7, application HTTP/s


AWS Network Address Usage (NAU) 
    - a metric to help you monitor VPC size.
    - refers to the number of unique IP addresses in VPC. 
    - "weight" applied to resources
        NAT Gateway = 6 NAU
        IPv4/6 address = 1 NAU
        ect
    More things that have NAU:
        private and public IPs
        EC2 instances
        load balancers
        VPC endpoints
        NAT gateways
        ect
    - For:
        billing and network management, 
        Scaling Considerations 
        IP Management
        Used in services like VPC IP Address Manager (IPAM) and PrivateLink.
    - Can enable "NAU monitoring" on a VPC to view metrics at: "CloudWatch NAU metrics" 

#####################################
#####################################
######        IAM           #########
#####################################
#####################################
AWS Identity and Access Management (IAM)
    - allow and deny users to AWS resources.
    - limit users, groups, and via permissions

    IAM Policies
        - JSON documents of permissions for a specific user, group, or roles 
        - deny or allow to access services. 
        - Policies are attached to IAM Identities

    IAM Permission
        - The API actions that can or cannot be performed (Create bucket, view cloudwatch, ect)
        - They are represented in the IAM Policy document

    IAM Identities
        IAM Users
            - policies are attached to Users
            - End users who log into the console or interact with AWS
              resources programmatically or via clicking Ul interfaces
        IAM Groups
            - policies are attached to Groups
            - Group up your Users so they all share permission levels of the group
              eg. Administrators, Developers, Auditors
        IAM Roles
            - policies are attached to Roles, (meant to be assumed temporarily)
            - Roles grant AWS resources permissions to specific AWS API actions
            - Associate policies to a Role and then assign it to an AWS resource
            - often used with temporary credentials
            
    Effect – Allow or Deny
    Action – API actions
    Resource – aws resources, what the action applies to
    Condition – optional
    
    - An IAM policy is a collection of permissions.
        eg) below, 1 policy of 3 permissions:
            {
              "Version": "2012-10-17",
              "Statement": [
                {
                  "Effect": "Allow",
                  "Action": "s3:ListAllMyBuckets",
                  "Resource": "*"
                  "Condition": ...
                },
                {
                  "Effect": "Allow",
                  "Action": "s3:GetObject",
                  "Resource": "arn:aws:s3:::example-bucket/*"
                },
                {
                  "Effect": "Deny",
                  "Action": "s3:DeleteObject",
                  "Resource": "arn:aws:s3:::example-bucket/*"
                }
              ]
            }
        
        Managed Policies
            - a polcy by AWS, you cannot edit (Has orange box icon, in web console)
        Customer Managed Policy
            - created by you the customer, editable
        Inline Policy
            - directly attached to the user
            

IAM - Temporary Security Credentials
    - short-lived credentials
    - Automatically Rotated (no manual management)
    - Scoped Permissions (defined by a IAM role or policy)
    - all temporary security credentials are issued by the Security Token Service (STS). 
    ✅ STS is the sole issuer of temporary credentials.
    
    
    - Use cases:
            Access stuff without IAM User Credentials (no hardcoded keys)
            Cross account access
            AWS Lambda, EC2, and ECS Roles
            Federated Identity
    - temp credentials are the basis for *roles* and *federate identity*
     
    Cross Account Roles
        - grant other aws accounts access to another account
        - Account A assumes Account B
            - Action: sts:AssumeRole
        
    How to Obtain
        - AWS Security Token Service (STS) API Calls:
            sts:AssumeRole 
                → when EC2 assumes a role.
            sts:AssumeRoleWithSAML 
                → for enterprise SSO.
            sts:AssumeRoleWithWebIdentity 
                → for web identity provider (google, facebook, ect)
            sts:GetSessionToken 
                → for MFA-protected sessions.
            GetFederationToken 
            
        $ aws sts assume-role (cross account)
        $ aws sts get-session-token (Extend session)
        $ aws sts get-federation-token (SSO)
        Example respone:
        {
            "Credentials": {
                "AccessKeyId": "ASIAxxxxxxxxxxxx",
                "SecretAccessKey": "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
                "SessionToken": "FQoGZXIvYXdz...=", 
                "Expiration": "2025-02-28T12:34:56Z"
            },
            "AssumedRoleUser": {
                "AssumedRoleId": "AROxxxxxxxxxxxxx:role-session",
                "Arn": "arn:aws:sts::123456789012:assumed-role/MyRole/role-session"
            }
        }
        
    Supports 2 typs of Identity Federation
         - SAML (For Enterprise SSO)
            - Users log in via their corporate IdP and get temporary AWS credentials.
            - ✔️ Use SAML (enterprise) for SSO with ADFS, Okta, or Ping ID
            $ aws sts assume-role-with-saml --<boilerplate>
         - OpenID Connect (OIDC) 
            - aka Web Identity Federation
            - mobile & web applications.
            - ✔️ Use for mobile/web apps where user logs in with Google, Facebok, or Cognito
            - idk when ever to use this
            $ aws sts assume-role-with-web-identity --<boilerplate>
          ---------------------
        - SAML (Enterprise SSO) Matter?
            ✅ If you're in a company where employees need AWS access, SAML allows them
               to log in using their existing company credentials instead of having separate IAM users.
            📌 Example: Corporate IT Teams
                - company with 500 ppl need AWS.
                - Instead of creating 500 IAM users, the company links Okta, ADFS, or Google Workspace to AWS.
                - Employees log in to AWS using their corporate credentials.
                - AWS grants them temporary credentials via AssumeRoleWithSAML.
        - When Does OpenID Connect (OIDC) Matter?
            - (This makes no sense to me)
            ✅ If you're building a web or mobile app that needs to access AWS, 
                OIDC allows users to authenticate with Google, Facebook, or Amazon Cognito.
            📌 Example: A Mobile App Uploading Photos to S3
                - You're building an Instagram-like app where users upload photos.
                - Your app needs to store photos in an AWS S3 bucket.
                - Instead of storing long-term IAM credentials in your app (bad security), users log in via Google
                - AWS grants temporary credentials to the app via AssumeRoleWithWebIdentity, 
                    allowing the user to upload to S3 securely.
            📌 Example: Using OpenID Connect for Temporary AWS Access in a Corporate Environment
                - developers need access to AWS to manage EC2 instances
                - Instead of creating 50 separate IAM users, 
                    - they log in using their company's Google or Microsoft account via OpenID Connect (OIDC).
            - ?? this is dumb
            AssumeRoleWithWebIdentity:
                Goal:
                    - get user's temp login credentials who has been authenticated in mobile/web with 
                        web identity provider, like Google or Facebook
                How to:
                    1. Login with facebook
                    2. Get token from Web Token (facebook)
                    3. you AssumeRoleWithWebIdentity
                                             +---------------------------------------------------+
                                             |                                                   |
                                             v                                                   |
           Facebook <--> (JWT/OAuth) <--> User/Dev --> (AWS CLI AssumeRoleWithWebIdentity) --> STS
                                             |
                                             V
                                         RDS/S3/EC2/ect
                    
                    
AWS Single Sign-On (SSO) 
    - create & connect with your workforce identities in AWS once and manage them centrally across you AWS organization
    - "AWS SSO" is now "AWS IAM Identity Center"
    - a cloud-based identity and access management service 
    - manage AWS accounts, applications, and third-party services using a single set of credentials. 
    - simplifies identity management
    - enhances security 
    How AWS SSO Works
        - Administrator Sets Up AWS SSO
            - Connects it to an existing identity provider (like Active Directory, Okta, or Azure AD).
        - User Logs via AWS SSO Portal
    Setup:
        Sign in as an Admin (IAM with AdministratorAccess)
            Go to the AWS Management Console.
            Enable AWS SSO
        Choose Your Identity Source
            Default AWS Directory (built-in user directory).
            Active Directory (AD) (for corporate networks).
            SAML-based or OIDC Identity Provider (like Okta, Azure AD, Google Workspace).
        Provide Users with SSO Access
            - Share the AWS SSO URL with users.
        They log in using their corporate credentials.
        
    
########################################
########################################
########                      ##########
########        EC2           ##########
########                      ##########
########################################
########################################

Cloud-Init
    - the industry-standard tool for initializing cloud instances (like AWS EC2, Azure VMs, Google Cloud VMs, etc.)
    - automate the configuration of an instance when it boots up for the first time.

    User Data Scripts
        - Run User Data Scripts
            - a first-time-boot script,
            - When you launch an EC2 instance, Cloud-Init executes user scripts (shell scripts or YAML configuration) 
                to install software, update packages, or set up configurations.
            - bash script eg: $ yum install -y httpd
            - or a yaml file
        - Configure Networking
        - Manage SSH Keys
        - Install and Configure Software
        - Attach Storage and Filesystems

    aws ec2 run-instances \
        -- image-id ami-0abcdef1234567890
        -- count 1 \
        -- instance-type t2.micro \
        -- security-group-ids sg-1234567890abcdef0
        -- subnet-id subnet-12345678 \
        -- user-data file://path/to/your/userdata-script.sh  <--------- BAM
        [or "userdata.yaml", or inline the script]
        
    Fun fact:
        - EC2's metadata tell you where it gets its user-data script from
            - you can "curl" it from the ec2:
            -> http://169.254.169.254/latest/ user-data
        - EC2's metadata is at:                 
            -> http://169.254.169.254/latest/ meta-data
            -> http://169.254.169.254/latest/ meta-data/instance-id
            -> http://169.254.169.254/latest/ meta-data/local-ipv4
            -> http://169.254.169.254/latest/ meta-data/iam/secruity-credentials
            

EC2
    - a highly configurable VM
    - has resizable compute capaciy
    how to:
        1. Choose AMI
        2. Choose Instance Type
        3. Add Storage
        4. Configure 

EC2 types:

    Naming:
               c 7 g n . l a r g e
               ↑ ↑ ↑ ↑       ↑
               | | | |       |
               | | | |       └─── size
               | | | └─────────── some feature (optional)
               | | └───────────── processor family (optional)
               | └─────────────── generation
               └───────────────── instance type

    1. General Purpose
        T Series (T3, T3a, T4g) – Burstable performance instances.
        M Series (M5, M6i, M7i) – Balanced compute, memory, and networking for general workloads.
        -> web servers, code repos
    2. Compute Optimized
        C Series (C5, C6i, C7g)
        -> scientific modleing, gaming servers, ad servers
    3. Memory Optimized
        R Series (R5, R6i, R7g) – For memory-intensive applications.
        X Series (X1, X2idn, X2iezn) – For high-performance databases and SAP HANA.
        Z Series (z1d) – High memory and compute performance.
        -> in-memory caches, in-memory databases
    4. Storage Optimized
        I Series (I3, I4i) – High IOPS SSD-based storage.
        D Series (D2, D3, D3en) – High-capacity HDD storage.
        H Series (H1) – High disk throughput.
        -> NoSQL, transactional databases, data warehousing
    5. Accelerated Computing
        P Series (P3, P4d, P5) – NVIDIA GPUs for deep learning.
        G Series (G4, G5, G6g) – GPUs for graphics and ML inference.
        F Series (F1) – FPGA-based for custom hardware acceleration.
        Inf Series (Inf1, Inf2) – AWS Inferentia chips optimized for ML inference.
        -> Machine learning, seismic analysis, computational finance
    6. High-Performance Computing (HPC)
        Hpc Series (Hpc6a, Hpc6id)
        - super science stuff
        
    EC2 sizes 
        - small, medium, large, xlarge, ...
        - size generally doubles in price and in attributes
                    RAM (GiB)   $/month
            small    12         16.00
            medium   24         33.00
            large    36         67.00
    
    EC2 Instance Profiles 
        - attach IAM roles to EC2 instances
        - secure b/c no hardcoded credentials.
        - is a "reference to an IAM role that will be assumed by the EC2 instance when it starts up"
        📌 Key Points:
           - An Instance Profile is different from an IAM Role, but it contains one.
           - temporarily assume permissions via STS (AWS Security Token Service).
           - AWS automatically rotates temporary credentials, making it more secure than hardcoded credentials.
           
    Note many AWS services can assume IAM roles 
        - EC2
        - Lambda
        - ECS Tasks & Fargate
        - EKS (Kubernetes)
        - SageMaker	Access
        - App Runner
        - CodeBuild
        - Glue

EC2 States
    - pending (powering on, or first launching)
    - running
    - stopping
    - stopped (not being used but can start up)
    - shutting-down
    - terminated (perm delete)
    
EC2 Actions
    You, the user can:
    - Launch
    - Stop
    - Start
    - Terminate
    - Reboot
    - Retire - (notifies instance is scheduled for retirement due to hardware failure or EoL. (rare))
    - Recover (automatcally recovered a failed instance on new hardware if enabled, keeping same instance ID and configs)
    
    EC2 fun fact:
      Via CLI you can prevent the instance from being:
        - turned off via any API 
        - stopped via any API 
        - enable auto-recovery behavior
            - "auto-recovery behavior" = AWS auto recovers an instance if it becomes impaired 
                b/c hardware or software issue on the AWS side.
                - is not a reboot
                - is a relaunch on healthy hardware.
        You can screenshot the EC2's console (for debugging when SSH or RDP (Remote Dekstop Protocol) not working 
        
EC2 Hostnames
    - a dns name is assinged to an EC2
        - public and private given (free both)
    - To prevent new hostnames each reboot You can edit cloud-init 
        `sudo vi /etc/cloud/cloud.cfg` -> preserver_hostname: true
        `sudo hostnamectl set-hostname webserver.mydomain.com`
        `sudo reboot`
    Public:
        ec2-[public-ip-address].[region].compute.amazonaws.com
        ec2-34-220-15-10.us-west-2.compute.amazonaws.com
    Private:
        ip-[private-ip-address].[region].compute.internal
        ip-192-168-1-24.us-west-2.compute.internal
        
    - AWS assigns two types of hostnames to an EC2 instance.
    - you get both
        - IP-Based Hostname ("IP Name") <---- Default
            ip-<IP ADDRESS>.ec2.internal <--- us-east-1
                or
            ip-<IP ADDRESS>.<region>.compute.internal  <--- not us-east-1
            eg)
                ip-10-24-34-0.ec2.internal
                ip-10-24-34-0.ca-central-1.compute.internal
                ip-<IP ADDRESS>.<region>.ec2.internal
        - ID-Based Hostname ("Resource Name")
            i-<RNG ID>.ec2.internal
                or
            i-<RNG ID>.ca-central-1.compute.internal  <-- not us-east-1
            eg)
                i-0123456789abcdef.ec2.internal
                i-0123456789abcdef.ca-central-1.compute.internal
        OR with Instance
        
    Recall:
        - Hostnames are human readable names assinged to computers w/ an IP
        /etc/hostname (or w/e):
            $ <user>@<hostname> 
            - Without a router or DNS, a hostname  is just a label and doesn’t affect anything
            - You can choose your own hostname, but it only matters if something (router/DNS) uses it.
            - Local networks (WiFi, routers) may assign hostnames automatically.
            - Public hostnames are assigned by DNS providers like AWS, Google, and Cloudflare.
    
    hostnames are nice b/c:
        - Ansible, Chef, and Puppet use hostnames for managing multiple servers.
        - Many system logs and monitoring tools use hostnames.
        - Readability & User Experience (Main Reason)
        - CLI name = [whoami]@[hostname]: $
            (whoami = you logged in user)
            (hostname = machine's name)

            - ssh user@web-server
                vs
            - ssh user@ip-192-168-1-100
            or 
            Logs: 
            web-server kernel: New connection from 10.0.1.5
                vs
            ip-10-0-1-5 kernel: New connection from 10.0.1.5
        - ISSUE! Changing the hostname on one computer only affects that machine.
            - for LAN communication, you'd need to: Manually update /etc/hosts or Use Route 53 (Private Hosted Zone)

Recall:
AWS Session Manager 
    - like SSH, opens secure sessions 
        - legit your have a CLI, like in ssh
    - but much more secure and managed by AWS. 
    - no SSH keys
    - no open ports, 
    - no bastion hosts. 
    - Uses: IAM roles and the SSM Agent (SSM Agent is installed on your EC2)


AWS EC2 Burstable Instances (T Instances)
    - T3, T3a, T4g, etc
    - for workloads with variable CPU usage
    - can "burst" above their baseline performance for short periods.
        - No throttle/bottleneck for crazy rng workloads
    - accumulate CPU credits when operating below the baseline
    - (Optional) Unlimited mode: use more CPU, via $$$ if CPU credits are exhausted.
    Use Cases:
        - Web servers 
        - Development and test environments.
        - Small databases and microservices.
        - periodic activity.
    - YES if I am paying for a 1000 hz CPU, and I only use 10% of the CPU for 10 hours, 
        -> I have +90% * 10 hours of Earned credits
    - but you Still Pay for the Instance Even If You Underutilize
    Note:
    - Since min/max tracking CPU usage and money is complex
    - can use:
        Auto Scaling for Variable Workloads
        AWS Compute Optimizer 
            - Recommended, looks at only EC2, Lambda, EBS, and ECS
        AWS Trusted Advisor for Cost Optimization 
            - Covers all AWS services (EC2, RDS, S3, EBS, etc.)
        - analyzes your EC2 usage patterns and recommends better instance types to reduce costs.
      
EC2: Source and Destination Checks
    Scenerio:
    - Suppose your building your EC2 to be a NAT instance, Router, or Firewall. It will:
        -> Traffic goes it
        -> EC2 does NAT-ing, Routing, or Firewall-ing on traffic
        -> Traffic goes out
    - to do this, we must disable "source and destination check"
    - If enabled = The instance only accepts and sends packets where its own IP is the source or desintation
        - It drops any traffic that isn't explicilty meant for it.
    - disabling it allows the instance to forward traffic that does not originate from or terminate at itself.
    Enable on if:
        - you are a web server.
            -> you are NOT a NAT instance, Router, or Firewall
        - (this is default)
    Disable if:
        - you're a NAT, ect...

EC2: System Logs
    - You can find the "system logs" through the AWS Console
        - no need to ssh in
    - found @ EC2 Managment Console
    - optionally: install the cloudwatch unified agent and logs can go to cloudwatch.

EC2 Placement Groups
    - the placement of EC2 instances within the AWS infrastructure 
    - reasons: for performance, high availability, or low latency
    - is "the logical placement of your instances to optimize communication, performacne, or durabiliyt"
    
    - When launching an EC2 instance, under the “Advanced Details” 
    - you can explicitly specify a Placement Group
    CLI:
            $ aws ec2 create-placement-group 
                 --group-name  my-cluster-group 
                 --strategy  cluster
            $ aws ec2 run-instances 
                  --image-id  ami-xyz            |
                  --instance-type  c5n.18xlarge  |
                  --count  1                     V
                  --placement "GroupName=my-cluster-group"
            
    Cluster  <--- least fault tolerant
    Partition
    Spread   <--- most fault tolerant
    
    1. Cluster Placement Group
       (All instances packed close together in same AZ for low latency)

       [AZ-1]
       +-------------------------------+
       | [Instance A] [Instance B]     |
       | [Instance C] [Instance D]     |
       +-------------------------------+

    2. Partition Placement Group
       (Instances grouped into isolated partitions with no shared racks)
       partition = rack, power source, network

       [AZ-1]
       +---------------------------------------------+
       | Partition 1 | Partition 2 | Partition 3     |
       | +--------+  | +--------+  | +--------+      |
       | | A, B   |  | | C, D   |  | | E, F   |      |
       | +--------+  | +--------+  | +--------+      |
       +---------------------------------------------+
       
    3. Spread Placement Group
       (Instances spread across different hardware/AZs to reduce failure impact)

       [AZ-1]       [AZ-2]       [AZ-3]
       +------+     +------+     +------+
       |  A   |     |  B   |     |  C   |
       +------+     +------+     +------+


                  
    1. Cluster Placement Group
        - pack instance inside a AZ
            - cannot be multi AZ
        Use Cases: HPC (High-Performance Computing), big data applications, low-latency, high-throughput.

    2. Partition Placement Group
        - instance are in "partitions"
        - each partition is isolated at the infrastructure level
        - they do not share racks, power sources, or network with other partitions.
        Use Cases: Large-scale distributed workloads like Hadoop, Cassandra, Kafka, or any system requiring isolation.

    2. Spread Placement Group
        - instances are on different racks
            - racks can be in different AZs
            - Limited to 7 instances per AZ.
            - Any # of AZs
        - Maximizes availability, hardware tolerance
        Use Cases: high availability, fault tolerance, minimal risk hardware failure 
            (eg critical applications, database nodes).
            
            
EC2 Connect
    - securely connect to an Amazon EC2
    - multiple ways to do so:
    1 SSH Client
        - old school, private and public keys
        - from local machine, port 22 open
    2 EC2 Instance Connect
        - short lived SSH keys controlled by IAM policies
        - works only with linux and not all instances
        - use aws CLI (see below)
    3 Session Manager
        - Temporary, browser-based or CLI-based SSH access via AWS Console
        - "SSH" access via AWS.
        - no open ports, aws magic
  
    4 Fleet manager Remote Desktop
        - RDP within the web-browser
        - Windows only
    5-ish EC2 Serial console
        - troubleshoot hardware issues
        - hardcore
            - low-level access to the serial port (ttyS0) 
            - fix boot or network connectivity
        - without  SSH or RDP
        
    SSH Example:
        1. aws ec2-instance-connect send-ssh-public-key \
            -- instance-id i-033ee47a499a786a0 \
            -- instance-os-user ec2-user \
            -- availability-zone ca-central-1a \
            -- ssh-public-key file://ec2connect.pub

        2.   ssh -i ec2connect ec2-user@3.99.178.17
        
EC2: Amazon Linux Extras
    - is a CLI tool
        $ amazon-linux-extras list
    - you can install newer or additional software packages that are not in the default Amazon Linux 2 distro
    - is not a package manager itself 
        - it's on top of the existing package manager (yum) that's specific to Amazon Linux 2.

EC2: ENA (Elastic Network Adapter) can be enabled
    - makes Internet fast = 100Gbps
    
    
AWS AMI (Amazon Machine Image)
    - pre-configured template to launch EC2
    Contains:
        1 Root Volume Template
            - Operating System (OS) 
            - applications 
                - Pre-installed packages, configurations. 
        2 Launch Permissions
            - public, private
            - which AWS accounts
            
        3 Block Device Mapping
            - Defines volumes to attach to the instance at launch.
                - EBS volumes and/or instance store volumes

    Types of AMIs:        
        Public AMIs 
            – Available to all AWS users. 
        Explicit 
            - Private AMIs where the owner grants specific AWS accounts permission to use them.
        Implicit 
            - Private AMIs that are only accessible within the owner's account (default setting).

    AMI Lifecycle:
        Deprecated AMI
         - no longer recommended for new deployments.
        Disabled AMI
         - Prevents new instance launches while keeping the AMI intact.
            - (you can return to it, re-enable)
         - The AMI still exists, and existing instances aren't affected.
        Deregistered AMI
         - permanently removed from AWS.
         - Existing instances remain unaffected but can't be re-launched if terminated.

    Create an AMI (from ec2):
        $ aws ec2 create-image --instance-id i-0ace245ddca5d8aa3 --name "MyAmi-000"
    Copy an AMI into new region, encrypted
        $ aws ec2 copy-image 
            --source-region us-east-1  
            --source-image-id ami-06bb02361dd3b8449 
            --name "My copied AMI" 
            --region ca-central-1 
            --encrypted
            
    AMI Fun Facts:   
       - private and public AMIs exist 
            - some free, some $$$ on marketplace
        - !!! AMIs are region specific 
        - ENA (Elastic Network Adapter) can be enabled
            -> ZOOM makes Internet fast = 100Gbps
        - AMIs helps track *incremental changes*
            - OS, app code, and system packages
            eg)
                web-server-000
                web-server-001
                web-server-002
            - AMIs are used with "Launch Configurations/Templates" to manage revisions
        
        AMI System Manager Automation
            - routinely patch your AMIs "and bake those AMIs"
            
        AMI vs. Snapshot:
            - Snapshot is NOT like VMWare fusions snapshots!
            - Snapshot is just for EBS, not the vanilla EC2 storage, eg Instance-Store/Ephemeral Storage
                  - EC2 is designed as stateless compute. 
                  -> Persistent state should live in EBS or S3, not in instance memory or ephemeral disks.
                - Alernatively, stop the instance → take an AMI
                    - or Use AWS Backup
            - AMI includes OS & configurations; used for launching instances.
            - Snapshots are backups of EBS volumes (storage).
            - AMIs enable quick scaling, consistent deployments, and easy disaster recovery in AWS.
        
        - 2 boot modes:
           - BIOS (Legacy Mode)
           - UEFI (modern)
           
        - AMI storage can be EBS or Instance
            - (or as they call it, "EBS-backed" and "Instance-backed")
            recall: 
                EBS - storage is independent of instance. A terminated EC2 won't lose data
                Instance storage - you know, storage on the ec2, gg on terminated ec2

        - you can create an AMI from an EC2 instance that's running or stopped :O
            - if running you risk issues if your app is doing something, like middle of writting files
            - can copy an AMI across to another region
                - since AMIs are region specific, you prob need to
            - can encrypt AMI

        - you can store your AMIs into your s3 bucket, and restore from it
            pros:
                save money
                portability - you can move your VM image/AMI to on-prem or to offline env
                compliance
                disater recovery

    AMI Marketplace
        - you can purchase subscriptions from vendor maintaied AMIs
        -> *Secuirty Hardened* AMIs are popular
            eg) Center of Internet Secuirty (CIS) AMIs
        !! note AMI IDs will differ region to region
            eg) "Amazon Linux 2023" ami-0440d3b780d98b29d <--- us-east-1
                "Amazon Linux 2023" ami-015b5b1643fdfee5c <--- ca-central-1
        States of purchased/sold/maintained AMIs:
            Deregister
                - perma delete
                - running instances remain
                    - you will still be charged
                - snapshots remain
                    - you will still be charged for snapshots as though they AMIs
                - when you dont want to allow new instances to be launched
            Disabling
                - prevents the AMI from being used. It can be reenabled.
                - disabled = prevents instances from using it. cannot lauch new instance
            Deprecating
                - indicate that AMI should not be used
                - they do not appear in Market place listings
                - but existing users/launch-templates/Auto Scaling groups, 
                    can continue to use a deprecated AMI via its ID. 
                - you must "Deregister" it to prevent usage

    Launch Permissions
        public - anyone can launch the AMI
        explicit - specific AWS Accounts, Orgs, or Org-Units can launch the AMI
        implicit - the owner can launch
    Virtualizaton Types
        Hardware Virtual Machines (HVM)
            - modern
            - faster
            - hardware technology connects host with system's CPU
        ParaVirtualizaiton (PV)
            - relies on hypervisor to simulte hardware
            - old and shitty
            - you'll never use it
                
         
Auto Scaling Groups (ASG)
    - manages a group of EC2 instances, ensuring automatic scaling based on demand.
    - Works with Elastic Load Balancer (ELB) to distribute traffic 
    - a minimum number of instances are always running
    - needs an AMI, for scaling
    
    ASG Configs:
        Min size
            - EC2 instances at least running
        Max size
            - EC2 instances at most running
        Desired Capacity
         - "initial capacity"
         - ASG will rarely go to min. such times it will is:
            - If you set up a Target Tracking Scaling Policy (eg "keep CPU at 50%"), 
                AWS will reduce Desired Capacity if CPU usage is too low.
            - Manual Changes Lower Desired Capacity
            - Instance Health Checks Fail
            - Spot Instances, AWS can reclaim them at any time.
            
    - Automatic scaling occurs via policies:
    Policies:
        1 Simple Scaling
            - add/removes based on certain CloudWatch Alarms, once triggered
                - Cloudwatch metrics
            - slow. "Cooldown" period, waits for each action to complete before the next.
                - If you dont want cooldown, use Step or Target scaling
            - (is a Dynamic scaling Policy)
            - Example: If CPU > 80%, add 1 instance.
        2 Step Scaling
            - Step Scaling is essentially an enhanced version of Simple Scaling.
            - add/removed based on Cloudwatch metrics (also)
                - fast. no wait.
                - slighlty more complex
            - (is a Dynamic scaling Policy)
            - Example: If CPU > 70%, add 1 instance; if CPU > 90%, add 3 instances.
        3 Target Tracking Scaling
            - *Maintains* a specified metric (e.g., CPU utilization) like a thermostat.
            - (is a Dynamic scaling Policy)
            - Example: Keep CPU at 50% by auto-scaling.
        4 Predictive Scaling
            - Uses machine learning to forecast demand and scale preemptively.
            - Proactively adjusts capacity, rather than reactively (like Step Scaling).
            - Best for: Predictable traffic spikes (e.g., daily workload patterns).
            Modes of Predictive Scaling
                ForecastOnly 
                    - Generates predictions but does NOT adjust capacity.
                    - so you can evaluate the forecast before scaling 
                        - view in the Amazon console. 
                    - You dont click a "scale now" button, its just for viewing and prediction
                        - instead, you switch to "ForecastAndScale"
                ForecastAndScale 
                    - Predicts and automatically scales instances.
            Data
                Historical Data Lookback	
                    - 14 days (recommended), minimum 24 hours
                Prediction Window	
                    - 48 hours into the future (you cannot adjust, AWS fixed always)
                Scaling Adjustments	
                    - Every 5 minutes, adjusts capacity based on predicted demand.
    Note:
        Cooldown
            - If you disable cooldowns or set them to 0
            -> it wont scale wildly
            - It still monitors metrics (like CPU usage) at CloudWatch intervals (eg 1-minute)
            - evaluation periods: scaling policies still check  "CPU > 70% for 2 consecutive periods"

    -ASGs can be used with EC2s. And with ECS and EKS, like as a wrapper.
        - not Fargate
    Health Checks
        - when instances are unhealthy, then ASG replaces them
        - You configure the Health Check on the EC2, or ELB, ect, NOT the ASG
        - ELBs are good b/c reduce stress on EC2 
            - AND b/c you can use the ELB's healthcheck instead
    
    Termination Policies:
        - determine which instances to terminate first when scaling down.
        - AWS has predefined polices:
            - Default
            - AllocationStrategy
            - OldestLaunch Template
            - OldestLaunchConfiguration
            - ClosestToNextInstanceHour
            - NewestInstance
            - OldestInstance
        - you do NOT write your own termination policy from scratch. 
            - But you can go ham with a Lambda if you need to.
                - lambda can analyze instance metics, query oth aws services (DynamoDB, s3. ect
                
                
Elastic Load Balancer (ELB)
    - automatically distributes incoming traffic across multiple targets
    - improves availability, fault tolerance, and scalability.    
    - distributes onto EC2, ECS, Fargate, and EKS instances
    - ELB is a *suite of load balancers* 
    - can span multiple AZs
    
    Types of ELBs
    
        Application Load Balancer (ALB)
            - Operates at Layer 7 (Application Layer of the OSI model).
                - for HTTP and HTTPS traffic.
            - You can configure ALP to authenticate users 
            - Supports WebSockets and server name indication (SNI).
                - WebSockets & gRPC
                - for real-time applications, (but dont use for high performance apps).
            - can use AWS WAF (Web Application Firewall).

            Has routing :
                - eg. directing requests to different services based on URL paths

                1 Path-Based Routing
                    - /api/* → API service
                    - /shop/* → Shop service
                2 Host-Based Routing
                    - api.example.com → API backend
                    - blog.example.com → Blog service
                3 Query String & Header-Based Routing
                    - query string: ?type=premium
                    - headers (User-Agent: mobile)
            Use Cases:
                Microservices & Containers
                    - since path-based and host-based routing good with ECS, EKS, and Kubernetes.
                API Gateway Alternative
                Web Applications
                    - sticky sessions, WebSockets, and SSL termination.
                Authentication & Security
                Multi-Domain Hosting
                    - route based on hostnames (shop.example.com → Shop Service, blog.example.com → Blog Service).
                    
        Network Load Balancer (NLB)
            - Operates at Layer 4 (Transport Layer).
                -> NOT HTTP!!
            - TCP, UDP, TCP_UDP, and TLS protocols traffic.
            - Capable of handling millions of requests per second with ultra-low latency.
            - Best suited for applications requiring high performance and static IP addresses.
            Use Cases:
                High Traffic Applications
                Gaming & Financial Applications
                Hybrid Cloud or On-Prem Integration:
                    - Can distribute traffic to IP addresses, including on-premises servers.
                Fixed Static IP Addressing
                    - Unlike ALB, NLB provides a static IP per Availability Zone.
                VoIP & Real-Time Streaming
                    - Supports UDP traffic for voice/video streaming applications.
                Non-http traffic
                    
        Gateway Load Balancer (GWLB)
            - Operates at Layer 3 (Network Layer).
            - Routes traffic to third-party security appliances like firewalls and intrusion detection systems.
            - Provides a single entry and exit point for traffic monitoring and inspection.
            - designed to work with third-party security appliances
            Use Cases
                Firewall-as-a-Service (FWaaS)
                Intrusion Detection/Prevention Systems (IDS/IPS) 
                    → Detect suspicious behavior in real-time.
                Deep Packet Inspection (DPI) 
                    → analyze network traffic.
                DDoS Protection
                Regulatory Compliance(eg. PCI-DSS, HIPAA)
                
        Classic Load Balancer (CLB)
            - Layer 4 (TCP) and Layer 7 (HTTP/HTTPS) 
            Limitations:
                No advanced routing like ALB.
                No support for WebSockets.
                No static IPs like NLB.
                AWS recommends using ALB for HTTP-based applications and NLB for high-performance network traffic.
        Pricing based on:
            - ELB type.
            - Number of Load Balancer Capacity Units (LCUs).
            - Data processing and outbound data transfer.
            
    Traffic:
        managed via:
            1 Listeners 
                - entry point
                - checks incoming client connection requests.
                - Each ELB must have at least one listener.
                - Listener configured with:
                    A protocol (HTTP, HTTPS, TCP, UDP, TLS).
                    A port (80 for HTTP, 443 for HTTPS).
                    optional set of rules that determine traffic routing.
            2 Rules 
                - "Rules" apply ONLY to ALB (because ALB operates at Layer 7 and can inspect requests).
                - logic for routing
                - which target group should receive incoming traffic.
                Rules:
                    Path-based (/api/* → API backend)
                    Host-based (shop.example.com → Shop backend)
                    Query string (?version=beta → Beta backend)
                    Headers (User-Agent: Mobile → Mobile backend)
                    HTTP methods (POST requests → Specific backend)
            3 Target Groups 
                - where traffic is sent
                - receives traffic from an ELB.
                - is a logical grouping of backend instances, containers, IPs, or Lambda functions 
                - ELB routes traffic only to healthy targets (via health checks).
                - Target options:
                    EC2
                    ECS Containers (Fargate or EC2 mode)
                    IP Addresses (for on-premises or multi-cloud targets) 
                    AWS Lambda Functions 
                - Classic LB's have no Target Group
                    -> feature unavailable for CLBs
                
        - ELBs use traffic rules to determine distribution
        - rules vary based on the type of ELB


Route 53
    - is a domain name service (DNS)
    - Buy domains
    - domain registration, 0
    - DNS routing 
    Feature:
        - you can route Domains/sub-domains to AWS resources (EC2s, API GW, Cloudfront, Lambda, IP, ect)
        
    Record Sets 
        - are a collection of records which determine where to send traffic
        The following record types supported by Route53
            . A record type
            · AAAA record type
            · CAA record type
            · CNAME record type
            . DS record type
            · MX record type
            . NAPTR record type
            . NS record type
            . PTR record type
            · SOA record type
            . SPF record type
            . SRV record type
            · TXT record type
    Hosted Zones
        - "Record sets" eg, MX, TXT, CNAME...
        - Hosted zones hold information about how to route traffic on the internet for a domain
            - (like example.com and its subdomains mail.example.com)
            Hosted Zone = bski.one
            Hosted Zone = twitchtranscripts.com
            - the Records define what to do with "resume.bski.one" or with "MX @ captions.bski.one"
            
        - ???"is a container for record sets, scoped to route traffic to specific domains or subdomains"
        - HZs holds the DNS records for a domain or subdomain
        -  DNS container:
            - you manage how traffic is routed for that domain.
            
        a) Public Hosted Zone
            - publicly accessible
            - how you want to route traffic from the outside internet
            - Maps domain-name → IP addresses (eg, www.example.com → 192.0.2.1).
            
        b) Private Hosted Zone
            - how you want to route traffic within a AWS VPC
            - internal DNS resolution.
            - not public internet
            - Helps manage private services, such as databases or microservices running within AWS.
            
        Key Components of Hosted Zones
            - Hosted Zone ID 
            - NS Records (Name Servers)
                - when public, Route 53 assigns four name servers to each public hosted zone.
            DNS Records
                - You configure A, CNAME, MX, TXT, and other records within a hosted zone.

    Alias Record 
        - AWS feature that extends standard DNS.
        - is a special type of DNS record that allows you to map a domain name to an AWS resource 
            - eg. Elastic Load Balancer, CloudFront, S3 bucket, API Gateway.
            - without requiring an IP address.
            eg) example.com --> s3-website-us.east-1.amazonaws.com
            ✅ No Extra DNS Query (Faster Performance)
            ✅ Automatically Updates for AWS Resources
            ✅ Free
           
    Route 53 Traffic Flow (R53WS) 
        - (Route 53 Web Service?)
        - an advanced traffic management feature
        - sophisticated routing
        - allows you to create routing policies for user traffic based on various conditions, 
            - latency, geolocation, weighted distribution, and failover.
        - is a visual editor

    Routing Policies
        - controls how DNS queries are resolved. 
        - how traffic is directed
        
        Note:
            - you cannot stack routing policies on a single record
                - such as Failover Routing with other policies like Geolocation, Latency, or Weighted
        
        1. Simple Routing
            Maps a domain to a single endpoint (eg, an IP address, S3 bucket, or ELB).
            - This is the default you don't select "Simple Routing"
            - 1 record (www.exmaple.com) to 1+ IP addresses
                - user will be directed to a random IP, if you specify 2 or more
                    value: 24.123.41.122
                    value: 4.223.123.94
                    value: 59.95.4.55
        2. Weighted Routing
            traffic across multiple endpoints based on assigned weights.
            - can test out experimental features
            - sorta blue/green deployment
                Server A (Weight: 70) → 70% traffic
                Server B (Weight: 30) → 30% traffic
                    value: 24.123.41.122    weight: 10
                    value: 4.223.123.94     weight: 20
                    value: 59.95.4.55       weight: 30
        3. Latency-Based Routing
            Directs users to the AWS region with the lowest latency.
            - you'll create multiple A-Record, same DNS name (www.example.com)
            - Each A-Record points to your app (duplicated, deployed in different regons)
            - AWS DNS magic handles the quickest route            
                US users → US-East-1 (Virginia)
                Europe users → EU-West-1 (Ireland)
        4. Geolocation Routing
            - the location that user/DNS-queries originate from
                - based on THEIR geographic location (continent, country, or state).
            - you'll create multiple A-Record and specify the County code.
            for compliance or localization
                US users → US server
                UK users → UK server
        5. Geoproximity Routing (With Bias)
            - routes to closest resource. 
            - optionally route more or less traffic to a resource via a bias. 
                - expands or shrinks volume per
            - CANNOT use CLI.
            - Must use Route 53 Traffic Flow (R53WS) see above.
                - the UI thing in AWS Console
                - weird colorful "puzzle-ish" map
        6. Failover Routing
            If the primary resource fails, it automatically switches to a backup resource.
            Disaster recovery, ensuring high availability.
            - you create 2 A-Records, same DNS name (www.exmaple.com)
            - 1st A-Record points to primary
            - 2nd A-Record points to secondary
            - AWS auto does health checks 
        7. Multi-Value Answer Routing
            - it will direct the user to a random, healthy IP address
            - you create an A-Record with multiple IP addres 
            -> just like in Simple-Route
                - BUT AWS will health check the servers
            - Like Simple Routing but with a health check
            - Like Failover but Failover is just 1 primary, 1 secondary
            NOTE
            Multi VS Failover
                - Multi is lower level, "check if DNS is working, this IP should resolve"
                - Failover is smart "run a health check on the EC2 or ALB"
                
    Route 53 Health Checks
        - can do health checks on the dns level
        - option, CloudWatch Alarms if unhealth
        
    Route 53 Resolver
        - ON PREMISE ON PREMISE ON PREMISE ON PREMISE ON PREMISE 
        - a DNS server that resovles DNS queries between your on-premise network and your VPC
        - on premise to vpc stuff
        - enabling seamless name resolution between AWS resources and on-premises networks.
        bound & Outbound Query Resolution
        - Inbound:  Resolves DNS queries from on-premises to AWS VPCs.
        - Outbound: Resolves DNS queries from AWS VPCs to on-premises or external DNS servers.
        
    DNSSEC with AWS Route 53
        - good idea to use it. Prevents ppl from impersonating you.
        - DNSSEC (Domain Name System Security Extensions) 
        - a security feature 
        - protects domain name resolutions from spoofing and tampering by ensuring DNS responses 
            are digitally signed.
        - public hosted zones only (not private, make sense)
        - Meets compliance
        
    AWS Zonal Shift
        - ! If AZ A is bad, route to AZ B instead
        - a feature in "Route 53 Application Recovery Controller (ARC)"
        - shift traffic away from a bad AZ 
        - gives high availability and disaster recovery.
        - Zonal Shift does NOT support:
            - ALB behind AWS Global Accelerator
            - Other aws services,  eg EC2, RDS, or S3 directly
        - Zonal Shift only works with:
            Application Load Balancers (ALB)
            Network Load Balancers (NLB)
            
    Route 53 Profles
        - apply and manage DNS realted Route 53 configs accorss many VPCs in different AWS accounts
        - Sharing configs accross many VPCs
        ???

| Feature                        | Weighted Routing (Route 53)                                       | Elastic Load Balancer (ELB)                                         |
|-------------------------------|--------------------------------------------------------------------|---------------------------------------------------------------------|
| Traffic Distribution          | Routes traffic at the **DNS level** based on weight               | Distributes traffic at the **network level**                        |
| Use Case                      | Directs traffic to different servers, regions, or AWS services     | Balances traffic **within a single region**                         |
| AWS Dependency                | Can route traffic to **AWS and non-AWS endpoints**                | Works **only with AWS resources** (EC2, ECS, Lambda, etc.)          |
| Cross-Region Load Balancing  | ✅ Yes, supports **global traffic routing**                        | ❌ No, ELB only works within a **single region**                     |
| Custom Traffic Splitting      | ✅ Yes, set **custom weights** (e.g., 70/30 split)                 | ❌ No, automatically distributes traffic evenly                      |
| Failover Support              | ✅ Can define **primary and secondary failover paths**            | ✅ Can handle failover **within a region** but not globally         |
| Performance Impact            | DNS-based, **faster at initial resolution** but doesn’t check...  | Network-based, **actively checks health and distributes in real-time** |
|                               |   ...real-time server health                                      |                                                                     |


AWS Global Accelerator
    - a networking service that routes traffic through the AWS global network instead of the public internet.
    - super fast via Edge
        - has a speed comparison tool for you to try
    - Supports TCP and UDP
    - Operates at network layer (4)
    - You get 2 static IP addresses
        - for redundancy/failover-support
        - they are not region specific
        - they are fixed entry points to your app 
        - you should associate the AWS Global Accelerator IPs (says chatgpt)
    fun fact: 
        - when you delete an accelerator, you lose the Global Accelerator static IP addresses
    - two "types" of Accelerators
    Use case:
     - Ideal fo non-HTTP cases such as gaming, IoT, VoIP, says "Digital Cloud Training
    
    1 Standard Accelerator
        - directs traffic over the AWS global network to endpoints that you include in specified AWS Regions 
            - Routes to nearest healthy AWS endpoints (eg, ALB) across multiple AWS regions.
        - endpoints could be: Network LBs, ALBs, EC2s,Elastic IP addresses 
    2 Custom Routing Accelerator
        - lets you use custom application logic to direct users to a specific destination
        - Routes directly to specific EC2 instances
            - Used for apps needing fine-grained traffic control, like gaming or VoIP. 
        - btw, Standard Accelerators cannot route multiple users to a specific EC2 behind your accelerator
            -> When you want game servers based on geo location, or w/e
            -> when you ant VoIP
        - how to: you direct users to a unique port on your Accelerator. The Accelerator will map the port to a specific EC2
        
    Traffic:
        - someone makes a request to you anywhere in the world
        - Instead of going through multiple random public ISPs, the request first reaches the nearest AWS Edge Location.
        - AWS Global Accelerator best path 
            - latency, health chekcs, custom traffic policies
            - fiber optics. fastest, lowest-latency path
        - Response travels back through AWS’s private metwork

    Things: (prob not important?)
        1. Listeners
            - checks for incoming traffic on a specified port and protocol (e.g., TCP, UDP).
            eg. 
                Listener 1: TCP on port 80 (HTTP)
                Listener 2: TCP on port 443 (HTTPS)
                Listener 3: UDP on port 12345 (for real-time gaming
        2. Endpoint Groups
            - collection of endpoints 
            - traffic distribution (eg 70% to US, 30% to Europe).

        3. Endpoints
            - actual resources that handle requests within an endpoint group.
            eg
                Endpoint Group 1 (North America) → EC2 Instances in US-East
                Endpoint Group 2 (Europe) → Network Load Balancer in EU-West
        
AWS Cloudfront
    - AWS CDN
    - "a distributed netowrk of servers that delivers web pages and contnet to users base
        on their geographical location"
    - Used to deliver:
        Static content
        Dynamic content
        Streaming Videos
        Web Sockers
    - Caches exist on TWO layers
        - AWS handles what goes where, you configure nothing.
    1 Edge locations 
        - cache closest to user
        - fastest delivery.
    2 Regional edge 
        - caches kinda close,
        - fast-ish delivery 
        - less frequently requested
    - If content missing at edge location
        -> CloudFront checks regional cache 
        -> else fetches from the origin.   
        
    CloudFront
        User <---> CF Edge Location <---> CF Region Cache <----> Origin
        Edge Location + Region Caches = Distribution = colleciton of Edge & Regional

    Cool side logic:
        1 Lamdba@Edge
            - override/modify reqeusts and Responses
            - At Regional Cache
            Specs
                - max time 15 min
                - memory 128MB - 3GB
                - 5 to 30 seconds
                - max 10,000 requests / second
                
            Choose where it runs:
                - Viewer Request
                - Viewer Response
                - Origin Request
                - Origin Response
                
            Viewer Request  ---> |CloudFront| ---> Origin Request ----> Origin (your server)
                                                                         |
            Viewer Response <--- |CloudFront| <--- Origin Response <-----+
                                                                    
        2 Cloudfront Function
            - lightweight
            - At Edge Locations
            Specs:
                - time ~1 millisecond
                - memory 2mb
                - max 10,000,000 requests / seond
            - Use Cases: header manipulation, rewrites

            Choose where it runs:
                - Viewer Request
                - Viewer Response
                
            Viewer Request  ---> |CloudFront|
            Viewer Response <--- |CloudFront|
        
        
#####################################
#####################################
######                          #####
######         STORAGE          #####
######                          #####
#####################################
#####################################        

Elastic Block Store (EBS)
    - "for attaching persistent block storage volumes to EC2 instances"
    - is raw storage attached to your server.
    - for EC2 instances. 
    - storage similar to traditional hard drives, allowing fine-grained control over data.
    - SSD volume 1  GiB - 16 TiB
    - HDD volume 16 GiB - 16 TiB
    - IOPS eg (16 KiB I/O) Does not scale linearly to Max Throughput (though in theory it will)
        - but EBS enforces hard limits
        - b/c AWS limits it for their own infrastrusture reason
    Features
        Persistence
        Scalability
        Backup & Snapshots
        Performance Tiers 
            - different volume types
        Encryption & Security
        Availability & Durability
    Types:
        General Purpose SSD
            - gp3
            - gp2
        Provisioned IOPS SSD
            (fast input/output)
            - io1 
            - io2 Block Express
            - io2 (deprecated, replaced by Block express)
        Cold HHD
            (lowest cost HDD volume)
            - sc1
        Throughput Optimized HHD
            (sc1 ^ but faster)
            - st1
        Magnetic
            (previous generation HDD)
            - standard
    Recall
        Hard Disk Drive (HDD)
            - Good at throughput (moving continous data from a -> b)
            - Poor at IOPs (randomly writing a to b, c to d, e to f)
            - RPMs (Revolution Per Minute) more = faster = more money $
        Solid State Drives (SSD)
            - no physcial moving parts
            - great I/O
            - like "flash memor"
            SATA SSD, NVMe SSD, M.2 SSD, U.2 SSD, Portable SSD, PCIe SSD
        Magnetic Tap
            - old school, like cassette tapes
            - durable (30 years)
            - cheap
            
        RAID (Redundant Array of Independent Disks)
            - fault tolerance
            - combines multipel physical volumnes into one "logical unit".
            - data storage virtual technology for HHD (magnetic disks)
            - used in servers and data centers
            
            RAID 0 (Striping)
                - Data is split across multiple HDDs but treated like 1 drive
                Pros: Speed
                Cons: No redundancy
            RAID 1 (Mirroring)
                - Data is duplicated on two or more HDDs. 
                Pros: has redundancy
                Cons: Storage capacity is halved
            RAID 5 (Striping with Parity
                - Data and parity (error-checking) are distributed across at least 3 HDDs.
                - Good performance and redundancy
            RAID 6 (Striping with Double Parity)
                - RAID 5 but with double parity for extra fault tolerance (requires at least 4 drives).
                - two disks can fail, and everything still good
            RAID 10 (RAID 1 + RAID 0)
                Combines RAID 1 (mirroring) and RAID 0 (striping)
                High performance and redundancy.
                - fast read/write, data redundant
                - storage capacity is halved
                - if 2 drives in the same mirror fails, data is lost
    Multi-Attach 
        - a single EBS volume attached to multiple EC2 instances simultaneously. 
        - for distributed applications that share the same data. ( SQL Database)
        -  for io1 and io2 volumes
        - up to 16 EC2 instances within the same Availability Zone (AZ).
        - simultaneous writes must be handled at the application level to avoid corruption 

    Boot Volume 
        is an EBS volume that contains the operating system and is used to start an EC2 instance.

    NVMe Reserve 
        - the reserved portion of an NVMe-based EBS volume for system metadata and management.
        - persistent storage that remains even if the EC2 instance is stopped or terminated.
        Giga
            1 GiB (Gibibyte) = 1,073,741,824 bytes (2^30 bytes, base 2)
            1 GB (Gigabyte) = 1,000,000,000 bytes (10^9 bytes, base 10)
            1 GiB ≈ 1.074 GB
            1 GB ≈ 0.931 GiB
        Tera
            1 TiB ≈ 1.10 TB
            1 TB ≈ 0.91 TiB
        
    Elastic Block Store (EBS) are cool
        ✅ Persistent Storage 
            – Data is retained even when an EC2 instance stops.
        ✅ Scalable 
            – Can increase storage size without downtime.
        ✅ High Availability & Durability
            – Replicated across multiple Availability Zones (AZs) for redundancy.
        ✅ Snapshots & Backup 
            – Create EBS snapshots for backups or AMI creation.
        ✅ Encryption 
            – Supports AES-256 encryption for security.
        ✅ Attach & Detach 
            – EBS volumes can be attached/detached from EC2 instances.
        X Shrink
            - you cannot decrease the size of an EBS volume after increasing it
            - workaroudn via snapshot it, then create smaller volume from snapshot, mount it

AWS EFS (Elastic File System)
    - yes "System" not "Storage"
    - A native AWS cloud file system.
    - Used primarily with EC2 instances or AWS services that need shared storage.
    - network file system, NFS
    - for AWS services (EC2, Fargate, Lambda) and on-premises resources.
    - Elastic Scaling 
        – Automatically scales up or down as data is added or removed (up to PETA-bytes)
    - Multiple EC2s can use same single EFS, (must be same VPC)
    - charged on Storage usage
    - you will "mount" the EFS to your systems
    
    NOTE
        AWS EFS vs Storage Gateway
        - EFS when you're working within AWS and need a file system.
        - File Gateway when you want to connect your on-premises file-based applications to cloud storage (S3)
        
    Two ways to mount an AWS EFS file system:
        1 Using the standard Linux mount command with an NFS client (manual setup).
            $ sudo apt install nfs-common
            $ sudo mount -t nfs4 fs-example.efs.us-west-2.amazonaws.com:/ /mnt/efs
        2 Using amazon-efs-utils (recommended for automation, encryption, and better integration).
            - https://github.com/aws/efs-utils
            - Linux only, not windows
            
            EFS Client
                - a package to mount and interact with EFS easily.
                1 Install client (amazon-efs-utils) on Linux.
                    $ sudo yum install amazon-efs-utils
                2 Create a mount directory (/mnt/efs).
                    -`t efs`
                        - use the EFS-specific mounting 
                        - amazon-efs-utils required
                3 Mount the EFS file system using the mount.efs command.
                4 Automate mounting by adding an entry in /etc/fstab.


AWS FSx
    - "File System X"
    - a file storage built on popular file systems. 
    - run file systems without having to manage the underlying hardware or file server software.
    - pretty fast
    - scalable
    - integrates with AWS and/or  on-premises environments.
    -> EFS does not natively support Windows, Lustre, NetApp, or OpenZFS.
        - FSx is the solution
        - EFS supports NFS protocol
        
    BTW - AWS Storage Gateway's "FSx File Gateway" is RIP.
    
    - high performance!
    - feature rich
        - FSX for NetApp ONTAP
            - proprietry enterprise storage
        - FSX for OpenZFS
            - storage platform by Sun Microsystems, open source
        - FSX for Windows File Server (WFS)
            - for windows
        - FSX for Lustre
            - for paraelle computing, open source

    EFS (vs) FSx
        File System Type	
            AWS FSx - Supports Windows File Server, Lustre, NetApp ONTAP, and OpenZFS
            AWS EFS - Native NFS-based file system
        Protocol Support	
            AWS FSx - Depends on the FSx type (SMB, NFS, iSCSI, Lustre)	
            AWS EFS - NFS (Network File System) v4 
        Best For	
            AWS FSx - Windows workloads, high-performance computing (HPC), hybrid cloud storage	
            AWS EFS - Linux-based workloads, containerized apps, web servers
        Performance	
            AWS FSx - High throughput and low-latency options	
            AWS EFS - Scalable but lower latency compared to FSx for Lustre
        Use Cases
            AWS FSx - Windows-based applications, HPC, hybrid storage with NetApp, AI/ML workloads	
            AWS EFS - Shared storage for Linux apps, containerized workloads, DevOps

    FSx use cases:
        A file system optimized for Windows 
            -FSx for Windows File Server
        High-performance computing (HPC) or machine learning workloads 
            -FSx for Lustre
        Enterprise-grade hybrid cloud storage 
            -FSx for NetApp ONTAP
        A ZFS-based reliable and performant file system 
            -FSx for OpenZFS

    EFS if you need:
        - serverless architectures, containers, or web applications
            - Shared storage (EKS, ECS, Kubernetes).
        - Simple; automatic scalable storage

    Recall:
        - AWS Storage Gateway is different b/c:
            - Hybrid cloud storage (extend on-premises storage to AWS)
            - connects on-premises environments with AWS storage
            - does NOT include FSx or EFS

    ZFS 
        - Zettabyte File System
        - Enterprise storage (databases, big data).
        - Cloud-based and hybrid storage 
        - Snapshots (Backup & disaster recovery)
        - NAS (Network-Attached Storage) systems like TrueNAS.

    Recall:
        Managed file systems
            - any service where the cloud provider does
                setup, scaling, maintenance, security, backups, and performance tuning
            - users don’t manage the infrastructure 

Amazon File Cache
    - a high speed cache 
    - its like a Network File System (NFS), but fast
    - linux only (POSIX compliant)
    - Files/cache is stored in AWS File Cache (not your local machine)
    - sub millisceocnd latencies
    - 1st, you set up Amazaon File Cache in aws
    - 2nd, you run linux commands to connect to the cache
        $ sudo mkdir -p /mnt/filecache
        $ sudo mount -t nfs4 -o nfsvers=4.1 <cache_dns_name>:/ /mnt/filecache
        
    - Once you mount to your storage (FSx, or S3, EC2 if your crazy)
        -> s3://my-render-assets
        -> /mtn/filecache/
        -> GET s3://my-render-assets/some/file
        Your app: 
            open("/mnt/filecache/some/file", O_RDONLY)
    - On first call to /some/file
        AWS File Cache checks for /some/file
        Not found → Pulls from S3 → Caches it → Returns to your app.
    - On next calls to /some/file
        File Cache sees it's already cached → Serves it immediately
        Done.
    Use Cases:
     High-Performance Computing (HPC)
     Rendering Farms
     ML
     hybrid cloud workflows
     
     
AWS Backup 
    - automates data protection across AWS and hybrid environments. 
    - automated scheduling, encryption, cross-region storage, and compliance. 
    - a LOT of services
    - automate the backup of data across various AWS services
    - Backs up the entire instance/database/tables ect. It does the WHOLE thing essentially.  
        You dont point to files
    Backup Plan
        - a policy defining schedule, dinwosw, lifecycle.
    Backup Vault
        - where the data is stored

    Key benefits:
        ✅ Automated
        ✅ Cross-Region
        ✅ Security & Compliance (HIPAA, GDPR, SOC 2)
        
        
#####################################
#####################################
######                          #####
######         SNOW FAM         #####
######                          #####
#####################################
#####################################        

        
AWS Snow Family
    - compute devices for data migration, edge computing, and/or storage in environments where cloud connectivity is limited or nonexistent.
    - my def: remote "cloud-like" computers
    - compute devices used to physically move data in/out of the cloud
    -> when moving data over the internet or private connection is too slow/difficult/costly

    Snowcone 
        - small and portable 
        - edge computing and data transfer 
        Specs:
            - 8 or 14TB
            - 5 lbs / 2kg
            - Battery powered (6 hours)
            - 1 Variant (2nd variant is just bigger SSD 14tb)
        - sends data back to AWS via:
            1. physcially shipping the device back to AWS
            2. aws DataSync, which runs on the device
        Use Cases:
            - remote locations (oil rigs, military zones, disaster recovery sites)
            - Securely transferring data to AWS from limited-bandwidth areas
            
    Snowball Edge
        - Snowcone but larger
        - but also: 
            LCD Display
            can be used in a cluster (3-16 nodes) 
            data transfer (Http/s, and NFS)
        - 5 Variants
            storage optimized v1 (80TB)
            storage optimized v2 (210 TB)
            storage optimized w/ EC2 Compute
            Compute optimized
            compute optimzied w/ GPU
        - No battery option
        - 80 TB - 210 TB
        - 104 vCPUs, 416GB, GPU=Type P3
        
    Snowmobile
        - a fucking 45ft truck
        - 100 PB
        - GPS tracking, alarms, 24/7 video survillance
        - escort security

#####################################
#####################################
######                          #####
######     TRANSFER FAM         #####
######                          #####
#####################################
#####################################       

AWS Transfer Family
    Securly transfer files into and out of AWS storage using common file transfer protocols like
        SFTP (Secure File Transfer Protocol)
        FTPS (File Transfer Protocol Secure)
        FTP (File Transfer Protocol)
        A2 (https)
    - transfers into S3 or EFS
            
AWS Transfer Family Managed File Transfer Workflow (MFTW)
    - "is a fully managed serverless File Transfer Workflow serive
        to set up run, automate and monitor processing of files, uploaded"
    - eliminates manual intervention or additional scripts for post-processing
    - To execute a workflow, you upload a file to a Transfer Family server 
        that you configured with an associated workflow.
    - files get stored in S3, EFS, of FSx
    How to
        - You set up AWS Transfer Faimly
            Create a server for Transfer Family
                - choose SFTP, FTP, or FTPS
                - choose S3 or EFS
                - endpoint type
        - you get a hostname
            s-abc123.server.transfer.region.amazonaws.com
        - Connect with FTP/SFTP
            $ sftp -i /path/to/private_key.pem user1@s-abc123.server.transfer.us-east-1.amazonaws.com
                OR
            FileZilla
            WinSCP
            Cyberduck
    
    1. Upload
        - file is uploaded via AWS Transfer Family (SFTP/FTPS/FTP) 
        - triggers a pre-defined workflow.
    2. Workflow
        - Copying
        - Tag File w/ metadata
        - Decrypting/encrypting
        - Delete file
        - Custom procesing (pass to lambda)
    3. Completed workflow 
        - ✔ 
        
    Key Features
        - No Server Management
        - Pre-built Workflow Steps
            - Common operations like file movement, tagging, and notifications.
        - Custom Workflows
            - use Lambda for advanced processing.
        - IAM-based security and AWS CloudTrail.
        - Event-Driven
            - reacts to file uploads.

AWS Migration Hub
    Recall: 
        Migration - moving your apps, data, and infrastructure from 
            on-premises (or another cloud) to AWS.
    - "a single place to discover your existing servers, plan migrations,
        and track the status of each application migration"
    - can *monitor migration status* from:
        - Application Migration Service (AMS)
        - Database Migration Service (DMS)
    Things:
        AWS Discovery Agent
            - An agent installed on your VMs to discover migration servers
        AWS Migration Evaluator Collector
            - You submite requests to AWS to help asses a migration
        AWS Migration Hub Refactor  
            - some shit with legacy  and new services, cross aws acounts
        AWS Migraton Hub journey
            - guided templates for end to end migrations
        
AWS DataSync 
    - simplifies data transfer to/from:
        - on-premises storage
        - AWS services
        - AWS regions or accounts
        - other cloud provides
            - Google, Micsosfot, Wasabi, Oracle, ect
        -> can do S3 to EFS
    - eg. upload a file to s3, that file get copied to another s3
            
AWS Database Migration Service (DMS) 
    - DATABASES
        - from DB1 -> DB2 (any kind)

    - helps migrate databases 
    - you create a "Schema" to help with automatiaclly convering database A -> B
    - homogeneous (same database engine) 
        - Oracle → Oracle 
        - MySQL → MySQL
    - heterogeneous (different database engine) migrations.
        - SQL Server → PostgreSQL 
        - Oracle → Amazon Aurora
    - Cloud to Cloud
        - AWS-to-AWS
            S3 -> s3
            account -> another account
        - Azure SQL -> AWS Aurora
        - Google Spanner -> Aws RDS
        - mongoDB Atlas-> Amazon DocumentDB
    - On-premises & cloud
    Change Data Capture (CDC)
        - copies new data entering source DB while copying
        - can run indefinitely until you turn it off
        - can run one-time, where CDC stops once all data is copied.
    - Requires 1 EC2 that you select to do the work (you dont ssh in the vm, ect).
    - 2 Endpoints - Source & Destination
    
AWS Schema Conversion Tool (AWS SCT) 
    - desktop app
    - converts databases from one database engine to another. 
    - "convert your databse schema to another databse engine"
    - Microsoft SQL Server -> Amazon Aurora


#####################################
#####################################
######                          #####
######     APP SERVICES         #####
######                          #####
#####################################
#####################################       
    
AWS Auto Scaling
    (repeative but in diff words)
    - setup app scaling for multiple resources across multiple services
    - cool UI with graphs of predicted demand
    - gives recommendations
    - build scaling plans for resources:
        EC2 
        Spot Fleets
        ECS tasks
        DynamoDB tables and indexes
        Amazon Aurora Replicas
    - helps you auto adjusts the number of AWS resources based on demand. 
    - it can find resources in your application that can be scaled.
    Scaling plans:
        Dynamic Scaling 
            – Automatically adjusts capacity based on traffic patterns.
            - Target Tracking
                - like a thermostat
                - inc/dec based on metrrics and targetvalue
            - Step Scaling
                - inc/dec  based on size of the alarm/metric
            -  Simple Scaling
                - inc/dec  based on a single activity
                - has a cooldown
        Predictive Scaling 
            - looks at historical data
            – Uses machine learning to forecast demand.
            - good for Cyclical traffic
            - good for regular traffic patterns
    HOW TO
        Step 1: Create a Launch Template (or Launch Configuration)
            - AWS Console → EC2 → Launch Templates
        Step 2: Create an Auto Scaling Group
        OR 
            $ aws autoscaling create-auto-scaling-group 
                --auto-scaling-group-name MyAutoScalingGroup 
                --launch-template LaunchTemplateName=MyAutoScalingTemplate 
                --min-size 1 
                --max-size 5 
                --desired-capacity 2 
                --vpc-zone-identifier subnet-xxxxxxxx,subnet-yyyyyyyy
Rng note
    EC2 
        - instanc type, VPC, subnetId,  security group AMI, key-pair (ssh), storage,
            - Instance Type Defines the Hardware
    AMI Defines the Software Environment
        - Operating System (Linux, Windows, etc.).
        - Pre installed Applications (e.g., web servers, databases).
        - System Configurations (firewall rules, network settings).
        
        
AWS Amplify 
    - helps developers build, deploy, and scale full-stack web and mobile apps
    - simplifies backend development, hosting, authentication, APIs, and storage with minimal AWS expertise.
    - "is an opnionated framework and fully managed infrastructure that enables devs 
        to focus on building web and mobile apps"
    - Easy to use: No deep AWS knowledge is required.
    - apparently is awful
    - Suppose to be developer friendly 
        - BUT Andrew Brown says its shit https://youtu.be/c3Cn4xYfxJY?si=voN3agAea42wI5nV&t=87510
    - composed of multiple tools
        Amplify Hosting
            - host your website (static or not) easily
        Amplify UI
            - pre-built UI components for React, Vue, Angular, and Flutter 
        Amplify CLI
            - cli for Amplify things (authentication, APIs, databases, and storage)
        Amplify SDK
            - libraries that allows web and mobile apps to talk to AWS services.
        Amplify Studio
            - low-code app builder (backend, frontend, data)
        Amplify DataStore 
            - GraphQL thing?
        Amplify GraphQL Transformer 
            - create graphQL
            
AWS AppFlow
    - transfer data between SaaS apps (eg Salesforce, Slack, ServiceNow, Google sheets) 
        to AWS (S3, Redshift, AWS Glue).
    - Bidirectional 
    - 80+ apps
    - good for data integration without extensive ETL (Extract, Transform, Load) coding.
    - Suppose to be easy, "in minutes, few clicks", but Andrew Brown says its painful and difficult to figure out https://youtu.be/c3Cn4xYfxJY?si=SRLEpTq-cIz1LaZw&t=88710
    - "low code" 
        -> UI in console
    
    Step 1. 
        - AWS Console -> Amazon AppFlow 
    Step 2. Create New Flow
    Step 3. Choose Source (Saas)
        - Select from Salesforce, Slack, Marketo, Zendestk, SAP, SeviceNow, ect
    Step 4 Choose Destination
        - Select your s3, Redshift, EventBridge, Glue, Database/table
    Step 5 Mapping
        - Match fields between source & destination
            - manual or auto
    Step 6 Set up Trigger
        - Scheudled
        - Manual/on-demand (user inits it)
        - or Event

Graph QL
    - is a query language and runtime for APIs 
    - clients request exactly the data they need
    - rather than a fixed set of data 
    - Single Endpoint
        - Unlike REST, which has multiple endpoints (/users, /posts, etc.), 
        - GraphQL has single endpoint 
    Server Side:
        - implements a GraphQL schema 
        - implements resolvers 
            - define how to fetch data
            - resolver connect GraphQL queries to specific databases or APIs.
            - resolver determines whether is MongoDB, MySQL, PostgreSQL, or an S3 bucket.
        - Libraries like Apollo Server (Node.js), GraphQL Yoga, Express-GraphQL, help build a GraphQL server.
    Client Side;
        - client uses a GraphQL client library to make queuries to a GraphQL API
    - GraphQL acts as a middle layer between the client and backend data sources.
    
    Resolver example code:
    ```const resolvers = {
          Query: {
            user: async (_, { id }) => {
              return await mongoDB.getUserById(id);  // MongoDB query
            },
            orders: async () => {
              return await sqlDatabase.getOrders();  // MySQL/PostgreSQL query
            },
            profilePicture: async (_, { userId }) => {
              return await s3.getProfilePicture(userId);  // S3 bucket fetch
            }
          }
        };
        
AWS AppSync
    - GraphQL service
    - built-in authentication (IAM, Cognito, API keys)
    - good for web, mobile, and IoT applications. 
    - simplifies backend development
    - supports "real-time" data (instant updates)
    - supports "offline & caching" support
    - Two API types
        GraphQL API 
            - typical graphQL API
        Merge APIs
            - many graph APIs that act as 1
            - when many teams manage their own APIs 
    Caching options:
        - None
        - cache all requests (Full Request)
        - specificic oprations or fields defined in resovler (per-resolver caching)
    - connect to DynamoDB, Aurora, RDS, OpenSearch/Elasticesarch, Lambda, ect
    Use Cases
        📊 Live Dashboards
        🛒 E-commerce Apps
        📱 Offline-first Mobile Apps – work offline, sync later.
    
    Real Time data
        - instant updates between clients and servers as soon as data changes
        - opposed to polling every 10 seconds (high delay) or manual "update" btn
    "Offline" Mode?
        - client side caching
        - still function without internet connection                    
        - via
            1 Caching last data on user's device.
            2 Syncing once connection is restored.
        - after client fetches data, it's stored locally
        - uses
            Apollo Client (for web apps)
            AWS Amplify DataStore (for React Native, iOS, Android)

AWS Batch                    
     - run a large/batch computing workload. 
     - schedule batch jobs
     - provisions the optimal quantity of compute 
     - Supports dependencies between jobs (eg, job B can wait for job A to complete before execution
     - works with ECS (containers), Fargate, ECs, EKS  
     - auto scales??
     
    - Job = unit of work (shell script, docker iamge
    - job definitions = how to run the job, compute & memory
    - job queues =  collections of jobs w/ job priority
    - job scheduler = when, where, and how to run jobs, 
            - default: First in First out
     
    How make
        step 1. Choose an instance type (EC2, Fargate, Spot Instances, etc.).
        step 2. set execution order or jobs
        step 3. schedule
        
    Use cases
        Data Processing & ETL
        Scientific Computing
        Media Encoding & Rendering
        Machine Learning
        Financial Risk Analysis
        
AWS Fargate
    - like EC2s, but with containers.
    - is a serverless compute engine for containers 
    - run Amazon ECS and EKS workloads 
    - Serverless 
    - Automatic Scaling
    - For batch processing, microservices, and API-based applications.
    -  Scale at the container level,
    - ephemeral workloads
        - EC2 for  long-running, 
    - if youre running a server w/ API, 
        you'll have a Load Balancer (ALB) 
        and you'll have 1 instance always running
    - if you want Fargate to respond to events, you might do this:
        S3 → EventBridge Rule → ECS RunTask (Fargate)
        Event → Step Function → ECS Fargate

    
    Fargate vs EC2 (w/ auto scaling) vs. ECS (on EC2)
    Infrastructure Management	
        Fargate: automatic, serverless	(memory, cpu, ect)
        EC2: manual
        ECS: ❌ Manual unless integrated with Auto Scaling
    Scaling	Automatic	
        Scaling: aws automatic per task/pod
        EC2: create scaling policies
        ECS: create scaling policies
    Cost	
        Fargate: Pay per CPU/memory used	
        EC2: Pay for provisioned instances (even if idle)
        ECS: Pay for provisioned instances (even if idle)
    Security	
        Fargate: ✅ Each task/pod is isolated	
        EC2:⚠️ Shared resources, more security management required
        ECS:⚠️ Shared resources, more security management required
    Control & Customization	
        Fargate: Limited (no direct instance control)	
        EC2: Full control over EC2 instances, OS, and networking
        ECS: Full control over EC2 instances, OS, and networking
    Persistent Storage	
        Fargate: No direct EBS support (only EFS or S3)	
        EC2: ✅ EBS, EFS, and other storage options	
        ECS: ✅ EBS, EFS, and other storage options
    Use Case	
        Fargate: Serverless container apps, quick deployments, event workloads, unpredictable scaling	
        EC2: long-running apps !cost-efficient at scale
        ECS:  Large-scale microservices, custom hardware needs (eg GPUs, ARM instances)
    
    
AWS OpenSearch Service
    - OpenSearch and Elasticsearch
    - deploy, operate, and scale
    - ELK Stack
        Elasticsearch - full text search and analytics engine
                - search engines used for large datasets
        Logstash - prcesses data
        Kibana - visualization
        
        
AWS Device Farm 
    - test mobile apps 
    - real physical devices 
    - automated tests or manual interactions 
        - real mobile devices via remote Access
        - connects you to that device in your web browser.
    - Also test on various browsers (for web apps)
    - like Sauce Labs
    How to:
        1 AWS Console → Device Farm.
        2 Upload Your App (APK for Android, IPA for iOS).
        3 Select Devices from AWS’s real device pool.
        4 Choose a Test Type (Automated or Manual).
        5 run
    - Test via "Appium"
    
    
AWS Quantum Ledger Databse
    - ledger database
    - transparent, immutable, crytpographical variable transaction logs
    - serverless
    - Use case: for records of financial activities, compliance
    
AWS Elastic Transcoder 
    - THIS IS IS DEPRECATED AND GG'D @ November 13, 2025
    - media transcoding service 
    - convert media file formats
        - Optimize for mobile, tablets, ect
        - Content Delivery – Transcode before distributing via CloudFront or other CDNs
        - Multi-format Output
    
    - Cant use CloudFormation for it
    - Older version of AWS Elemental MediaConvert
    - prob on exam
        - expensive says Andrew Brown
    - configs for the output bucket/vides, including bucket name, permissions, and storage class. 
    
    Steps:
    1 AWS Console -> Elastic Transcoder -> Create New Pipeline.
    2 Fill in form:
        - Pipeline Name
        - Input Bucket (source)
        - IAM Role
            - IAM role that grants Elastic Transcoder access to your S3 buckets.
        - Output Bucket (destination)
        - Storage Class: (Standard or Reduced Redundancy Storage (RRS))
        - Notifications (Optional)
            - SNS 
        - Create
    3 Create Preset/Job
        - Predefined presets (HLS 720p, MP4 1080p, ect).
        - or Custom
        
    CLI:
        # IAM role
        aws iam create-role --role-name ElasticTranscoderRole --assume-role-policy-document file://trust-policy.json
        aws iam put-role-policy --role-name ElasticTranscoderRole --policy-name ElasticTranscoderPolicy --policy-document '{ "Version": "2012-10-17", ..... }
        
        # Transcode pipeline
        aws elastictranscoder create-pipeline --name "MyPipeline" 
            --input-bucket <YOUR_INPUT_BUCKET> 
            --content-config file://content-config.json 
            --role "arn:aws:iam::<AWS_ACCOUNT_ID>:role/ElasticTranscoderRole" 
            --notifications "Progressing=" "Completed=" "Warning=" "Error="

        # Get pipeline-id
        aws elastictranscoder list-pipelines
        
        # Create a Transcoding Job
        aws elastictranscoder create-job 
            --pipeline-id 1713880324699-qws2vn 
            --inputs file://inputs.json 
            --outputs file://outputs.json 
            --output-key-prefix "videos/" 
            --user-metadata file://user-metadata.json 
            --region us-east-1 
            --query Job.Id
        # check jobs 
        aws elastictranscoder list-jobs-by-pipeline --pipeline-id <PIPELINE_ID>

AWS AWS Elemental MediaConvert 
    - video transcoding service 
    - convert or compress video 
    - Video-on-demand VOD or streaming
    - Content Monetization (Ad Insertion)
    - OTT (Over-the-Top) Video Streaming
    - unlike ElasticTranscoder, no pipeline, just job
    - how to. Create a job:
        bucket source -> transcode -> bucket destination
    Can do cool things:
        - color correction
        - timecode source
        - rotation
        - pad video
        - embedded time code override
        - black barring
        - video cropping 
        - ect

Publisher Subscriber
    - implemented in "messaging systems"
    - an "event bus" categorizes messages into groups
    - alt names: Event Bus = Broker
    - 2 models
        Push Model
            - event bus    pushes messages to subscribers
            - low-latency, real-time notifications
            - eg. AWS SNS, webhooks,
        Pull Model
            - subscribers polls (pull) the event bus 
            - used when subscribers process messages at their own pace
            - AWS SQS, Apache Kafka
        Hybrid
            - idk if this exists ???
            - RabbitMQ
    - "event bus"
    - publishers do not send messages directl
    publishers ---> EVENT BUS ---> Subscribers
        

Recall:
 - Two fundamental messaging patters in software architecture

    Queueing System 
        - Pull based
            - Customers retrieve messages whenever
        - mechanism for storing and processing messages 
        - for distributed systems
        - messages persist in the queue until processed
        - AWS SQS
        - decouples and scales distributed software systems 
        Use Cases:
        ✅ Order processing systems
        ✅ Background job execution (e.g., image processing, machine learning jobs)
        ✅ Rate-limiting API requests
        
    Messaging/Notification System
        - Push based
            - messages delieved instantly
        - mechanism for broadcasting messages to multiple subscribers
            - real-time
        - Asynchronous communcation via message/events 
            - sender/receiver and producer/comsumer
        - messages NOT storage 
            - once message sent, it's not retrievable
        - fan-out messaging
        - AWS SNS
        - communication channels
        Use Cases:
        ✅ Real-time alerts & notifications (eg, SMS/email notifications)
        ✅ IoT device messaging
        ✅ Event-driven architecture
        

AWS SQS
    - a Queueing Service
        - PULL
    - enables decoupling, and scaling microservices, serverless apps & distributed systems
    - SQS does not have "Topics" to divide/categorize messages.
    - message size: 1 byte - 256 KiB
        - for larger you need a library (aws maintained)
        - payload is in an s3 bucket, and a reference the object in bucket in the SQS message queue
        - max 2GB
        - oddly, SNS is used in the code
    Retenion:
        - how long SQS holds before deleting
        - default: 4 days
        - choose from: 1 min to 14 days
    - Parallel: multiple consumers pulling messages simultaneously
    - Batching: pull 1-10 messages per request
    
    Types:
        1. Standard
            - Unlimited TPS (transactions per second)
                Message Ingestion 
                    - (Producers Sending Messages)
                    - Unlimited – send infinity messages per second.
                    - SQS automatically scales.
                Message Consumption 
                    - (Consumers Retrieving Messages)
                    - No limit – consume infinity messages per second
            - At-Least-Once (possible duplicates)	
                - write your app to be cautious of this
                    - there exists a message deduplication-ID to help you solve this
                    - the visibility timeout causes this problem
            - Best-effort (out-of-order possible)	

        2. FIFO
            - 300 TPS (3,000 with "High Throughput")
                - Optionally, enable "High Throughput" to allow 3,000 per second with Batching (10 grab per request)
                - eg 300 send, receive, or delete operations per second 
            - Exactly-Once (no duplicates)
            - Strict ordering guaranteed
            - no duplications
            - Ordered and exactly-once processing (eg, financial transactions, order processing)
        (Cant convert Standard to FIFO)
    Note
        - You cannot pull messages based on "type" or "attributes" of the message
        - Queues are designed for things like tasks. 
            - A machine grabs a new task/message from the queue, executes the task, then deletes the task.
            - messages within the same group are processed in order and one at a time.
        - In FIFO MessageGroupIds are requried:
            - but receiving messages don’t use MessageGroupId at all.
        - In FIFO, deduplication ID is required and prevents duplicate messages (if mulit send by publisher)
        - ✅ You must delete the message after processing; otherwise remains forever-ish.
        - ❌ No! Messages do NOT auto-delete after being retrieved.
        
    AWS SQS ABAC
        - Attribute-Based Access Control
        - control mechanism 
            - fine-grained permissions
        - ! Uses tags that are attached to users and AWS resources
        - control access to SQS based on tags and aliases associated with an SQS Queue
            - eg, SQS tag Environment=prod
        - you define policies based on attributes (instead of IAM roles).
        - helpful in large environments, where policy management for each resource can become cumbersome.
        - helpful in Auto-scaling infrastructure where new queues are created dynamically, 
            and you don’t want to manually update IAM policies each time.
        - Seems stupid af to me

    SQS Access Policy
        - like IAM but not
        - Defines who (IAM users, roles, or accounts) can do what on the queue, and under what conditions.
        Recall
            - IAM permissions are assigned to IAM roles and user
            - However! Access policies are attached directly to the resource itself, ie an SQS queue
        - SQS accepts messages from multiple sources:
            SNS, Lambda, or even external AWS accounts. 
        - IAM roles alone can't handle this because roles are tied to an AWS identity, not a resource.
        - IAM works only within the same AWS account, nor Acces Policies allow cross-account
        
    SQS Message Attributes
        - metadata
        - attach meta data to messages
        -> Your consumer can use message attributes to handle a message in a particular way 
            without having to process the message body first.
        
    SQS Visibility Timeout
        - a period when a message is temporarily hidden
            -> AFTER being read/consumed
            -> to avoid duplicate reads
        - hidden from other consumers after being retrieved.
        - Recall: messages exist until deleted, so this feature helps prevent bugs
        - consumser must delete the message
        Messages are invis after read
            - default 30 seconds
            - maximum 12 hours
        ✔ Visibility Timeout helps avoid concurrent handling of the same message.
        ✖ It does not eliminate duplicates; your code must still handle idempotency.
        
        -  It does not guarantee no duplicates. Messages might still be redelivered due to:
            - Delays in deletion
            - Network issues
            - Internal retries by SQS
        
    SQS Delay Queues
        - new messages are initially hidden for x seconds
            - new messages sent to the queue are hidden from consumers for the duration of the delay period.
        - postone new message
        - SET at the Queue for all messages
        - 0 second - 15 min
            - Default: 0 sec
    
    SQS Message Timers
        - *a individual* new message is initially hidden for x seconds
        - SET for indidivual message
        - not supported for FIFO
        
    SQS Temporary Queues 
        - you use an AWS Queue Library to build your own queue ?
        - use the Temporary Queue Client # https://github.com/awslabs/amazon-sqs-java-temporary-queues-client
        - "high-throughput, cost-effective, application-managed temporary queues."
        - "helps you ... when using common message patterns such as request-response"
        - Not easy to impliement
        
    SQS Short vs Long Polling
        1. Short Polling
            - the default
            - returns a response immediately, even if no messages are available in the queue.
            - It checks only a subset of the available servers, which may lead to empty responses.
            - Increases the number of API requests, leading to higher costs.
            - Suitable when you need immediate responses and are okay with higher request rates.
            - as you poll SQS, eventually all servers in SQS will have your message, 
                even if you're using short polling.
        2. Long Polling
            - you increase wait_time (max 20 seconds)
            - Waits until a message becomes available or until the timeout expires.
            - AWS queries all servers for messages, 
                - instead of just subset of servers
            - reduces the number of API requests, lowering costs.
            - cost effective
            - MONEY SAVE $$$
        

    AWS SQS CLI

        # SEND        
        # https://awscli.amazonaws.com/v2/documentation/api/latest/reference/sqs/send-message.html
        QUEUE_URL=https://sqs.ca-central-1.amazonaws.com/982383527471/sqs-standard-MyQueue-ONKfvvgwWJo2
        aws sqs send-message 
            --queue-url $QUEUE_URL 
            --message-body "Hello 1!" 
            --delay-seconds 1 
            --message-attributes "file://C:/Brodski/docs/send-message.json"

        # SEND AGAIN
        aws sqs send-message 
            --queue-url $QUEUE_URL 
            --message-body "Hello 2!" 
            --delay-seconds 2 
            --message-attributes "file://C:/Brodski/send-message.json"
            # send-message.json = {
            #  "Fruit": {
            #     "DataType": "String",
            #     "StringValue": "Apple"
            #   }
            # }

        # RECIEVE    
        QUEUE_URL=https://sqs.ca-central-1.amazonaws.com/982383527471/sqs-standard-MyQueue-ONKfvvgwWJo2
        aws sqs receive-message 
            --queue-url $QUEUE_URL 
            --attribute-names All 
            --message-attribute-names All
            --max-number-of-messages 10

        # DEPLOY thee SQS (This is done before)
        aws cloudformation deploy \
            --template-file "file://C:/Brodski/template.yaml" # https://github.com/ExamProCo/AWS-Examples/blob/main/sqs/standard/template.yaml
            --capabilities CAPABILITY_NAMED_IAM 
            --no-execute-changeset \
            --region ca-central-1 \
            --stack-name $STACK_NAME
        protip
            $ aws sts get-caller-identity
            
            
            

AWS SNS
    - is a pub/sub messaing service
    - decouples microservices, distributed systems, and serverless apps
    - PUSH MODEL
    - a broker/eventbus with "SNS Topic" sit in the middle
    - like every aws service can publish events to SNS
    
    SNS Destinations:
        - dests are the subs who recieve messages
        App to App (A2A)
            - Data Firehouse
            - Lambda
            - SQS
            - HTTP endpoints
            eg)  Email, HTTP, Data Firehose, SQS, Lambda
        App to Persion (A2P)
            - mobile apps
            - phone numbers (SMS)
            - email
            eg) Email, text message, phone number
        
    Python:
        # SNS Topic ARN
        topic_arn = 'arn:aws:sns:us-east-1:123456789012:MyTopic'
        
        # Publish a message
        response = sns.publish(
            TopicArn=topic_arn,
            Message='Hello from AWS SNS!',
            Subject='Test Message'
        )
        
        print("Message ID:", response['MessageId'])
        
    SNS Topics
        - is a "communciation channel"
        - the channel that receives and distributes messages
            - an access point
            - a category
            - intermediary that groups related messged
        - SNS Topics exist indefinitely in AWS SNS until you explicitly delete 

        2 types
            Standard (Default)
                - High-throughput - "nearly unlimtied # of msgs per second"
                - order not gauranteed sequtial
                - message might be delivered 2+
                    - possible duplicates
            FIFO (First-In-First-Out) 
                - not as high throughput - 3,000 per second or 10 MB per second 
                - Strict ordering
                - 1 message gaurante
                - 300 transactions per second (TPS) per message group.
            
    SNS Message
        - the actual data (content) 
        - sent by a publisher to an SNS topic
        - usually programmatic
        eg, Email:
            The SNS Topic is like a mailing list.
            The subscribers are the people on the mailing list.
            The message is the actual email content that is sent
    SNS Message Attributes 
        - attach metadata with the message body
        - it adds context about the message
        - useful for message filtering

    SNS Filtering 
        - filters based on message attributes
        - receive only relevant messages.
        - by default, subscribers receive every message published to the topic
        - subs define attribute-based filters to receive messages with specific conditions.
        - subs use "Filter Policy"
        Filter policy 
            - json object 
            - can be complex if you want

        1. Publisher Sends Messages with Attributes
            sns.publish(
                TopicArn='arn:aws:sns:us-east-1:123456789012:MyTopic',
                Message="System crashed!",
                MessageAttributes={
                    'event_type': {
                        'DataType': 'String',
                        'StringValue': 'error'
                    }
                })
        2. Subscriber Sets a Filter Policy
            {
              "event_type": ["error"],
              "severity": [{ "anything-but": "low" }],
              "priority": [{"numeric": [">", 2]}]
            }
            - `anything-but` is an  SNS filter policy keyword (used to blacklist values).

        
    SNS Subscription 
        - connects a subscriber to an SNS Topic
        - a sub can only sub to many topics
        - Subscribers receive the message based 
        Python
            sns.subscribe(TopicArn='arn:aws:sns:us-east-1:123456789012:MyTopic',
                  Protocol='email',
                  Endpoint='user@example.com')
                  
        Cloudformation with AWS SAM
            https://github.com/ExamProCo/AWS-Examples/tree/main/sns/basic
            
        Cloudformation w/o AWS SAM (chatGPT)
            (scroll down) https://chatgpt.com/c/67d50212-8dec-8001-a4c6-3c02e266d76c 
                  
    SNS Data Protection 
        - safeguards data by masking sesntivie info
        - "audit, mask, redact, or block the sensitive information"
        - Scans for PII, PHI
        - can use Predefined Identifies (Name, Adresses, credit card #)
        Operations:
            Audit - ensure compliance, security, and operational integrity on data
                    send findings to s3, cloudwatch, or Firehose
            De-idenfity (mask data)
            Deny - (block data from being sent)
        - detects the sensitive data by using machine learning and pattern matching.
        - uses json policies 
        - Good for financial, legal, and regulatory risks. ... HIPAA, GDPR, PCI, FedRAMP

    SNS Raw Message Delivery
        - messages in their original, unmodified form.
        - by default, SNS wraps messages in a JSON structure 
            - when sending notifications to SQS, Lambda, or HTTP/HTTPS endpoints. 
        - default includes metadata such as:
            -  message ID, subject, TimeStamp, Signature, message attributes, and other attributes.
        Benefits
            - simple, Eliminates the need to parse JSON-wrapped messages.
            - Smaller payload size
        - Http/s endpoint have a `x-amz-sns-rawdelivery` header
        - with Raw Message Delivery enabled, SNS sends the message as-is without additional 
              JSON formatting, making it easier for some applications to process
        
        
    SNS Delivery Retry Policy
        - behavior when SNS cannot deliver a message (5xx, 4xx)
        - when delivery errors occur, the policy defines how SNS retries message delivery 
        - After a long time (when delivery policy is exhausted), SNS stops retrying and dumps the message
            - unless a Dead-Letter queue is attached to the subscription
        - you can write these policies, but AWS has defaults
        
        Policies for Different Protocols
            AWS Managed endpoints
                - Data Firehouse
                - Lambda
                - SQS
                -> aws tries harder (more attempts and less delay) for these compared to Customer managed endpoints
            Customer managed endpoints
                SMTP
                SMS
                Mobile push
    Dead Letter Queue (DLQ) 
        - queue of failed messages 
            - (after exhausted delivery attemps)
        - A dead-letter queue is attached to an Amazon SNS subscription (rather than a topic) 
            because message deliveries happen at the subscription level. 
        - is a SQS


Amazon EventBridge
    - EventBridge was formerly called Amazon CloudWatch Events
    - A alternative/newer SNS
        - VERY similar to SNS (below has more info)
        - has pub/sub
        - used for application integraiton for streaming real-time data
    - *is a serverless event bus service*
    - you push events to EventBrindge vis CLI or SDK (or AWS Console for testing)
        - you dont register publishers, anything can send events
        -> This is the same for SNS, you dont register a publisher
    Recall:
        Event bus = receives events from a source and routes events to a target based on rules
    Note:
        your account comes with a default event bus, created in every AWS account
    
    Example Flow:
        Producer → Event Bus → Rule (with Event Pattern) → Target
    
    Components
        1. Event/Message Bus
            - the central routing layer
            - defines rules too
        2. Events  
            - Events are sent to EventBridge
            - "data emitted by services"
            - JSON objects
        3. Rules
            - filters events and maps events to a target
                - 5 targets per rule
        4. Targets
            - Things, AWS Services (Lambda, ect)
            - subscribers
        5 Schemas
            - Define the structure of events
        6. Producers/Publisher
            - push (emit) events to the event bus
                - or as they call it "emit events"
                - Can be ec2, s3, lambda
                - SaaS (datadog, pagerduty, zendesk)
                
        -> There is no explicit "Filter" section in EventBridge.
        -> The filtering happens inside the rules

        SNS v EventBridge
        ------------------------------------------------
                               SNS Wins	                EventBridge Wins
        Setup & Simplicity	    ✅ Yes	                    ❌ No
        Notifs (email/SMS)	    ✅ Yes	                    ❌ No
        Event Filtering	        ❌ Limited	                ✅ Advanced
        SaaS Integration	    ❌ Limited	                ✅ Built-in (datadog, pagerduty...)
        Message Fan-out	        ✅                          ✅ But more configuration
        Replay & Archiving	    ❌ No	                    ✅ Yes
        Schema Validation	    ❌ No	                    ✅ Yes
        Fan out                 ✅ 12.5 subs per topic      ❌ 5 targets per rule   
        Scale?                  ✅ 100,000 topics           ❌ 300 rules per account
        
    - EventBridge's Biggest attraction: has 3rd party integration, says Be a Better Dev
        - shopify
        - data dog
        - pager duty
    
    - Sources = AWS services and apps
    - Can make a schedule
        - cron
        - like in my transcriber app :)
        eg) send an event every monday at 1pm
    - Not all AWS services can emit CloudWatch Events
        - use CloudTrail Events as work around, and be tricky
        - solution: turn on CloudTrail, allowing EventBridge to track changes and do cool stuff   

    Step-by-Step
        1. Event is sent to event bus
            - via "publisher"
        2. Event Bus receives event
        3. Rules are evaluated
            - each rule, one by one
            - if a rule matches we have a "hit"
            - If a rule’s filter pattern matches the event, it's considered a match (a "hit").
            ! a Rule is composed of a Event Filter, a target, and maybe other things (I think)
        4. Matched rules trigger 
            - for each rule that matches, the event is forwarded to tht rule's target
        5. Event is delivered to target

    Event:
        - It represents something that happened, like:
            - a file being uploaded to S3
            - an EC2 instance starting
            - or a custom application event (eg "order placed")            
        - When an event is pushed to EventBridge, the event will have fields like:
            {
              "version": "0",
              "id": "6a7e8feb-b391-4ef7-e9f1-bf3703467718",
              'detail-type": "EC2 Instance State-change Notification",
              "source": "aws. ec2",
              "account": "121212121212",
              "time": "2020-05-22T14:22:48Z",
              "region": "us-east-1",
              "resources": [
                "arn: aws : ec2 : us-east-1: 123456789012 : instance/i-1234567890abcdef0"
              ]
              "detail": {
                "instance-id": " i-1234567890abcdef0",
                "state": "terminated"
               << OR >>
              "detail": {
                "orderId": "12345",
                "customerId": "abcde"
                }
              }
            }     
        

    Rules
        - Rules listen to an event and apply event patterns. 
        -> When an incoming event matches the pattern, the rule triggers a target action. A rule can have multiple targets.
        - Each rule does its own matching separately.
            Component	Meaning
            Rule A      Has Event Pattern: Match "order placed" events
            Rule B      Has Event Pattern: Match "payment failed" events
            Rule C      Has Event Pattern: Match "user deleted" events
        
            - Each Rule is independent, with its own Event Pattern and Targets.
            - you can filter and route events from various sources to targets
        - each Rule has exactly one Event Pattern.
        
        - are essential in an event-driven architecture, 
        
        - 1 rule has 1-5 targets MAX **
            - eg Lambda, SQS, SNS, Firehose
            
        - when you have a match, the Rule can "Configure the Input" that gets forwarded
        
        Configure Input 
            - allows you to modify event data before passing it to a target.
                - optimize payloads and processing efficiency
                You can:
                    - filters event data
                    - transfomr event data
                    - send constant json
            Actual Options in AWS:
                Matched Event
                Part of matched event
                Constant (JSON text)
                Input transformer)


    !!!
    Event Patterns
        - the "filter" logic
            - only events matching the Pattern are processed by the Rule.
            - "defines the structure that EventBridge uses to match incoming events to rules"
        - used to filter which events should be passed to the target
            - (this seperates EventBridge from SNS, imo)
        can use operators like:
            - prefix matching "ca-*"
            - anything-but (match strings from an array)
            - numeric  (greater than, equal to, ect, >, >=, =
            - ip addresses (CIDR)
        - the filter is a json objects describing what should be passed
          eg) this is your "Event Pattern"
              {
                "source": [ "aws.ec2" ],
                "detail-type": [ "EC2 Instance State-change Notification" ],
                "detail": {
                "state": [ "terminated" ]
              }
          - Meaning when you receive an event, EventBridge runs the event
              against all your Rules (which will have Event Patterns), if it finds a match, 
              then forward to its target
              
    Partnered Event Sources
        - integrate 3rd party apps with EventBridge
        - event will emit from the service provider into your EventBus
        - Zendesk, PagerDuty, datadog, ect
    
    Schema
        - defines the structure of events that are sent to EventBridge. 
        - EventBridge provides schemas for all events that are generated by AWS services. 
        - will be a json object of various Fields and the data type
          { 
            details-type: {
                "type": "string"
            }
            resources: {
                type: array,
                items: {
                    type: string
                }
            }
            ... ect
          }

    Schema Registry
        - a feature that helps developers create, discover, and manage OpenAPI schemas on EventBridge
        - "allows you to create, discover, and manage OpenAPI schemas for events"
        - Schema registries are a collections of schemas.
            AWS event schema registry 
                – The built-in schemas.
            Discovered schema registry 
                - schemas discovered by Schema discovery.
            All schemas 
                – AWS , discovered, and custom schema registries.

            You can create custom registries to organize the schemas you create or upload.
        - allows applications to infer event formats, store them, 
            and generate code bindings to simplify event-driven development.
        - nice to view version history

            
            
AWS MQ
    - a managed message broker 
    - open-source message brokers
    - message-oriented middleware
    - like SQS or SNS but you have alot more control for what you need
    Choose either:
        1. Apache ActiveMQ 
        2. RabbitMQ
    
    Recall:
        ActiveMQ and RabbitMQ 
        - are message brokers that enable applications to communicate with each other asynchronously. 
        - used in distributed systems, microservices, and event-driven architectures
        Apache ActiveMQ 
            - is an open-source message broker developed by the Apache Software Foundation. 
                It is written in Java 
        RabbitMQ
            - is an open-source message broker developed by Pivotal and now maintained by VMware. 
                It is written in Erlang
            - considered faster and lighter
            - Newer, but still 2007 vs 2004
        - Protocols: both support AMQP, MQTT, and STOMP

AMQP (Advanced Message Queuing Protocol) 
    - "is an **open standard wire-level prtocol* desinged for messaing middleware
        that enables conforming client apps to communicate with conforming messaging middleware servers
    - for passing business messages between applications or organizations.
    - for messaging interoperability
    - is an open standard application layer protocol 
        - open standard = agreed-upon technical standard
    - It enables communication between distributed applications.
                                    +--------------------------+
                                    |    Broker (Rabbit MQ)    |
                                    |                          |
     Publisher  --> (Publish) --> Exchange --> (Routes) --> Queue --> (Consume) --> Consumer
    (AMQP Client)                   |                          |                  (AMQP Client)
                                    +--------------------------+
    - messages pushed to exchanges
    - exchanges distributes messages to Queues




    
MQTT (Message Queuing Telemetry Transport)
    - "is a lightweight publish-subscribe messaging protocol"
    - "uses minimal network bandiwth"
        - for low-bandwidth, high-latency, or unreliable networks. 
    - used in IoT, mobile applications, and real-time messaging.
                       +---------------------------+
                       |        Broker             |
                       |                           |
   Publisher  ----> Publish ---> (RULES) ---> Subscribe ----> Subscriber(s)
    (MQTT Client)      |                      (push/pull)    (MQTT Client)
                       +---------------------------+ 
    
STOMP (Simple/Streaming Text Oriented Messaging Protocol)
    - STOMP is a text-based messaging protocol 
    - for asynchronous communication between clients and message brokers. 
    - simple
    - for messaging systems and message queuing, 
    - for publish-subscribe patterns
    - Also needs a Broker
    
    Wire protocol 
        - *at the byte level* is the exact format and rules used to exchange data over a network connection
        - How messages start and end.
        - How two systems interpret the bytes they send to each other.
        
NOTE
    - AMQP, MQTT, and STOMP are all messaging protocols 
        that define how messages are structured, transmitted, and acknowledged. 
    - AMQP, MQTT, and STOMP are NOT message brokers 
        — they rely on Event Buses or Message Brokers to handle message routing.
    - Messaging Protocols
        - (AMQP, MQTT, STOMP)
        - Define how messages are formatted and transmitted between clients.
        - Do not store or manage messages.
        - Require a broker to function.
    - Message Broker / Event Bus 
        - (RabbitMQ, Kafka, ActiveMQ, EMQX, etc.):
        - Implement one or more messaging protocols.
        - Handle message routing, persistence, and delivery.
        - Provide features like load balancing, scalability, and fault tolerance.

        – Brokers/Event-bus are the software components that implement messaging protocols 
            (like AMQP, MQTT, STOMP, Kafka, etc.) and manage message routing.
        
AWS Service Catalog
    - orginzations can create and manage approved IT services
        - "catalog of products", orgs can create & manage
    - products must get approved for use on AWS
    - services include: 
        - vm images, servers, software, databases, and more to complete multi-tier application architectures.
    - enables organizations to maintain governance and compliance
        -"End users can deploy the approved IT services they need, 
            following the constraints set by your organization."
        
        
    Portfolio
        = a collection of 
            Products
            Permissions (who can do what
            Contraints (can restrict how products are used, more below)
    Product 
        = Cloudfomration Template
    Provisioned Product 
        = a launched product
            - purchased by customer
    Catalog
        = cute UI to view and launch product
        - a collection of products
        - admin publishes, end user uses/launches/views, ect
    Service Actions
        - allows end-user to perform maintenance
            - "are SSM Document associated with a Product"
            - Admin can extend functionality to thier product via these actions
        - Admin "associates" an actin to their product (via clicking buttons in Console)
        - End-user can run the action (via clicking in the Console ("Actions" -> AWS_custom_restart)

    How It Works
        - Administrators create a Product Portfolio (set of approved AWS services).
        - Users browse and deploy services from the Service Catalog.
        - AWS Service Catalog ensures resources comply with organizational policies.

    Two Users:
        Catalog Admin
            - They manage the catalog
        End user
            - They use the catalog
            - use AWS Console to launch products
    
    Constraints:
        - you creat constraints for specific products in your portfolio
        Launch
            - Use specified IAM roles instead of end-user credentials
        Notification
            - send product notification to a stack (stack = thing end-user can view & click ?)
        Template
            - limit the options in the CloudFormation template
            eg) Only allow t2.micro
        StackSet
            - configure product deployment across accounts and regions
        TagUpdate
            - update tags after product provisioned
        
    
    Features (aws copy paste)
        Standardization
            - Administer approved assets (IT Services) by restricting where the product can be launched, 
                the instance type, and other configuration options. 
            - The result is a standardized landscape for product provisioning for your entire organization.
        Self-service discovery and launch
            - Users browse listings of products (services or applications) (that they have access to), 
                and launch it as a provisioned product.
        Fine-grain access control
            - add constraints and resource tags to be used at provisioning, 
                and then grant access to the portfolio through IAM users and groups.
        Extensibility and version control
            - Updating the product to a new version propagates the update to all products in every portfolio 
            that references it.
            
    Use Cases
        -Enforce security and compliance
        -Simplify with pre-approved templates.
        - manage services across different AWS accounts .
    
    
#####################################
#####################################
######                        #######
######     CloudWatch         #######
######                        #######
#####################################
#####################################
    
Pillars of Observability 
    Observability:
        - data that help us understand and troubleshoot complex systems.
        - the ability to measure systems to understand performance, tolerance, security, ect
        - to achieve observability you need Metrics, Logs, and Traces.
            - All 3 together. 1 is not enough for observ.
    Metrics
        - Numeric data
        - measurements
        - CPU, Memory, Latency, ect
    Logs
        - text records of events
        - context rich info
        - "Database connection failed"
        - "User X logged in at 10:05"
    
    Traces
        - end-to-end journey of a reqeust through a distributed system
        - history of a request that traves through mulitples apps/services
        
    (?) Alarms
        - arguable a 4th pill
        - alerts when things go south

AWS Cloudwatch
    - a monitoring service
    - an umbrella service for a collection of monitoring tools
    - All Cloudwatch services build off of cloudwatch Logs
    
    CloudWatch Logs
        - logs
        - any data, eg App data
    CloudWatch Metrics
        - a time-ordered set of data points monitoring resources
        - eg memory usage
    Cloudwatch Alarms
        - Can trigger notifications via SNS, Lamba, or Auto Scaling based on defined thresholds
        - Two types of metrics
            - Metric Alarms
                based on metrics
            - Composite Alarms
                uses a "rule express" and takes into account mutlipel alarms
    Cloudwatch Events
        - detects changes in AWS and trigger actions automatically
    Cloudwatch Synthetics
        - monitor endpoints and APIs
        - detect isssues in latency and availability
    Cloudwatch ServiceLense
        - distributed tracing via AWS X-Ray
        - microservices debugging
        - serverless
    Cloudwatch Dashboard
        - create dashboards
    Cloudwatch Contirbutor Insights
        - view top contributors impacting performance
        - detect bottlenecks and spikes
    
    Cloudwatch Logs
        - logs!!!
        - text based logs
        - "is used to monitor, store, and access your log files"
        - you can export logs to s3 to save $ or custom analysis
        
        log groups
            - collection of log streams
        log streams
            - sequence of events from an app or instance
        log events
            - represents a single event in a log file
  
        Log Group > Log Stream > Log Event
    
    CloudWatch Logs Insights
        - search and visualize log data.
        - interactively search and analyze logs
        - more robust
        - dont have to export to S3 and use Athena
        - it "enables you to interactively search and anlyze your cloudwathc log data"
        Query
            - you use the Console
            - Has it's own "language" called Query Syntax (looks splunk like)
            - Query up to 20 log groups
            - time out after 15 min if incomplete
            - lasts 7 days after
            - you can save queries
            - aws provides sample queries to get  you started
            
            Discovered Fields 
                - logs might come with Structured Data (the discoverable fields).
                - disc fields are key-value pairs that CloudWatch detects from structured data. 
                - "automatiaclly discoveres fields in logs" like
                    @timestamp
                    @message
                    @hostZoneId
                    @starTime
                    @endTime
                - Useres can find these fileds under Discoverable Fields panel.
            Example:
                { "timestamp": "2024-03-16T12:00:00Z", "status": 200, "message": "Request successful" }
                - thus the discoverable data = timestamp, status, message
                eg)
                    fields @timestamp, @message
                       | sort @timestamp desc
                       | limit 10
                eg)
                    filter action="REJECT"
                       | stats count(*) as numRejections by srcAddr
                       | sort numRejections desc
                       | limit 20
                       
               
    CloudWatch Metrics
        - time-ordered set of data points!
        - a variable monitored over time
        - predefined metrics
        - a variable monitored
            CPUUtilization, NetworkIn, DiskReadOps, ect
        - used to monitor applications and infrastructure. 
        - track system health and trigger automated actions.

    Cloudwatch Custom Metrics
        - user-defined metrics 
        - you can publish  your own "Custom Metric" using CLI or SDK
        - you monitor specific application data
            - eg) user activity, transaction counts, response times, or error rates.
            
    Resolution
        - aggregates 
        - it determines how AWS aggregates your metric data, not how often you send data. 
        - it determines how frequently CloudWatch records and makes data available
        - even if you push a metric 100 times per second with the CLI, 
            CloudWatch still processes it based on the configured resolution.
                
        2 types of "resolutions"
            - Standard Resolution	
                1 minute (60 seconds)	
                General monitoring (eg, EC2 CPU usage)
            - High Resolution	
                1 second (1s, 5s, 10s, or 30s)	
                Real-time monitoring (eg, stock trading apps, live analytics)
                
        Example 
        - Standard-Resolution (60s): 
            - CloudWatch automatically aggregates multiple data points per minute (e.g., averages them).


    Data Availability 
        - when sending data to Cloudwatch, the data isnt instantly available to be viewed (duh)
        - it takes time for data to be ingested
        - For every service in AWS, monitoring will come in 1/3/5 minutes
            - usually in 1 min
        - For EC2s you can choose Basic or Detailed Monitoring
            - Only EC2s have these options:
            Basic Monitoring
                - EC2s Every 5 minutes ******** EXAM KNOW THIS
                - free
                - general health
            Detailed Monitoring
                - EC2 Every 1 minute  ***********
                - $ costs money (~0.30$ / metric)
                - for optizaton and important stuff
        
            
            
    CloudWatch Agents
        - a thing that sends logs running on your EC2 to CloudWatch Log Groups.
        ✅ A software agent 
        ✅ runs on Linux & Windows instances.
        ✅ Collects host-level metrics (CPU, memory, disk, network, etc.).
        ✅ Collects custom logs and sends them to CloudWatch Logs.
        ✅ Works with EC2, on-prem servers, hybrid cloud (via AWS Systems Manager).
         EC2 metrics by default (without agent).
            - CPU Usage
            - Network Usage
            - Disk Usage
            - Status checks: Hypervisous status check, EC2 Status Check
         EC2 metrics WITH agent
            - Memory utilization ***
            - Disk Swap utilizaiton
            - Disk Space Utilization ***
            - Page file utliziaton
            - Log Collections 
                - the agent is used to collect various logs and send them to cloudwatch log group
                
        Agent Collecting Logs
            - the agent needs to be updated to include the logs
            - the agent needs to be restareted
            - the agent's configs @ /etc/awslogs/awslogs.conf
                - specify log file, log group, log stream

        Installation:
        - Either manually or via AWS Systems Manager
            Method 1 (Manual)
                $ sudo yum install -y amazon-cloudwatch-agent
                $ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard
                ect
            Method 2 (AWS Systems Manager (SSM))
                - AWS Systems Manager → Parameter Store → Click Create Parameter.
                    - Paste your CloudWatch Agent JSON configuration.
                - click Run Command → AWS-ConfigureAWSPackage → Targets → Action → Install → AmazonCloudWatchAgent ect
                
          
    CloudWatch Alarms 
        - trigger actions based on metric thresholds or anomaly detection models.
        Trigger:
            - the action triggered could be:
                Auto Scaling Group action (grow/shrink)
                EC2 action (stop, terminate, reboot, recover
                Notification (SNS)
                invoke lambda
        Anatomy
            Threshold Condition:
                defines when a datapoint is breached
            Period
                how often Cloudwatch evaluates the alarm/checks
            Evaluation Period
                number of previous period
        Condtions:
            Static:
                - you define a threshold (CPU > 80%)
            Anamoly
                - uses statistical and machine learning algorithm
                - The model generates a range of expected values that represent normal metric behavior.
                - works with seasonality and trend changes of metrics
                - the seasonality changes could be hourly, daily, or weekly
                - dynamic trends, smart
                - avoid false-positives b/c cyclicial metrics are expected sometimes
                
        Composite Alarms
            - alarms that watch other alarms
            - combine several alarms into one composite alarm
            - Composite alarms determine their state by monitoring the states of other alarms.  
                - aggregate health indicator over a whole application. 
            - *reduce alarm noise*
            -> Once triggered, send SNS notification is the only thing you can do
            example 1, create a composite alarm to send a notification if any alarm triggers. 
                - the play: When any alarms enters ALARM state, the composite alarm enters ALARM state 
                    -> sending a notification to your team. 
                    - If other alarms related to your web server also go into the ALARM state, 
                        your team does not get overloaded with new notifications since the 
                        composite alarm has already notified them about the existing situation.            
                - alarm when many different conditions are met
            example 2, send alert/notification if both the CPU and the memory alarms have triggered.
            



        Metric alarm states    
            OK – metric is in good threshold
            ALARM – metric is in bad threshold
            INSUFFICIENT_DATA – The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.
            
    CloudWatch Dashboard
        - dashboards
        - some predefined "Widgets" to help you out
          
          
###########################################
###########################################
####                                 ######
####            LAMBDA               ######
####                                 ######
###########################################
###########################################

AWS Lambda
    - serverless, Function as a Service
    - run code w/o provisioning or managing servers
    - auto scaling
    - event-driven 
        - s3 (file upload), dynamoDB (data changes), API Gatewy (HTTP req), SNSN
    - pay per use pricing, (RAM, storage, cpu arch)
        
    Use case:
        A.
        - photos are uploaded to s3.
        - Event triggers Lambda
        - lambda processes photo, creates, thumbnail, back to s3.
        B.
        - User fills out a form. Submit
        - Lambda validates the form data
        - saves in DynamoDB and sends email via SNS 
    Triggers:
        - Thing that invokes lambda
        
        API Gateway (common)
        DynamoDB
        Kinesis
        S3
        SNS
        SQS
        EventBridge
        AWS IoT
        Alexa
        Cloudfront
        CloudwatchLogs
        -> 3d party partners
            - Datadog, PagerDuty, OneLogin, Zendesk,
        -> AWS SDK

    AWS Lambda Destinations 
        - you can manage failures/successes of lambda functions
        - when it's invoked ASYNCHRONOUSLY.
        Asynchronously
            - some services can invoke lambda but does not listen for a response
            - Caller waits for the function to return a result.
            eg) S3, EventBridge, SNS,
        Synchronous
            - Caller waits for the function to return a result.
            eg) API Gateway, SDK call, SQS, HTTP, lambda CLI, or CLICKING THE "TEST" BUTTON in the console
            
        - You configure your lambda to send success to a Success Destination
        - failures (execptions) go to the Failure Destination 
            -> Probably to a SQS
        - you can few a message about the failure/success in a JSON format 
        eg)
                           +---> Success ---> AWS Lambda
        SNS ---> Lambda ---+
                           +---> Failure ---> SQS
        
            
    
    Versions
        - you can use versions
        - helps manage deployment
        - when you reference a Lambda ARN it has 2 versions
            Qualified ARN                                                 |
                - WITH VERSION                                            | 
                - version suffix                                          V
                - arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST
            Unqualified ARN
                - WITHOUT version suffix
                - arn:aws:lambda:aws-region:acct-id:function:helloworld
                - if you use Unqualifed ARN it defaults to $LATEST

    Alias
        - give friendlier names to specific versions
            - for accesing the lambda progrtammatically
        - cannot create Aliases with Unqualified ARNs
        - cannot create Aliases that that directly points to the $LATEST version 
        eg) stable-version
        
    Layers
        - Lambda layer is a .zip file that contains supplementary code or content. 
        - usually contain library dependencies or configuration files or custom runtimes.
        - Instead of packaging these dependencies with your Lambda,
            - you upload them as a layer
            - now multiple Lambda functions can use them.
        - 5 layer max attached to a function
        - 250MB = all layers unzipped size limit
        - Layer does not reside in your Lambda function’s ZIP file but is referenced by the function at runtime.
        Good for:
            - Faster Cold Starts
            - Smaller Deployment Packages: (less storage usage)
    
        - ?? put all packages installed via pip install or all node_modules as a layer. 
            - Since Lambda auto scales, if I have 100 lambda functions running, 
                - instead of 1 node_module per lambda, 100 node_modules total, 
                - i could just have 1 layer with 1 node_modules. right?
                - ChatGPT says yes correct, (but its not literally 1, aws does magic)
            - 🚨 Do NOT put your actual function code inside the layer—only shared dependencies.
    
    Instruction Sets
        - Arm64 (cheaper, better)
        - x86_64
    Runtimes
        - preconfigured environemts to run a language
        - refered as "Managed Runtimes"
        - eg) Named version: Node.js 20
              Identifier: nodejs20.x 
              OS: Amazon Linux 2023
        - runtimes: Java, Python, Node.js, .NET, and Ruby

    OS-Only Runtimes
        - Your own environment because the Managed Runtime ^ doenst work for your app
        - sometimes your programming language is not available as a managed runtime
            - use an "OS-only runtime" then.
        - OS-Only Runtimes, come with only the OS (Amazon Linux 2) but no pre-installed language runtime.
        - only possible though Docker images
        - you customize the runtime, and a custom bootstrap script to launch and run your function.
        OS-Only Runtimes:
            Amazon Linux 2      - provided.al2023
            Amazon Linux 2023   - provided.al2
        3 cases when to use:
            1. Native ahead-of-time (AOT) compilation
                - Natively compiled languages (like Go, Rust, C++) 
                - some laguages ^ compile to an executable binary, which doesn't require a language runtime.
                    - These languages only need an OS environment in which the compiled binary can run. 
                - You Must Include a Runtime Interface Client in Your Binary
                    - AWS Lambda functions do not run like normal scripts or executables.
                    - Runtime Interface Client (RIC) acts as a "middleman" between AWS Lambda and your function
            2. Third party runtimes
                - unique runtimes that AWS does not support, so you use another party's runtime
                eg) Bref for PHP or the Swift AWS Lambda Runtime (github stuff)
            3. Custom runtimes
                - runtimes aws doesnt supports, so you set it up yourself
                - Node.js 19

        How OS-Only Runtimes Work
            - Instead of a built-in runtime (python3.11), 

            1 AWS starts the function in an OS-Only environment (Amazon Linux 2).
            2 Your function includes a bootstrap script, which acts as the runtime.
            3 The bootstrap script listens for Lambda events, processes them, and returns results.
        
        - OS-only Runtime: provided.al2023 or provided.al
    
    Fun Fact (java and compling)
        - languages like Python, Java, and JavaScript require an AWS-provided runtime
            eg, Python runtime, Node.js runtime
        - However, Go, Rust, and C++ do not need a runtime 
            because they are compiled into self-contained executables that can run directly on the OS.
        - AWS sends function invocation requests to your code through the Lambda Runtime API.
            - This is done using a Runtime Interface Client (RIC), provided by AWS
        
        - Does Java Compile to a Binary?
          Yes, but not a native binary like Go or Rust. Here’s why:
          
          - Java source code (.java files) is compiled into bytecode (.class files).
          - bytecode files are packaged into a JAR or WAR file.
          - However, Java bytecode is NOT machine code -> It requires the Java Virtual Machine (JVM) to run.
          - At runtime, the JVM interprets or Just-In-Time (JIT) compiles the bytecode into native machine instructions.
          🔹 Key point: A JAR file is not a standalone binary. It needs the JVM to run.
       - Ahead-of-Time (AOT) Compilation
            Go, Rust, and C++ use AOT Compilation
              - The code is fully compiled into a native machine code binary before execution.
              - The resulting binary does not need a separate runtime like a JVM.
              - The OS can directly execute the binary.
    
    
    Deployment package
        - your app 
        - a package which contains your function code and/or depencies and configs that lambda will deploy
        - package =  ZIP or JAR or image
            - (not WAR. B/c WAR = Java web apps running in servlet containers like Tomcat or Jetty)
        zip
            - upload it to s3 or to lambda directly
            - 50 MB max ZIP.
            - 250 MB max after unzipping 
            - Cloudformation could zip your app
        container image
            - push image to ECR
            - 10 GB max image size.
            - Cold starts can be slower 
            - Cloudformation could build your Dockerfile
    
    
AWS Step Functions
    https://www.youtube.com/watch?v=s0XFX3WHg0w - AWS Step Functions with Lambda Tutorial | Step by Step Guide
    
    - is a serverless orchestration service to coordinate AWS services into serverless workflows
    - WORKFLOW ORCHESTRATION
    - based on state machines and tasks. 
    - an abstract model which decides how one state moves to another 
    
    - it's like a flow chart
    - a graphical console to visualize the components of your app as a series of steps
    - when things go wrong, it's easy to diagnose and debug problems
    
    - The entire Step Function (the state machine) 
        -> Is defined in a JSON
        -> you can build it in the UI and copy the JSON, someone called it 'workflow studio'
    
    - JSON-based language (uses the Amazon States Language).
         - JsonPath Syntax
    - "highly scalable and low cost"
    
    Terms: 
        workflows - a series of event-driven steps. 
        state - A step in a workflow
            eg. task state - a unit of work
        executions - a running workflows performing tasks
        activities - workers that exist outside of Step Functions.
        

    Key Features
        Workflow Orchestration
            – Coordinate services, 200+ (Lambda, AWS Glue, Amazon ECS, DynamoDB, and more)
        Visual Workflow Editor 
        Easy Error Handling & Retries 
            – failures, timeouts, and retries.
        Human interaction "Callback"
            - wait for irl human do something.
    Two Workflow Types:
        Standard Workflows 
            - general purpose
            – Suitable for long-running processes, supports retries, 
                and stores execution history for up to 90 days.
            - 0.025 / 1,000 state transitions
        Express Workflows 
            - for streaming data
            – Optimized for high-volume, short-duration tasks, running at a lower cost.
            - ~ $1.00 / 1M requests @ 64 MB Memory (linear scale for memory, eg 124 GB = ~2.00 / 1 mill req
            - ~ $0.000001 per request
        
    Use cases:
        Manage a Batch Job
            - user submits a batch job to AWS Batch
            - notify SNS if success or fail.
        Manage a Fargate Container
            - notify SNS if job success or fail
        Data Processing Pipelines 
            – ETL (Extract, Transform, Load)
        Microservices Orchestration 
            – Manage interactions between multiple microservices.
        Event-driven Applications 
            – Trigger workflows based on events from S3, DynamoDB, or other AWS services.
        AI/ML Workflows 
    
    Types of States:
        Recall, all these stats are written and defined in JSON
        Task State
            - Calls an AWS service
                - you'll specificy what service to call (arn)
                - popular choices: Lambda, AWS Batch, ECS/Fargate
        Activity State
            - is a unit of work ouside of AWS
            eg) 
                - on premise work
                - human approval step
                - a phone
        Choice State
            - Branches execution based on conditions.
        Parallel State
            - Runs multiple branches concurrently.
            - stat machine doesnt not move forward until all states are complete
        ? Map state 
            - for parallel and batch processing. 
            - works with an Array
        Wait State
            -  Introduces a delay.
        Pass State
            - does nothing, for debugging
            - Passes input to the next state without modifying it.
        Fail & Succeed States
            - Ends the execution with a failure or success.
            - Doesnt have a "Next" field
            - can specify Cause (reason for Fail).
    
    Input and Output
        - you pass data between states and steps
        - Each state receives input and produces output.
        - passes and consumes JSON
            {
              "orderId": "12345",
              "customer": "John Doe"
            }

        
AWS Elastic Beanstalk
    Recall
        Platform As a Serivice (PaaS)
            - a cloud computing model that provides a platform for developers 
                to build, deploy, and manage applications 
                without worrying about the underlying infrastructure. 
            - PaaS includes everything needed for application development
                - OS , runtime environments, databases, middleware, and development tools.
            - "Focus on Code" "nice for developer"
            - usually expensive, b/c hopes that you suck
            
    - EB is a PaaS
    - not recommeded for "Production" apps, --> actually, they mean "enterprise" apps
    - simplies
        - auto scales
        - monitoring
        - deploying
        - versioning
    - Pay only for the resources you use
    - Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker., and more
    
    - creates:
        - auto scaling groups
        - optionally a applicaiton load blaancer
        - EC2s, SQS, Cloudwatch Alarms to scale ASG

    - Two Types:

    Web Environment
        Use Cases: 
            - Web apps (eg, Flask/Django/Rails )
            - REST APIs (eg, Node.js/Express or Spring Boot)
            - Frontends or dashboards
        - Web is for web apps
        - handles HTTP(S) requests using a web server.
        - APIs, and services that respond to incoming web traffic.
        Uses:
            - Autoscaling group (ASG)
                - EC2s
                - ASG across multiple AZs
            - probably has Application load balancer (ALB)
            - Apache, Nginx
        Two types of Web Envs
            1. Load Balanced Environment
                - Has a Load Balancer (ALB)
                - "Production" applications that need scalability and fault tolerance.                
            2. Single Instance Environmen
                - lightweight (cheaper)
                - one EC2 instance (no load balancer).
                    - ASG's desired-capacity is set to 1
                Use Case: 
                    testing, staging, or personal projects.
                
    Work Environment
        Use Cases:
            - Sending emails
            - Generating reports
            - Image/video processing
            - Data aggregation tasks
        - long running jobs
        - processes background tasks asynchronously using an SQS queue.
            - batch processing, data transformation, or scheduled tasks.
        Uses:
            - Autoscaling group (ASG)
                - EC2 for ASG
            - SQS
            - installs SQS Daemon on EC2s
            - Cloudwatch Alarms (to scale ASG)
        
    Deployment Options:
        All at once 
            - turn off all instances
            - start up new instances
            - fastest but downtime for a bit
        Rolling
            - shutdown a few instances
            - spin up a few new instances (buckets)
            - wait until healthy
            - then move onto a few more instances (next bucket)
        Rolling with additional batchs
            - like rolling but:
            - spins up a few instances on the side
                - all previous instances are still up
            - then swaps out few old instances with new ones
            - repeat next batch until complete.
        Immutable
            - spins up all new instances in a temp ASG
            - merges the new instances in with the old ASG (thus double atm)
            - old instances are terminated.
            - temp as deleted
            - high cost, double capacity, long deployment, zero downtime
        Blue Green
            - Not a direct feature of Beanstalk but you can do it
            - route 53 can be weighted to redirect some 10% to green (new), and 90% to blue
        
    PaaS (vs) Serverless 
        Infrastructure Management	
            PaaS: Hidden from users but still requires defining resources (e.g., container sizes, instances)	
            Serverless: Fully managed; no need to provision servers or scale resources manually
        Scaling	
            PaaS: Auto-scales based on traffic, but may require configuration	
            Serverless: Automatically scales down to zero when not in use, scaling up per execution
        Billing	
            PaaS: Charged based on provisioned resources, even if idle	
            Serverless: ay-per-execution; charged only when functions are invoked
        Use Case	
            PaaS: Best for running long-running applications, APIs, and microservices	
            Serverless: Best for event-driven workloads, short-lived tasks, and microservices
        Startup Time	
            PaaS: Generally always running, reducing startup latency	
            Serverless: Cold starts can add latency, especially in infrequently used functions
        Persistence	
            PaaS: Typically has persistent storage and long-running processes	
            Serverless: Stateless; each function execution is independent
        
        
AWS Kinesis
    - for real-time data streaming. 
    - real-time streams. real-time streams. real-time streams. real-time streams.
    - ANALYTICS
    - data processing at scale
    - stream data (not just batch)
    - ingest data and process in a durable, secure, scalable way
    - "you collect, process, and analyze large amounts of data in real-time"
    - for Data Engineers, Data Analysts, Big Data.
    - better than batch analyics, time for real-time analytics YEAH
    - confusing for us b/c 4 services, with overlap >.<
    
    - like Kafka
    
    Uses cases -> real-time data
        - Stock prices
        - game data (as the player plays)
        - social network data
        - geospatial dat
        - Log Files
        - IoT Device streams
        - Clickstream Events (people clicking on webstie)
        - Telemetry Data (ppl driving, speed, seatbelts, braking.)
        - analytics
    
    NOTE:
        Data Pipelines & Streaming Analytics 
            - Kafka, Kinesis, Flink
            Purpose: High-throughput, real-time event processing, analytics, and data transformation.
            
  4 services in "Kinesis" family:
    1. Kinesis Data Streams (KDS)
        - for Ingestion
        - "temporaray storage"
        - for real-time analytics, monitoring, logging, and machine learning.
        - you send processed records (data) to dashboards, generate alerts, or dynamically change pricing
        - very similar to Firehose, but FH is cheaper
        
        Producers     Data Stream         CONSUMERS
        ---------------------------------------------------
        EC2         |   Shard 1   |       RedShift
        Mobile ---> |   Shard 2   | ----> DynamoDB
        Client      |   ...       |       S3
        Server      |   Shard n   |       EMR
        
        Producers 
            - things that send real-time data into a Kinesis stream 
            - EC2s, IoT devices, web apps, servers, logs, etc.
            - the data is stored in shards
            - *PutRecord* and *PutRecords* are used to put data into stream
            
        Consumers 
            - process the data for real-time analytics, monitoring, or further storage.
                - Redshift, DynamoDB, S3, AWS Lambda, EC2, ect
            - can have multiple different consumers
            - the consumers have a pointer
            - a stream has 24 hours of records  
            - the consumer have a pointer to which record they are at, 
                - so they consumer at their own rate
            - the data is like a database (its not)
            - the data is durable, but temporaray
            - If you're app (consumer) is making a mistake, 
                it can roll back it's pointer 18 hours backwards and replay a from that point forward
        
        key units:
            1. Stream
                - a group of shards where data is continuously ingested.
                - the big thing wrapping all your data/shards
            2. Shards
                - determines how much data a stream can handle.
                - "unit of capacity"
                Each shard:
                    Ingests up to 1 MB/sec (or 1,000 records per second).
                    Outputs up to 2 MB/sec.
                More shards = higher throughput.
                - each shard has a ordered list of records

            3. Records
                - is a unit of data sent to KDS.
                - has a Data Payload/BLob (eg, JSON, text, binary).
                - has a Partition Key (used to route data to specific shards).
                - has a Sequence Number (an ID for tracking).
                
            4. Retention Period
                - default 24 hours.
                - Can be extended up to 365 days.
            5. Partition Key 
                - Data Routing Mechanism
                - is a user-defined identifier for which shard a record is assigned to.
                - when putting data into a stream IT MUST HAVE A PARTION KEY
                - eg k_client.put_record(PartitionKey="user123")
            6. Sequence Number
                - a unique identifier assigned to each record within a shard.

        NOTE:
            - you dont direct producers which Shard you're sending data to
            - b/c your consumers cannot query records based on Partition Key
                - Kinesis does not provide a direct way to query records based on Partition Key
                - you will still fetch data by shard (rather than querying like a partion-key)
            - Kinesis hashes the Partion Key to dertermin which shard the record goes inot
                - same parition key, same shard
                    partiton key = "user_123"
                    hash(user_123) ---> 1889ce ---> Shard #3
            - You fetch data by shard (not by Partition Key).
            - If multiple Partition Keys exist in a shard, 
                you must filter them in your consumer logic.
            - Yes, oddly, you consume not from the Stream as a whole, but your
                consumers read data from individual shards.
                
            Q: but if I want to read data based on time, if I have multiple shards, 
                how do I read data base on most recent to oldest?
            A: You can't. 
              - can't read across all shards in a global time-ordered fashion.
              - but ordering within a shard is gaurenteed, but not across shards. 
                 This design is intentional and based on how Kinesis scales

            Use Case (again):
            - You'd need a stream without strict global ordering in most real-world, 
                high-throughput, horizontally scalable systems.
                a. IoT Sensor Streams
                    - You only care about time-order within each device, not across devices.
                b. Clickstream Data / Web Analytics
                    - Ordering within each user/session is useful, but global order isn't.
                c. Application Logs
                    - want order per app/server/microservice instance.
                d. Metric Collection (e.g., Prometheus-style metrics)
                    - per VM not global
                e. Fraud Detection (or Anomaly Detection)
                    ""

        Two Capacity modes 
            (for Kinesis Data Streams)
            1. On Demand
                - for unpredicatable workloards
                - AUTO SCALES
                    - simplicity
                - 200 MiB / sec
                - 200,000 records / second
                Cons:
                    - money $
                    - cold starts
                    - if you have bucket loads, the max might be too little for your needs
                    
            2. Provisions
                - for predicatable workloards
                - customer managed
                    - You specify number of shard
                    - no scaling
                    - complexity
                - 1 MiB / second / shard
                - 1,000 records / second / shard
                - Shard max is 200
                Cons:
                    - under/over provisioning
                    - effort --> configuring shards
                    
        Kinesis Enhanced Fan-Out (EFO)
          (for Kinesis Data Streams)
          
          Producers     Data Stream         CONSUMERS
          ---------------------------------------------------
          EC2         |   Shard A      +--> RedShift
          Mobile ---> |   Shard B -----+  
          Client      |   Shard C      +--> S3
          
          - Each shard can pipe data in each consumer
          - EFO is a data delivery feature for KDS 
          - multiple consumers can read data in parallel with dedicated throughput.
          
          - By default, all consumers share the same 2 MB/sec read limit per shard (competing for data). 
          - With EFO, each consumer gets its own 2 MB/sec dedicated pipeline, 
              eliminating latency issues and read contention.

          🚀 best choice when multiple consumers need to process data independently without delays.
          - but cost money

        
        A.
        Kineseis Producer Libary (KPL)
            (for Kinesis Data Streams)
            - a java library to publish data into Kinesis data streams
            - by AWS
            - for high-volume
            - when you need a highly efficent producer
                - 100x vertical scaling
            - an optimized, high-performance library designed to efficiently write data to 
                Kinesis Data Streams (KDS). It helps batch, compress, and aggregate records, 
                reducing costs and improving throughput.
            Why Use Kinesis Producer Library (KPL)?
                - recall AWS SDK (PutRecords) does not scale
                ✅ Higher Throughput – batches and aggregates records to reduce API calls.
                ✅ Lower Costs – Reduces PutRecords API calls
                ✅ Automatic Retries
                ✅ Asynchronous Processing – sends and forgets
                ✅ "Multi-Languages" - Java natively but MultiLangDaemon= multi lang.
        B. 
        Kinesis Client Library (KCL) 
            - is a Java library to read Kinesis Data Streams. 
            - for high-volume
                - b/c consuming at the CLI or AWS SDK/boto3 level doesnt scale
            - simplifies shard management
            ✅ Automatic balancing Shards
            ✅ Checkpointing (DynamoDB) 
            ✅ Parallel Processing
            
        
        - Can integrate with AWS Lambda, Amazon S3, and Amazon Redshift for further processing.
    
    2. Kinesis Data Firehose
    
          Producers     Firehose         CONSUMERS
          ---------------------------------------------------
          EC2         |   Shard A      
          Mobile ---> |   Shard B ----> S3 or Redshift or Elasticsearch ... ONE CONSUMER
          Client      |   Shard C
          
        What Firehose Does
            1. You send data to Firehose
            2. It get buffered
                - Buffering data = collect records, and batch them records together
            3. Optionally, transforms it (via Lambda or record format conversion)
            4. data goes to destination
            
        - formerly "Kinesis Firehose Delivery Systems"
        - fully Managed
            - No shard or throughput management
            - No consumer applications to build
        - loads streaming data into destinations like Amazon S3, Redshift, Elasticsearch, Splunk, and OpenSearch.
            - into data lakes, warehouses, and analytics services
        - transform, and deliver data streams within seconds
        - ***one consumer***
        - No storage, just delivery
            - ***data immeidately disappears once consued***
        - tranforms data
            - KDS does not
        - batching feature
            - eg. batches data into 5 MB or 60 seconds
            - KDS doesnt not batch
        - single destination
            - no consumers, instead predefined destination
        - simple data transform
            - Convert JSON to Apache Parquet or ORC format.
            - compress data to gzip, zip, or snappy.
        - you pay-on-demand based on how much data is consumed
        - easy to use
        - could be ligher than KDS
            
        Desinations:
            - ***S3, Readshift***, Opensearch, Splunk
            
        Dynamic Partitioning (firehose)
            - "continously partition streaming data" by using keys with data 
                and then deliver data grouped by these keys into s3 by Prfixes
            - s3 prefixes!!!
            - automatically route and store data into partitions based on specific fields 
                eg, user_id, event_type, region
            - cannot turn off once enabled
            - means sorting the data in the s3 based on specific attributes 
                (like user_id, event_type, or region).
            - Unlike traditional Firehose behavior, which delivers all records to a single static destination, 
                dynamic partitioning continuously evaluates each record and routes it to the s3 path dynamically.
                eg)
                    s3://my-bucket/logs/year=2024/month=03/user_id=123/
                    s3://my-bucket/logs/year=2024/month=03/user_id=456/

        How to use it:
            1. Create a Firehose service
            2. Configure your destination (eg, S3 bucket)
            3. Send data via:
                AWS SDK/API (PutRecord, PutRecordBatch)
                AWS services (eg, CloudWatch Logs, IoT Core, EventBridge)
                Kinesis Data Streams (as source)
            4. Firehose buffers, transforms and/or Compress data (optionally), and delivers
        Use cases:
            1. Log Aggregation to S3
            2. Real-time Analytics with Redshift
            3. Search and Monitoring via OpenSearch
            4. Deliver Data to Splunk
            5. IoT Data Collection
            -> log ingestion, analytics, search, and archiving.
            - good AWS Glue synergy
    
    3. Managed Service for Apache Fink
        - (formally Kinesis Data Analytics)
        - run queries against data flowing through your streams
            so you can create reports and analysis on emering data.
        - process data using SQL, Apache Flink, or Java applications. 
            - Data Joins
            - Time Window Analysis
            - Late Arriving Data
            - can do wacky things
        data steps: 
            1 Input(Kinesis, Kafka, S3, DynamoDB, etc.) 
                ->
            2 Apache Flink Processing  
                -> 
            3 Output (S3, Redshift, Elasticsearch, DynamoDB, etc.) -> :)
    
    4. Kinesis Video Streams
    
          Producers            K. Video Streams     CONSUMERS
          ---------------------------------------------------
          Security Cam ---> |            |
          Web Camera   ---> |  Kinesis   |     ----> Sage Maker
          Mobile       ---> | (no shards |     ----> Rekognition
          Drone             | in Vid Streams)
          Body Cam
          
        - for ingesting video from external sources (like cameras, IoT devices, and sensors) 
            into AWS for storage, processing, or analytics. 
        - not designed for directly streaming video to customers on your website
        - for analytics, machine learning, and playback, Amazon Rekognition, SageMaker
        - no shards
        - no parition key
        - yes multiple consumers, using same video stream
        ❌ Not a video hosting or streaming service for customers
        ❌ Not for broadcasting live events or on-demand streaming
            
        
            
NOTE THIS ABOUT 
-Features for: 
    Pub/Sub (SNS) 
        --vs-- 
    Queue (SQS, RabbitMQ) 
        --vs-- 
    Data Pipelines & Streaming (Kafka, Kinesis, Flink)

    Message Delivery Model	
        Pub/Sub (SNS): Push-based (sends messages to subscribers immediately)	
        Queue (SQS): Pull-based (consumers request messages)	
        Kinesis: Pull-based (consumers fetch messages from partitions)
    Retention of Messages	
        Pub/Sub (SNS): No retention (messages disappear after delivery)	
        Queue (SQS): Until processed or expires	
        Kinesis: Fixed retention period (eg, 7 days, even if read)
    Message Ordering	
        Pub/Sub (SNS): No guaranteed order	
        Queue (SQS): FIFO (only with FIFO queues)	
        Kinesis: Ordered within partitions
    Message Replay	
        Pub/Sub (SNS): ❌ No	
        Queue (SQS): ❌ No	
        Kinesis: ✅ Yes (messages replayable by adjusting offsets/pointer)
    Scalability	
        Pub/Sub (SNS): Supports many subscribers	
        Queue (SQS): Scales via multiple queues	
        Kinesis: Massively scalable via partitions
    Consumer Model	
        Pub/Sub (SNS): Many subscribers get the same message	
        Queue (SQS): Each message is processed once	
        Kinesis: Multiple consumer groups can process the same message independently, 
            on their time, whenver (Message NOT deleted via an ACK)
    Use Case	
        Pub/Sub (SNS): Trigger events (notifications, system alerts, Lambda functions)	
        Queue (SQS): Task queues (job execution, processing pipelines)	
        Kinesis: Streaming analytics (log processing, fraud detection, IoT data streams, event-driven apps)
            
        
RECALL
    In-Memory Data Store
        - a high-speed database that keeps all data in RAM instead of disk-based storage like HDDs or SSDs
        - queries execute instantaneously
        example: Redis & Memcached

        Caching:
            - defined as temp storage for data desinged for fast retrieval, often, not durable
            - Often in-mem db is used for caching
            - hit/miss cache model
            - stores a copy, not the source of truth.
            - short term
                - has a TTL, until data is evicted.
            - acts like a glorified key-value hashmap
            - GET data only.
            - does not have Edge location. Just 1 DB (unless you go ham)

        App? (sorta database):
            - store relevant info but update a 'real' persistent database also
            - Session Storage
            - Chat Systems
            
AWS ElastiCache
    - in-memory data store ("RAM" database)
        - "a fully manage in-mem datasotre"
        - automatic scaling, multi-AZ failover
    - speeds up data retrieval using caching techniques.
        - reduces the load on databases 
    - can scale to millions of operations per second 
        - microsecond response time.
    - ! only accessible by resources in the saem VPC
    - can replicate cross-region via ElastiCache Global Datastore
    Options:
        Redis 
            - An open-source, in-memory data store that supports 
               complex data structures like lists, sets, sorted sets, and hashes.
            - Allow persistence by saving snapshots to disk periodically.
        Memcached
            - A simpler, multi-threaded caching solution primarily used for storing key-value pairs.
            - Store data only temporarily in memory and lose it upon shutdown.
        Valkey
            -also key-value like redis?
        
    Two Deployment options:
        Self-designed cluster
            - aka "Provision mode"
                - (andrew calls it standard idk why)
            - you manually manage cache nodes
                - instance types, configs, scaling strategies
                - choose: the node-type, number of nodes, and nodes in AZ
            - for predictable traffic
            - fine-tuned control over cache
            - multi-AZ support (high availability)
        Serverless
            - auto scaling and managed by AWS
            - for unpredictable workloads
            - good for variable or unpredictable workloads
            - pay per use 
        

AWS MemoryDB 
    - is very similar.
    - persistence gaurentees (unlike ElastiCache)
    - making **MemoryDB suitable as a primary databse***
    - writes are in the miliseconds
        - vs elasticache, writes are in the microseconds
    -> slower write but gaurentee of persistent data in MemoryDB
    - is redis but high durability b/c multi-AZ 
    - and has transactional logging (w/e that means)
    - and has and persistent storage (w/e that means, also)
    

AWS ClouldTrail
    - AWS account owners can ensure every API call made to every resource in their AWS account 
        is recorded and written to a log.
    Records identity:
        - Idenity includes:
            - Where: source IP
            - When: time
            - Who: user/user-agent
            - What: Region, Resource, Action
        - Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail
    - good for risk auditing, operational auditing, governance, and compliance 
    - Is ON by default. last 90 days
    - Action include:
        - AWS Management Console, 
        - AWS CLI
        - AWS SDKs and APIs.
    Record events:
        - in "Event History" = immutable record of the past 90 days of management events
            - 90 days max
            - on by default
        - in "Trails" = activities stored in an Amazon S3 bucket,
            - 90+ days or greater
            - you must turn on
            - Goes in to S3
                - Since it's s3 => no GUI => thus must use Amazon Athena
        - events are in "CloudTrail Lake" = a data lake
            => CloudTrail Lake converts existing events in row-based JSON format to Apache ORC format
    
    3 Event types::
        1. Management Events
            - actions on AWS resources 
            - on by default
            - "control plane event"
                - creating, modifying, and deleting resources.
            Examples:
                - Creating an EC2 instance
                - Changing IAM policies
                - Modifying an S3 bucket configuration
        2. Data Events
            - actions on data within AWS resources
            - off by default
                - S3 object-level operations or Lambda function executions.
                - queries on dynamoDB
            Examples:
                - Reading or writing an object in an S3 bucket
                - Invoking an AWS Lambda function
        3. Insights Events
            - help detect unusual activity in API usage patterns and provide insights into anomalous behavior.
            Examples:
                A sudden spike in API calls
                An unusual increase in failed login attempts
            - it requires activation.
            - is useful for security monitoring and operational troubleshooting.
    
RECALL:
    Database transcation
        - a transaction is work performed in a database
            eg) read and write
    Database
        OLTP
        Online Transaction Processing
            - a database built for single specific transactions
            - single
            - long transactions
            - reads!
            eg) adding items to you shopping list
        OLAP
        Online Analytical Processing
            - a database built to store lots of data, for fast complex queries on ALL data
            - multiple 
            - fast transactions
            - read and write (but more read)
            eg) Generating reports

DATA WAREHOUSE
    - a centralized database designed to analyze large volumes of structured (SQL) 
        data from multiple sources. 
    - optimized for querying 
    - for business intelligence (BI)
    - massive amounts of data
    eg) Imagine you run an online retail store (like Amazon). data coming from multiple sources:
        Website Transactions
        Customer Data
        Marketing Data
        Supplier Data
    - If you want to analyze trends like 
        "Which products sell the most during Christmas?" 
        "Which customers are likely to return?", 
        - then need a data warehouse
   
    - Extract: Data is collected from sales, marketing, and customer databases.
    - Transform: The data is cleaned and structured (eg, fixing missing values, standardizing formats).
    - Load: the transformed data is stored in the data warehouse
    

    - Yes, a data warehouse stores a copy of data from multiple other databases.
    - Yes, most data warehouses are relational databases(tables, rows, columns).
    - No, a data warehouse is NOT meant for customer-facing apps or high-traffic transactional systems.
        - Its for business Intelligence
        - for large, complex queries, not quick transactional reads/writes.
        - updated periodically (hourly, daily, etc.), so it isn't real-time.
        
    - Instead, it consolidates data from various systems into a single, structured repository 
        optimized for analysis. 
    
    
    Warehouse (OLAP) vs Transcational Database (OLTP)
    Name
        Database: OnLine Transaction Processing (OLTP)
        Warehouse: OnLine Analyitical Processing (OLAP)
    Purpose	
        Database: Fast, real-time transactions (eg, shopping cart, payments)	
        Warehoue: 'Slow', Reporting. Complex queries, reports, analytics
    Data Structure	
        Database: Normalized (minimizing redundancy)	
        Warehoue: Denormalized (optimized for querying)
    Reads/Writes	
        Database: Heavy reads & writes	
        Warehoue: Mostly read-heavy
    Example	
        Database: MySQL, PostgreSQL (used in apps)	
        Warehoue: Snowflake, Redshift, BigQuery
    
AWS RedShift
    - a data warehouse
    - petabyte sized
    - $0.25 / HOUR (????)
    - 1,000 / terabyte / year
        - pay per query too? seems complex
    - scale up to petabytes
    - 1/10 the cost of most similar services
    - COLUMNAR Storage
        - reduces I/O by a lot
    Use Case:
        - you want to power a Business Intelligent tool
        - it would need to copy the data from:
            EMR, S3, DynamoDB
        -> and put it in Redshift
        - after, we use some other app to query Redshift for data
         
         
    Configs
        Single node mode
            - 160 GB compute instance
        Multi Node mode
            - has 1 leader and 2-128 Compute nodes ---> 128 max!
            Leader Node
                - manages compute nodes
                - receives queries and distributes quries
                - coorinates, doesnt not compute itself
                - ** You are not charged for the leader node only for compute** 
            Compute Nodes
                - Stores data
                - processes queries
    Node Types
        - Dense Compute (dc)
            - high performance
            - less storage
            - old
        - Dense Storage (ds)
            - deprecated, no longer available
            - but suppose to have more storage
        - RA3 Nodes
            - NEW
            - compute & storage
            - "If you expect your data to grow, we recommend using RA3 nodes 
                so you can size compute and storage independently to achieve the best price and performance."
    Backups
        - enabled by default @ 1 day
            - optionally max 35 day retention
        3 copies of data
            1. the OG data
            2. replicated on the compute nodes
            3. backup in s3 

    Availability Zones
        - SINGLE AZ
        - redshift is single AZ, 
        - you can duplicate/clone it into another zone, 
            - but it's all manual, it's not a AWS feature, you do that work
        - you can restore redshift into a differetn AZ if you wish

    Massively Parallel Processing
        - Redshift uses MPP
        - DATA & QUERIES ar automatically distributed across all nodes
        - a large query is split into chunks and processed in parallel
            - divide and conquer the workload.
            - many processors/nodes work simultaneously to process data
        - easy to add new nodes
        
        Tradition Processing
            - a single server (many CPUs) process the query
        
    Columnar Storage
        - storing databases column-by-column (instead of row-by-row)
        - reduces overall disk I/O and data you need to load from disk
        - OLAP
        - good for SUM() AVG() COUNT() (analytical queries)
        
        eg. (sorta, rotate your head 90 degrees)
            Order_ID:       101,    102,    103,    104
            Customer_Name:  John,   Jane,   Mark,   Anna
            Product:        iPhone, Laptop, TV,     Shoes
            Amount:         799,    1200,   1500,   200

random fun fact
- EC2 (Ephemeral Storage)
    - is not encrypted by default and cannot use KMS-based encryption.
    recall Ephemeral Storage = Temporary, is physically attached to the EC2 instance host machine.
        -  Data is lost when the instance stops, terminates, or crashes.
        
        
        
AWS Athena
    - a query service for S3
    - serverless: you dont provison servers or w/e
    - often for big data analysis
    - often in the web console
        - but can use CLI or SDK
    - pay for the data scanned
        - $0.05 / 10 GB 
        - $5.00 / 1 TB scanned
    - use "SQL" on raw data in S3
        - s3 data stored as CSV, Json, Parquet
    - based on "Tirno", froked from Apache Presto
    - Can use "Apache Spark on Athena" whish is Athena through Spark
    Pros
        - No need for special query programs, eg EMR cluster
        - great for infreqeunt queries on huge datasets
        - Powerful when combined with data ingestion services like Kinesis Firehose
    Cons
        - Queries are asynchronous, best effort execution
            - could be minutes late, its not always promised to be fast
        - Large queries take longer and are more expensive
        - Performance limitations for complex queries
        - Not suitable for latency sensitive applications
    Athena Components
        Workgroup
            - a group of saved queries
            - you grant permissions who can
            - allows you to control costs, apply query execution policies, and monitor query usage.
            - query limits, output locations
        Data source
            - location where Athena queries data from.
            - WHERE
            - group of databases (s/times called a catalog)
            - S3 normally
            - can query on-premise or cloud-based databases using AWS Lambda-based connectors
            - ??? Cloudformation, cloudfront, cloudtrail, AWS Glue, Quicksight, ect...
        Database
            - a group of tables (aka schemas)
        Table
            - structured format of your dataset
            - it defines the schema
            - based on data stored in s3 and requries schema defintions (via Glue Data Catlog or CREATE TABLE)
            - "data organized as a group of rows or columns"
            Created two ways:
            SQL create table statement
            AWS Glue Wizard
                - the Glue crawler will crawl your data to produce a table schema, automatic, magic
                - Athena tables are AWS Glue Data Catalog tables, so they will exist in both services
                    - after creating a Athena table
                - tables are auto created used a Glue Crawler
                    - it crawls the data to produce a schema
                    - the product is a "Glue Data Catalog" table,
                    -> thus they will exist in Glue and in Athena
        Dataset
            - the actual data you're querying
            - WHAT
                - raw data
            - structred or unstructed stat in s3, databsae, or w/e
            - represnted as tables that point to data stored in S3
        
        data types
            - Athena has boolean, int, float, smallInt, String, array, ect,
                    
        SerDe 
            - short for Serializer/Deserializer
            - "is a ser/derser libray for parsing data from different data formats
                such as CSV, JSON, Parquet, and ORC
            - In AWS Athena and Apache Hive 
            - convert data between its raw format (like JSON, Parquet, CSV) and a structured table format 
                that Athena can query using SQL.

            - Since data in S3 can be stored in various formats (eg, CSV, JSON, Parquet), 
            - Athena needs a SerDe to interpret and parse that data correctly.
            - it is built in to Athena
            - you reference it in your Athena SQL table definition.
                CREATE EXTERNAL TABLE my_table (
                  id STRING,
                  name STRING,
                  age INT
                )
                ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
            
            or
            
                `ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'`

RANDOM SQL
    DDL (Data Definition Language)
        - a subset of SQL that defines schemas/tables
            eg) CREATE, ALTER, DROP

    DML (Data Manipulation Language)
        - a subset of SQL that changes data
            eg) INSERT, UPDATE, DELETE

    DQL (Data Query Language)
        - a subest to select data
            eg) SELECT
        
        
AWS CodeGuru 
    - "improve code quality and performance using machine learning.  "
    - performs code-reviews
        - makes suggestions
    Has three services:
        CodeGuru Reviewer
            - identify security vulnerabilities, hard-to-maintain code, and best practice violations.
        CodeGuru Profiler:
            - fix inefficiencies
                eg) CPU utilization, latency issues.
            - identify performance bottlenecks in applications.
        CodeGuru Security
            - security, secrets detection, and other common risks.

AWS Comprehend 
    - analyze text and extract info
    - a natural language processing (NLP) service 
    - machine learning
    Evaluates:
        Entities
            - Persons, Orginizations, Location
        Key Phrases
            - Text that apears important
        Language
            - eg English
        PII   
            - Personal identifing information
        Sentiments
            - angry, sad, ect
        Targeted sentiments
            - Positive/neg/neutral, on location/person/orginzations
        Syntax
            - adjective, noun, ect
        - you can use custom if desired.
    - is 'serverless'
    - you pay for size of request
        eg 1 unit = 100 characters
        
Amazon Forecast 
    - predict future trends based on historical data, 
    - no expertise in machine learning required.
    - generate time-series forecasts. 
        - product demand, resources needed, financial performance, ect
    - you upload your data to s3
        - Upload historical time-series data (e.g., sales, inventory levels).
        - CSV 
    - Supports both traditional statistical methods & ML ?
    - Uses IAM for secure data handling.
    - Confidence Intervals
    - Forecast Horizon (how far into the future)
    Pay-as-you-go:
        Data storage
        Training hours
        Forecast generation
    Use Cases
        Retail & E-commerce
        Finance
        Energy & Utilities
        Manufacturing

AWS Fraud Detector 
    - identify fraudulent activities in real-time. 
        payment fraud
        new account registrations
        other suspicious behaviors
    - uses machine learning 
    - comes with predefined models
    - which you setup and they "train" on
    - you'll use the Botos sdk to "train" your model ontop of one of these:
        Online Fraud Insights 
            - optimized to detect fraud when little historical data is available
            - For: user signing on, or fake account registration
        Transaction Fraud Insights 
            - risk of fraudulent transactions
            - uses historical data
            - for: credit card fraud
        Account Takeover Insights 
            - if an account was compromised by phishing or another type of attack.
            
Amazon Kendra 
    - "enterprise ML search engine service"
    - NLP and ML search service 
    - RAG model 
        - but some differences
            - RAG is Retrieval & Generation
            - Kendra is just Retreval
    - searches content instead of keyword-based results (like traditional)
        - "uses semantic and contextural understanding capabilities"
    - Understands natural language queries (e.g., "How do I reset my VPN password?")
    - Pre-trained on domains like healthcare, finance, IT, insurance, and more
    - Supports integration with SharePoint, OneDrive, S3, Salesforce, Confluence, Box, and more
    - reads PDFs, Word documents, PowerPoints, HTML, and FAQs
    - Uses OCR to extract text from scanned documents and images
    Use Cases
        🔹 Customer Support & Help Desks
        🔹 Internal Knowledge Management
        🔹 Healthcare & Life Sciences – retrieving patient data, research papers, and clinical trial documents.        
        🔹 E-commerce & Retail – Enhances product discovery
    - Getting Started with Amazon Kendra
        🔹 Create an Index
            - searchable content
        🔹 Connect Data Sources
            - S3 Sharpoint, Salesforce, RDS, ServiceNow, ect
        🔹 Configure Access Control – restrict search results based on user roles.
        🔹 Optimize & Train
        🔹 Document Addition API - an API to add documents directly to an index
        🔹 Deploy & Query
        
    Two Versions
        Enterprise Edition
            - all features
            - 5 indexes
        Developer Edition
            - smaller, limited version
            - 5 indexes
        
Amazon Lex
    - chatbot
    - build conversational interfaces into applications
    - voice and text. 
    - has natural language understanding (NLU) and automatic speech recognition (ASR)
    - enabling users to interact with apps the way they would with another person.
    Bot
        - performs an automated task, it's the input where interaction begins 
            with the conversation model
        Components:
            Version
            Alias
            Language

    Amazon Lex Netowrk of Bots  
        - a netwrok can intelligently route the query to the appropriate bot
        - "add multiple bots to a single network"
        -> apparentlys improves experience, reduces duplicat intent?
    Intent
        - Example: "BookFlight", "CheckWeather", "OrderPizza"
        - Bots use intents to determine what the user wants to do.
        - Represents the action the user wants to perform.
    Utterances
        -  The various ways a user might express an intent.
        - help the bot recognize different phrasings of the same goal.
        - "request user's intent"
        - text that might come from the user
        eg)
            "can I order a pizza"
            "yo dawg, I want a slice of pie, can you dig it"
    Slots
        - Pieces of information needed to fulfill the intent.
        Example for "BookFlight":
            Origin: “New York”
            Destination: “Paris”
            Date: “April 10th”
        - String, Enum, Number
        - The bot prompts the user for missing slot values if they aren’t provided.
       
    Fulfillment
        - The action taken to complete the intent.
        - Could involve calling an API, sending a message, storing info, etc.
        - Often uses AWS Lambda or some backend logic.
        - Example: 
            Once all flight info is gathered, call an airline API to book the ticket.
            
    Responses / Messages
        - What the bot says back to the user.
        Can be text, voice, or rich responses like buttons or cards.
        Helps guide or confirm steps in the interaction.
        
    Context (optional)
        - Memory of the conversation state or previous interactions.
    Error Handling
        Definition: How the bot deals with confusion or missing info.
        Examples: “Sorry, I didn’t get that. Can you rephrase?” or fallback to a human agent.
        
Amazon Personalize
    - creates recommendations
    - same tech used by amazon.com
    - "real-time recommendation"
    Dataset: 
        - A collection of structured data used to train models. 
        Three types of datasets:
            Interactions Dataset 
                – User activity data (eg, clicks, purchases, or views).
            Users Dataset 
                – User metadata (eg, age, location, preferences).
            Items Dataset
                – Item metadata (eg, product descriptions, genres, categories).
    Dataset Group: 
        - a group of datasets
    Solution
        - a trained model
        - created from a dataset
        - a selected recipe (alorithm)
    Recipe
        - a algorithm
        - popualrity, similar items, personalied ranking
    Event Tracker
        - use the "Ingestion SDK" for this
        - recordings of real-time user interactions (eg, clicks, page views, purchases) 
        - to be used for dynamic recommendations.
        - allows amazon to adapt to user behavior instantly rather than relying only on pre-collected batch data.
    Filters
        - Filters: Rules that allow customization of recommendations by excluding or prioritizing specific items.
    Campaign: 
        A deployed solution that serves recommendations via an API endpoint.
        Steps:
            1 Train a solution.
            2 Deploy it as a campaign.
            3 Get recommendations using the API.
            

Amazon Polly
    - text to speech
    - Standard $ - not natural (text-to-speach dono quality)
    - Long Form $$ - natural
    - Neural $$$ - crip natural (newcaster)
   Lexican
    - for speciallized pronunciated words
        - .xml, .pls
        - 100 pronunciaton rules (40,000 characts)
    Speach marks
        - metadata that describes the speech
        - Can use SSML (Speech Synthesis Markup Language)
            - xml that defines pauses, effects, volum, ect
        
Amazon Recognizion
    - computer vision
    - images
    - AND videos
    - will label many things
    - detects objects, scenes, and faces; extracts text; recognizes celebrities; and identifies inappropriate.
    - allows you to search and compare faces.
    Custome Lables
        - specific objects, logos, and scense in the image for your business needs
    
Amazon Textract
    - is OCR tech + cool features
        - extract printed text, handwriting, layout elements, and data from any document
    - bounding box coordinates
        - for locating where something originated from
    - you can use natural language queries to ask Textract questions about the document
    
Amazon Translate
    - localize content by translating text
    - can do real-time translations
    - can do batch translations
    - 75+ languages
    - can integrate with aws sercvices, eg S3,
    {
      "Text": "Hello, how are you?",
      "SourceLanguageCode": "en",
      "TargetLanguageCode": "es"
    }
    - u can define words unique to a certain industry get translated
    - auto finds languatge
    - can translate text and html files
    - 2 mill characters for free per months

AWS Data Exchange
    - datasets on aws
    - some free, some you pay 
    - some datasets are free, some you sub to
    - prob from someone's S3 bucket 
    
##################################
###           DATA             ###
##################################    
Recall
    ETL
     - An ETL service is a tool or platform designed to:
        Extract 
            data from various sources (databases, files, APIs, etc.)
        Transform 
            the data into a suitable format/structure (cleaning, joining, aggregating, etc.)
        Load 
            the transformed data into a target system (like a data warehouse or data lake)
AWS Glue
    - "a serverless data integration service for analytics users to 
        to discover, prepare, move, and integrate data from multiple sources"
    - is a ETL service
    - mainly to prepare and transform data from multiple sources
        - useful for analytics, ML, and app development
    - "discover and connect to 70 diverse data sources"
    - "manage your data in a centralized data catalog to visually create and run ETL pipelines, 
          to load into your data lake"
    Diverse data sources:
        - Datbases (MYSQL, PostgreSQL,ect...)
        - Data warehouse (Redshift, Snowflake
        - Object Storage (s3)
        - SaaS apps (Salesforce, Google Analytics
        - Streaming services (Kinesis)

    Note:
        - Not all ETL services are like AWS Glue
        - AWS Glue is serverless, 
            -> you don’t manage infrastructure. 
        - Is tightly integrated with AWS 
        - uses Apache Spark under the hood.

        Other ETL tools vary widely in architecture, features, and use cases.
        example:
            Apache NiFi 
                – flow-based, visual interface, often used in real-time pipelines.
            Talend 
                – more traditional, with GUI-based job design.
            Airbyte / Fivetran 
                – focus more on data replication (ELT), less on complex transformation.
            dbt 
                – handles transformations only (not full ETL), assumes data is already loaded.
    Glue Job
        - does the ETL
        - script engine options:
            Spark jobs
            Python Shell jobs (AWS glue feature)
            Ray jobs
            -> But you can use a cool Console UI to build it.
        - Apache Spark underneath it all.
        - Glue Jobs are the crux of AWS Glue, without jobs Glue is kinda pointless

        Example 
            - You have CSV files in S3 with sales data.
            - You want to convert them into Parquet format, filter by date, and save to a new S3 location.
            You’d:
            - Create a Glue job (in Glue Studio or script it):
                1 Define the source (S3 path)
                2 transformation logic (e.g., filter by date)
                3 Output (another S3 path).
            - Run the job or schedule it.

        - charged based on number of data processing units (DPUs)
        - Work Type + # of workers = DPUs
        
        - Job is ran: manually, scheduled, or event trigger
        Note 
            Apache Spark 
                - is an open-source, distributed computing system designed for big data processing. 
                - machines in parallel.
                - Data transformation (e.g., cleaning, filtering, joining, aggregating)
                "Real-World" Analogy
                    - chatGPT's real-world version lel
                - Imagine you have a massive spreadsheet (millions of rows), 
                - you need to calculate summaries for each region. 
                - Doing it on your laptop could take hours or crash. 
                - With Spark, you divide you spreadsheet, send each chunk to a team of computers, 
                - then then'll work at the same time
                - gg

    Glue Studio
        - a visual, no-code/low-code interface 
        - ETL jobs you create, run, and monitor  
        - Drag-and-drop interface
        - is free to use (but jobs still $, duh)
        - the code is in the console if you need it
        -> you can develop and test AWS Glue scripts locally 
        https://github.com/awslabs/aws-glue-libs
    
    Glue Data Catalog
        - "a fully managed Apache Hive Metastore compatible catalog service"
        - helps annotate and share metadata about your data
            - a centralized metadata store for all your data assets across AWS.
            - "database of databases"
        - Apache Hive Metastore
        - serverless
        Data Catalog
            - Used by AWS Glue Jobs
            - Used by Athena, Redshift Spectrum, EMR, SageMaker, and more
            - Supports schema discovery via Crawlers
        - chatgpt "one of the foundational components of AWS Glue and super useful when working with data at scale."
        
        Components:
            - Glue Table: the metadata about your data
            - Glue Database: holds many Glue Tables
            - Glue Crawler. It says:
                - What data you have
                - Where it's located (S3, RDS, Redshift, etc.) <--- connects to these
                - What format it’s in (CSV, Parquet, JSON, etc.)
                - What schema it follows (columns, types, partitions)
                - runs on demand or schedule


    Glue Crawler:
        - can discover schemas for your tables
        - scans your data then figures out the structure (schema) of that data, 
            and creates or updates tables in your Glue Data Catalog.
        - reads your raw data and builds a map
        - data sources:
            s3, Redshift, Snowflake, RDS, DynaoDB, MongoDB, Delta Lake, Apache Iceberg in s3, Hudi Tables in s3


Date Lake
    - centralized data repo for any data
    - vast amounts of data
    - stores structured, semi-structured, and unstructured data at any scale
    - uses object (blobs) or files as its storage medium
    - blobs:
        - Binary Large Object
        - type of data storage used in object storage (s3, Azue Blob, ect)
        - eg. text, images, video, logs
    Expect:
        1 Collect
            - injest data
            - gather raw data
            - from APIs, IoTs, databases, logs, spreadsheets, ect
            - JSON, CSV, images, videos, ect
        2 Transform
            - clean data
            - convert raw data into strucuted or semi-strucuted
            - filter, dedup, format
            - Spark, dbt, AWS Glue, Apache Beam
        3a Distribute
            - Move data to target systems, ie a Program or API
        3b Publish
            - share data
            - data is ready for consumption
            - publish to meta catalog for analyists 
            - APIs, Dashboards, deports, ML models, apps
    NOTE: 
        - AWS doesn't have a single "AWS Data Lake" service that you can just turn on. 
            Instead, they provide multiple services which together can build your data lake architecture.
            --> AWS Lake Formations
        - "data lake" is the combination of:
            Storage
                - Raw data (S3)
            Ingestion Tools 
                - to bring data in from various sources (Glue, Kinesis, DMS).
            Query & Processing
                - The ability to effectively query and analyze that data
            Metadata Catalog
                - to organize and describe the data (AWS Glue Data Catalog)
            Access Controls
                - manage permissions and auditing    

        
AWS Lake Formations
    - a data lake
        - store structured, semi-structured and unstructured data 
    - "Centrally govern, secure, and share data for analytics and machine learning"
    - STORAGE: is primarily built around Amazon S3 as the foundation
        - with other services to form a comprehensive data lake architecture.

    - METADATA in AWS glue Data Catalog
    - ACCESS: you can share data internally and externally accross AWS acounts, orgs, or direclty with IAM principals
        - permissions enforce granular controls at column, row,
            and cell-levels across: athena, quicksight, redshift, EMR, glue
            
    - Lake Formation is the governance and management layer for a data lake, but the actual data is stored in S3.
    What You Can See/Do:
        Data Catalog: 
            - databases, tables, and metadata
        Permissions
            - granular access controls
        Data Locations
            - Register S3 locations
        Data Lake Users/Roles
            - users
        Blueprints/Workflows
            - Set up ETL jobs and workflows with AWS Glue integration.
    What You Can’t Do (Directly)
        - you can't see raw data in a visual table
        - You can’t browse files
    To query or view the actual data, you typically use:
        Amazon Athena
        Amazon Redshift Spectrum
        Amazon EMR / Spark / SageMaker for more complex processing
        
    Note:
        - You do not have to use Lake Formation to build a data lake on AWS - it's a helper, not a requirement.
        - Lake Formation itself does not incur additional charges; it is provided at no extra cost. 
        - However, you will be billed for the underlying AWS services:​
            Amazon S3
            AWS Glue
            Amazon Athena, Redshift Spectrum, and EMR

            
OpenAPI 
    - describes RESTful APIs
    - language-agnostic interface to resful APIs
    - human & computer readable w/o access to source code, documentation, or network traffic inspection
    - what endpoints exist
    - what parameters they take
    - what responses they return
    - in yaml or json

API Gateway
    - a program that sits between a single entry point and multiple backends
    - api gatway allows for throttling, logging, authentication, routing logic of the request and response
    
AWS API Gateway
    - create, publish, maintain, monitor, and secure APIs at any scale.
    - **"create secure APIs, at any scale"**
    - the "front door" to your backend applications
                 +-------------------+
    Mobile ----> |                   |----> Lambda
    Web    ----> |    API GATEWAY    |----> DynamoDB
    Iot    ----> |                   |----> EC2
                 +-------------------+    
    Three types:
        REST API (V1)
            - Complete controll over request & response
                - complex
            - more features
            - higher costs
            - Public AND Private
            Use When:
                - You need full control over request/response transformations.
                - You want to use API keys, usage plans, throttling, and quotas.
                - You need request validation, models, or stage variables.
        HTTP API (V2)
            - low latency
            - low cost
            - less features than V1 :(
            - Public APIs only
            - has built-in JWT authorizers (for OAuth 2.0).
            Use When:
                - You need low-latency and low-cost.
                - You want a simple setup (eg for Lambda, HTTP backends, or AWS services).
                - You don’t need advanced features like usage plans, request validation, or fine-grained throttling.
        WebSockets API
            - for websockets
                - persistent connections
            - real-time
            - chat apps, dashboards, ect.
        - V2 is not better than V1, just different options.

    Features:
        - REST V1 option: 
            Edge-Optimized = API traffic is routed through CloudFront edge locations closest to the client, 
                    then forwarded to your API Gateway in its home region.
            Caching
            Canary release
            Custom Domains
            WAF
            Transform body
            Mock Integration
            NO JWTs (very odd,)
        - Both have CORS option
        - Both have custom domain name, you can set up
        - You can import OpenAPI 3 into AWS API Gateway
            - AWS extends OpenAPI's features via "x-amazon-apigateway-<extension>"
            - both REST V1 and HTTP V2 support it
    
    REST (V1) Components:
    
        -NOTE
        http request flow:    
        
            User ----> Method Req ----> Integration Req ----> INTEGRATION eg Lambda
                 <---- Method Res <---- Integration Res <---- 
                 
        API 
            - "container for multiple resources"
            - (wraps Resources)
        Resources 
            - represent an endpoint.
            - /hello
            - nested resource: /hello/world
        
        Methods Request/Response
            - HTTP method
            - GET, POST, DELETE, ect
            Request:
                - Defines how users can call the API
                    - what HTTP method (GET, POST, etc.)
                    - what parameters or headers are required
                    - what authentication is needed.
            Respone: 
                - Defines what kind of response the API promises to return 
                    - what status codes
                    - what headers
                    - what response body format

        Integration Request/Response
            - you can modify the response/request here
            Request: 
                - Maps the method request to the format your backend expects 
                    eg) turning query parameters into a JSON payload)
            Response: 
                - Translates the backend's raw response into the form that matches the Method Response 
                    eg) mapping a Lambda error to a 400 status code
                - this doesnt make sense to me
                    
        Integration
            - The backend service that API Gateway forwards requests to
            eg)
                - Lambda function (AWS_Proxy)
                - HTTP
                - Mock (REST API only)
                - AWS Service
                - VPC Link
        Stage 
            - Versions of your API
            - /dev, /prod, /v1
            - supports caching, throttling, logging, variables
        Deployment
            - a version/snapshot of your API
            - required everytime you update your API
        

    HTTP Components (V2)
        eg)
           +----------------------------------+
           | Route - PUT /hello               |
           |     Authorizaiton  & Integartion |
           +----------------------------------+
        API 
            - container for multiple routes (wraps Routes)
        Routes 
            - represent an endpoint.
            - Combines HTTP method + path
                GET /users
                POST /orders/{id}
        Integration
            - The backend (integration service) your route calls 
                - Lambda function (AWS_PROXY)
                - HTTP
                - AWS Service (limited to specific services)
                    - eg EventBridge, SQS, AppConfig, Kinesis Data Streams, Step Functions
                - VPC Link

        Stage 
            - Versions of your API
            - has a special stage called $default.
            - All changes that you make to your API are autodeployed to that stage.
                - prod, beta, etc. (if you create more)

        Authorizers
            - Controls who can access your routes.
                JWT Authorizer (eg, Cognito or any OpenID Connect provider)
                Lambda Authorizer (custom logic)
                IAM Authorization (SigV4 signing)
        CORS
            Built-in CORS configuration per route


AWS RDS
    - "managed databse"
        - easy to setup, operate, scale
    Features:
        - supports multiple relational DBs, (open source & proprietary)
        - auto backups, patching, monitoring, and failover.
        - Multi-AZ
        - blue/green deployments
        - Performance Insights  (dashboards?)
        - Supports multiple DB engines:
            - Amazon Aurora (MySQL and PostgreSQL compatible)
            - MySQL
            - PostgreSQL
            - MariaDB
            - Oracle
            - Microsoft SQL Server
    Encryption:
        - optional 
            encryption-at-rest 
            encryption-in-transit
        - at rest 
            - uses AWS Key Management Service (KMS).
            - must enable encryption when creating the DB instance—you can't enable it later. (but you can work around it)
            Applies to:
                Database instances
                Automated backups
                Snapshots
                Read replicas   
        - in transit
            - Uses SSL/TLS (certificates)
            - on by default
        
    Backups
        - Can do Automatic or Manual
        - An s3 has backup data
        Automatic:
            - done automatically by AWS
            - FREE
            - retention period (1 to 35 days).
                - disable with "retention = 0"
            - daily snapshots taken
            - you define when snapshot are taken (backup window)
                - **No additonal charge for automated backups**
                PITR = Point In Time Recovery = snapshot
                ^ when automated, they called PITR, when manual, they called snapshot
        Manual
            - can take manual snapshot
            - costs you money 
                - BUT you will be charged for manaul snapshots (storage charge)
                - manual ss, can copy snapshots accross regions
                - manual ss, share to another AWS account
                - Snapshots will exist even when the RDS was delete
            
        - to backup your database, your database must be in the "available" state
        - transaction logs are tracked
        
        ** Restoring a backup creates a new RDS instance, then restores the data to it **
            - slow b/c ^
            - can use CLI
    DB Subnet Group:
        - is a collection of subnets (usually private) in a VPC
            -> where you RDS are deployedl in
            - RDS instances *must* be part of a DB subnet group
        - a DB subnet group should be subnets in 2+ AZ
        - subnets can either public or private
            - to be public, then all subnets must be public
        ---> all it is, is a grup of subnets, the AWS rng picks ones to deploy your instance

    Deployment options:
        1. Single-AZ Deployment
            - single instance
            - no fail over
            - "an isoloated databse enviornment runnin in the cloud"
            - dev/test
            - each database has its own DNS hostname
                https://my-rd-instance-123.owfuiosnak.us-west-1.rds.amazonaws.com/
        2. Multi-AZ Deployment
            - Failover protecton
            - Muti AZ will create your a primary database (DB) and synchronously replicates the data
                to an instance in a different AZ. 
            - When it detects a failure, Amazon RDS automatically fails over to a standby 
                instance without manual intervention.
            - when you have 1-2 standby RDS clusters or instance in another AZ
                - which failover in a AZ becomes unavailable
            
            TWO Multi-AZ options:
                a) One Standby
                    - it auto creates a pimaray database and sync-ly replicate your data 
                        onto a 2nd instance in a 2nd AZ
                    - in case primary breaks, it auto fails over to 2ndary
                    - fail over within 60 sec
                    - the 2ndary doesn't serve read traffic
                    -> isn't a scaling solution 
                b) Two readable Standbys
                    - aka "Multi-AZ DB cluster deployments"
                    - 3 AZs total
                    - the 2 standboys allows you to create read-only and read-write endpoints
                        -> improve perfrmance and scalability
                    -> 2x improve write latency and better writ than 1 standby
                    - fail over within 35 sec
                - both ensure 0 data lose
                
                ?
      Note:
          Read Replicas Deployment
            - read-only copies of your RDS database.
                -> A Read Replica is a copy of your database that you can use to offload read-heavy workloads.
            - Async replication occurs between primary and replicas
                - 5 replicas max for MySql, MarioDB & PostgreSQL
                - 15 for Aurora
            - each replica has its own DNS Endpoint
            
            - optionally, Multi-AZs or Cross-Regions replica 
                - sync to a Replica in another AZ
                OR
                - sync to a Replica in annother Region
            - must have automatic backups enabled
            - replicas can be promoted but this breaks replications

    Note:
        Multi AZ
            - Synchronous replication              
            - Durable                              
            - Only primary instance is active      
            - backups are taken from standby       
            - Always span two AZ within a Region   
            - Happens on primary                   
            - Automatic failover to standby        

        Read Replicas
            - Asynchronous replication
            - Scalable
            - All read replicas active
            - No backups configured by default
            - Can be within an AZ, Cross-AZ, or Cross-Region
            - Independent from source instance
            - Can be manually promoted to standalone instance
            
    DB Instance classes
        - up to 40 RDS DB instances per AWS account
        - "an isoloated databse enviornment runnin in the cloud"
        - all have `db.` in its name
            - dbm3, dbm4, dbx2g, dbz1d, dbr7g,
    Storage
        - Elastic Block Storage (EBS) are being used by DB instances
            - data & log storage
        - 64 TB is the max
        - you can only increase, cannot reduce storage size
            - workaround: create a new DB instance with less provisioned space
    Performance Insights
        - cool UI dashboard
        - shows where the bottlenecks are.
        Helps identify:
            Slow queries
            Wait events (eg locks, I/O, CPU)
            Which SQL statements or users are causing load
            Visualizes DB Load over time, measured in Average Active Sessions (AAS).
        - free for up to 7 days of data retention.
        - $0.20 / GB / Month (vaires by region)
        
    RDS Custom  
        - "customers direclty manage aspsect of RDS instead of AWS"
        - offers greater control and customization over your DB
        - for apps that require custom configurations, third-party software, or specific database patches 
            that standard RDS doesn't allow.
        - offers Root/OS-level access to the database instance (unlike standard RDS).
        Use Cases:
            - Running legacy applications with hard OS/database requirements
            - Using custom backup/restore tools
            - Applying specific patches or security configurations
            
    RDS Proxy
        - instead of apps connecting to your RDS, they connect to the RDS Proxy
        - creates a pool of connections
            - improves performance
        - reuses database connections => reducing overhead and connections limits. 
        - enhances security with IAM authentication
        - Proxy connects to the a Target Gropu
            - Target group = writer + readers (leader + follower)
            - but connects to the Leader
        Use case:
            - Lambda Applications
            - Large Fleet Apps
            - When we dont want to flood the RDS sedrver with 100000000 connections
            
    Optimized Reads & Writes
        - improves I/O efficiency
        Reads: 
            uses local NVMe SSD-based temporary storage 
                - instead of EBS
        Writes: 
            - batching multiple write I/O operations before flushing them to storage. 
            ->  temporarily hods data in memory then writing it to disk storage in batches, 
                - instead of writing every change immediately.
        - write 50% faster
        - read 100% faster

    RDS IAM Authentication 
        - connect using IAM instead of a database username and password. 
        - It provides temporary authentication tokens
        - IAM auth token last 15 min
        - thus have to re-authenticate eveyr 15 min
        
        
    Kerberos Authentication 
        Note:
            Kerberos
                - is a network authentication protocol by Microsoft
                - the auth protocol is **directly integrated into Microsoft's Active Directory**
        - RDS supports Kerberos
        You can use it for:
            - authentication using Active Directory credentials 
            - single sign-on access to certain services using Kerberos tickets instead of password
                - no passwords
        
    Secret Manager Integration
        - use Secret manager instead of passwords
        - instead of hardcoding credentials, apps retrieve them from AWS Secrets Manager. 
            - Username and password still exists, but they are managed by Secreet Manager (ie, not a token thing)
        
        - rotates database credentials 
            - (Secret manager feature, not DBS specific)
            - 7 days by default
            - $0.40 per secret
        
        - Limited support. Does NOT work with
            - Microsoft SQL Server
            - Amazon RDS Blue/Green Deployments
            - Amazon RDS Custom
            - Oracle Data Guard switchover
            - RDS for Oracle with CDB
            
    RDS Master User Account 
        - Master User Account = the admin account created when launching an RDS instance. 
        - When you create a DB instance, you get many privileges for that DB instance.
        - he has full privileges to manage the database 
            - eg, create schemas, users, and manage permissions
        - but does not have full OS-level or superuser access like a native root account in some engines 
            - eg, postgres in PostgreSQL
            
        - it is an account (w/ username and password) visible in AWS Console
        - You can't change the master user name after the DB instance is created.
            - can change password though

            
    RDS Database Activity Stream
        - captures real-time data changes (inserts, updates, deletes) 
            and streams them to Amazon Kinesis Data Streams. 
        - for analytics, replication, and event-driven apps without impacting the source database.
        - "internal and external threats"
        - $$$ the stream is free, but Kinesis is not free
        
        
    Parameter Groups 
        - manages database engine configs/behavior. 
        - is a "container for engine config values"
            eg, memory, cache size, timeouts
        - When you launch an RDS instance, 
            - it's associated with a parameter group, 
            - and the parameters in that group determine how the database operates. 
        - Changes to a parameter group can require a reboot to take effect.
        
    Public Accessibility 
        - a setting that determines whether a database is accessible from the public internet.
        - "Yes" = public IP 
                    - accessible from outside the VPC, provided security group and network ACL rules allow it.
        - "No" = private
        - this matters for when you connect. Are you in the VPC or no?
        
        Fun fact:
            Connection Url String:
                - is a convenient way to connect to a DB
                - a single string with all the parameters
                MySQL Format: 
                    mysql://[hostname]:[port]/[databaseName]?[properties]
            - if youre trying to connect make sure Security Groups allow you
                eg) Port #

    RDS Blue Green Deployments
        - copies a production DB env in a separate synchronized staging env.
        - can enable via CLI
        
    RDS Extended Support
        - run your DB on version past the engine's supported dated
        - allows you run deprecated major engine versions (eg, MySQL, PostgreSQL) 
            beyond their end-of-life. 
            - Yes, the engine itself is deprecated, not AWS un-supporting it
        - AWS will provide critical security and bug fixes for up to three years, giving more time to upgrade. 
            - but after 3 years, AWS will auto update you :(
        - Extra charges money $$
        
        
        
AWS Aurora
    - relational DB cluster
    Combines
        - Speed & availablity of high end DBs
        - with simplicity and cost-effectiveness of open-source DBs
    Two Engines:
        - Aurora MySQL
        - Aurora PostgreSQL
    - 5x better performance is Aurora MySQL than MySQL
    - 3x better performance is Aurora PostreSQL than PostreSQL
    
    - 1/10th the cost of other solutions (compared to others when CLUSTER and full managed)
    
    - Good for predictable workloads!

    NOTE
    - Aurora has "Storage Layer" and "Compute Layer"
        Compute Layer:
            - This is the database instance
            - It’s what you interact with when you connect to Aurora via a DB client like MySQL 
            - One writer instance
            - Optional read replicas (extra read-only compute nodes)
        Storage Layer
            - This is fully managed by AWS. You don’t see it directly, but it’s where your actual data lives 
            - Aurora automatically replicates your data across 3 Availability Zones
            - It makes 6 copies of your data (2 per AZ)
            - It’s self-healing and auto-scalable up to 128 TB
            

    Durability and Fault Tolerance
        - Aurora Backup and Failover are handled automatically
        - Snapshots of data can be shared with other AWS accounts
        - Storage is self-healing, in that data blocks and disks are
            continuously scanned for errors and repaired automatically.

    Availability
        
                AZ 1               AZ 2                   AZ 3                AZ 3
               Master           Read Replica           Read Replica        Read Replica
                 |                  |                       |                  |
                 |                  |                       |                  |
                 V                  V                       V                  |
            [Data] [Data]     [Data] [Data]           [Data] [Data]  <---------+
                
        - deploys in a minimum of 3 AZs 
        - 1 AZ = 2 copies of your data at all times.
            = 6 replicas
        - Even with just one database instance, Aurora stores 6 copies of your data across 3 AZ (2 copies per AZ).
            - This is part of Aurora’s storage layer
            - 6 total copies across 3 Availability Zones (AZs) 
            -> 6 = FIXED. 6 total copies across 10 AZs
            -> independent of the number of database replicas you create.
                - you cant get 20 data coplies if deployed in 10 AZs
            
    Storage Layer
        - Storage is split into 10GB segments, 
            - each segment is replicated six times
        - Storage auto-scales
            - scales in 10GB increments 
            - max 64TB or 128 TB (depending on DB engine version).

    Security
        - encrypt with TLS/SSL certificate
        - Can use KMS keys
        
    Aurora Provsioned
        - default deployment mode 
        - you manually specify CPU, memory, instance type for your database.
        - 1 primary DB (read/write master)
        - 15 Aurora Replicas max  (reader DB instances)
        - CPU and Memory is fixed in Provisioned Mode, (but can scales in Serverless V2 (not Provisioned))
            CPU max: 32 vCPUs 
            Memory max: 244GB.
        - You have to create the Master/primary DB when you create the cluster 
            - can use CLI, specify configs: engine, username/pass, ect
        
    Write & Reader
        Writer Instance (Primary)
            - Handles all write operations (INSERT, UPDATE, DELETE).
            - **Can also handle reads.**
            - Only one writer per cluster.
            - Vertical scaling ONLY
            - $ Costs based on Size & IOPs

        Reader Instances (Replicas)
            - Handle read-only queries (SELECT).
            - Up to 15 readers per cluster.
                - Horizontal scaling
            - Used to scale read traffic and enable failover.
            - Automatically promoted to writer if the primary fails.
            - reader = replica (terminology)
            - $ Costs based on instance count
            
        Tip:
            - Via the CLI, the "create-db-instance" command is the same for both writer & reader
                - The first instance you create is the write
                - The second instance you create is the reader
    
    Aurora Serverles V1
        - is dead        

    Aurora Serverless V2
        - automatically adjusts capacity based on your workload.
        - it scales super fast
        - CANNOT scale to zero
            - 0.5 ACU is minimum
            - 40$ / month
        - $$ You will ALWAYS be charged for storage (storage cant scale to zero)
        - compute can scale up/down. 
            - $$ You're charged for compute
        - after your DB computes stuff you have a "wait period" 
            - After the "wait period" your DB will shutdown (b/c serverless) 
            - Wait peroid= 5min to 24hours
        - once traffic comes in, your DB starts up again
            - NO COLD START b/c 0.5 ACU min
        - ❌ No RDS Proxy

        Fine print if you use, then read docs on:
            - Wake-up Condtion
            - Pause Conditions
        
        - 1 ACU ( Aurora capacity unit)
        - 1 ACU ~= 2GiB of memory, CPU, and networking
        - you set min/max ACUs
            - Max 128 ACUs
            - Scales in 0.5 ACU increments?
            - Scales in seconds.
        - Billed per second per capacity used.
        - Supports Multi-AZ, failover, and replicas.
        - No cold starts
        - V1 is dead.

        Best for: Spiky, unpredictable, or infrequent workloads.
        
    Aurora Global Database 
        - a Aurora database spanning multiple regions for global low-latency and high availability.
        - Has a primary cluster in 1 region 
            - FIVE secondary AWS Regions, max
            - Write operation occur on the Primary cluster
            - Data is replicated to secondary cluster (typically under a second)
        - Global Database is only available in specific regions and specific database versions
            - Primrary cluster only 1 writer (possibly multi readers)
            - secodnary cluster only has readers -> no writer
        - Available for both provisioned and Aurora Serverless v2 
        
        
    Aurora RDS Data API
        - interact with your Aurora database using HTTPS requests, 
        - instead of needing a persistent database connection through a driver like JDBC or ODBC.
        - perform SQL queries over RESTful HTTP calls.
        - good b/c no connection tax
        
        - must be enabled on the cluster
        - DATA API calls are excluded by CloudTrails since they are data events
        - Available for both provisioned and Aurora Serverless v2 
            - has to be turned on
        
        
        
    Note:
        Babelfish (not AWS thing)
            - allows PostgreSQL to understand [Microsoft] SQL Server (T-SQL) commands. 
            - Babelfish runs T-SQL
            - It helps you migrate apps from Microsoft SQL Server to PostgreSQL with minimal changes.
            - is a open source project by Amazon (not Microsoft)
    Babelfish for Aurora PostgreSQL
        - Your Aurora PostgreSQL cluster now can accept DB connections from Microsoft SQL
        - Apps built for ms SQL servers can work directly with Aurora PostgreSQL with few code changes
        - has limited support, eg missing:
            - IAM, Blue/GreenRDS Data API, ect...
            

Document Database
    - aka NoSQL database 
    - aka "document store"
    - data are "documents" 
    - in formats like JSON, BSON, or XML.
    - flexible structure 
    - Document dbs are a subclass of Key/Value stores
    - Comparison
        SQL      NoSQL
        -----------------
        Table = Collection
        Rows = Documents
        Columns = Fields
    Advantages:
        - scale horizontally more easily.
            - Horizontal scaling = adding more servers, not just upgrading one.
        - great for rapid iteration and when the data structure isn’t fixed.
            - In early or fast projects your data model evolves constantly.
            - you don’t need to alter a schema every time you tweak a feature.
            - no migration headaches.
        -  often used for apps thats loosely structured or w/ nested data, like:
            - Product catalogs
            - User profiles: preferences, settings, and activity logs
    Sharding:
        1. Shard Key
            You define a shard key - eg, id.

        2. Shard Logic
            Behind the scenes, the system says:
                id 1-1000 goes to Server A
                id 1001-2000 goes to Server B
                id 2001-3000 goes to Server C

        3. Router/Coordinator
            - a coordinator/router knows the mapping of keys to shards.
            - when your app requests user.id: 2, the router says, "that lives on Server A" and forwards the request.
        Youtube might have this:
            {
              "video_id": "Xyz123",
              "title": "My First Vlog",
              "description": "Watch me eat a pineapple",
              "upload_time": "2024-02-01T14:32:00Z",
              "duration": 600,
              "tags": ["vlog", "pineapple", "funny"]
            }
MongoDB
    - open-source document database that stores data in JSON-like documents
    - uses BSON (Binary JSON)
    Use Cases:
        Content management systems
        IoT applications
        Catalogs and user profiles
    BSON:
        - binary represetnation of JSON-like Objects
        - efficent in storage space & scan-speed compared to JSON
        - Data types: Dattime, btye arrays, regex, MD5 binary data (raw output of MD5 hash functions), javascript code
    - horizontal scales

Amazon DocumentDB
    - is a NoSQL document DB
    - MongoDB compatible
        - but not 100% of mongoDB functionality
    - Fully Managed: Amazon handles backups, patching, scaling, and replication.
    - Can scale read capacity by adding up to 15 read replicas. Data is replicated across multiple Availability Zones (AZs).
    - deployed in a VPC
    
    Deployment
        1. DocumentDB cluster:
            - Instance based
                - One Primary instance (write/read)
                - Zero or more replica instances (read only)
                    - 1 primary, 0-15 replicas
            - each instance has the same distributed storage volume, automatically replicated across 3 AZs
            - you provision EC2 intances
            - 1 writer, 0-15 read replicas
            - manual scaling
            - good for predictable work
        2. Elastic Cluster
            - "serverless-like"
            - you define shard count and CPU/memory per shard
            - auto scales to support 100,000 read/wirts per second
            - virtualy ulniimnted read replicas
        
     
        
KEY-VALUE DATABASE
    - a type of NoSQL database that stores data as a collection of key-value pairs. 
    - each key is unique and is used to retrieve the associated value, 
    - each value is a blob of arbitrary data (string, JSON, binary, etc.). 
    - like a dictionary or hash map.
    Brands:
        Redis
        Amazon DynamoDB
    Compare to Document DB
        - a document DB stores data as documents, typically in JSON or BSON format. 
        - a single document can contain nested structures like arrays and sub-documents. 
        - both use unique key (_id field)
        -> in a document DB, the value itself has structure and can be queried internally.

    key-value Pros:
        1. Blazing Fast Lookups
            -  they don’t need to parse or understand the value
        2. Caching Layer
        3. Anything in the value
            - a key-value database doesn't care what the value is. 
            - stores just bytes. That "value" can be:
                A string
                A JSON object
                A binary blob
                A serialized protobuf
                A JPEG image
            - but at a "cost"
            - Your application has to deserialize it. 
            - If you want to find all sessions expiring soon, the key-value DB won't help you
    Key-Value stores (vs) Document DB
        - key-value stores can technically do almost anything a document database can, 
            if you're willing to handle all the structure and logic in your app layer. 
        But that's the point: 
        - document databases offload a lot of that complexity so you don’t have to write it yourself.
        
    Choosing Document DB over key-value:
        1. You Need to Query by Fields  
            -  if you want to gind users: role = "admin" AND price < 100
            - you cannot filter in a key-value without workarounds
        2. You need flexible data but strucutred data
            Note:
                - In a Relational DB, you define strict schemas.
                - In a key-value store, the value is just a blob.
                - But a document DB is the middle ground:
            - Flexible: You can add/remove fields per document.
            - Semi-Structured: Fields still have names and types.
        3. You can modify data easily in document DB


        
AWS DynamoDB
    - a NoSQL, key-value database
        - like redis
        - keyvalue db  = keystore db
    - optimized for performance at scale
    - "is a fully managed NoSQL database service with single-digit millisecond performance, 
        and is often used for serverless applications."
    - good for "apps with known access patterns"
        - good for; You have a table of data (with rows) BUT it's not relational data
    - no username/pass, instead IAM is used
    - data distributed across multiple partitions
    
    Pricing models = On-Demand mode & Provisioned Mode
    On-demand mode:
        - still a serverless mode
        - pay-per-request pricing for read and write requests
        - you don't specify expected read/write throughput
    Provisioned mode:
        - you specify reads and writes per second 
        - You'll be charged based on the hourly read/write you have provisioned
            - not how much you actually consumed. 
        IN ADDITION
        - you optionally have autoscaling on top of that
        - cost predictability.
        
    Provision capacity:
        - you set a max Read Capacity units & Write Capacity units
        - DynamoDB auto scales
    -you provision instances
        - set x instance
        - has autoscaling but its not ideal, (slow)
        - your data is stored on SSD storage across 3 diff AZs

    On Demand pricing
        - better scaling

    Partition
        - different from Partition Key
        - a unit of storage and throughput
            - you slice your table into smaller chunks
        - you dont see or manage paritions directly
        - dynamoDB uses it to distibute items across multiple servers
            - and scale horizontally
        eg)
            - DynamoDB needs to move id= "1234" → hash → maps to Partition A
            - Another item with id= "5678" as UserId might hash to Partition B
        - Good Partition Key Design = Even Load
            - b/c dynamoDB divides up your data to evenely spread it
        - Dynaomo automatically creates partition for you as your data grows
        - 2 cases where DynamoDO creates new partition:
            - for every 10 GB of data
            - when you exceed the RCU or WCU on a single parition
                - RCU = Read Capacity Units
                - WCU = Write " "
                - you define these settings
    Primary Key
        - Must be unique
        - For some reason, we MUST put our data into a Table.
            When we create a talbe we define a "Primary Key"
        - the PK determines where & how your data will be stored in parititions.
        - another words...
        - the PK is a single attribute, determines the partition ( partition != partition key).
        eg) PK = UserID
            and
           Data loaded to Dynamo = { 
                                      "UserId": "u123", 
                                      "Name": "Alice", 
                                      "Age": 30 
                                    }
        
        - you cannot change what is Partition key is (choose wisely)
        Two ways to choose a PK
            1. Simple Primary Key 
                - using only the Parition key
                    - using only 1 field, (prob the ID)
                - should be unique
            2. Composite key 
                - composite key = Partition Key + Sort Key (Range Key) (always unique like ID)
                - sort key = something like creation data
                - again: combination of partition key and sort key must be unique
                - with a sort key, when 2 records have the same Parition key value, they will be kept together and sorted A-Z
                - PK w/ sort key should be unique

    Eventually & Strongly are both options.
        Eventually Consistent Reads (default)
            - b/c we might write to server A, but read from server B, thus might be out of sync for x seconds
            - outdated replica (by seconds) is called "inconsistent data"
            - fast reads
        Strongly Consistent 
            - guarantee consistency BUT
            - higher latency (slower reads)
            -> Slower
            -> more $ (b/c they have to work more)
    
    Global Tables:
        - ex, table in us-east-1  -> data is replicated into eu-west-1 automaticaly
        - in global talbes, you dont pay for the replicated write, just the 1st inital write

    Query and Scan
        -  Query
            Retrieves items using primary key or indexes.
            Efficient and fast.
        -  Scan
            - combs through every item and returns you the result based on your filter
            - slow and inefficent
            - you can "query" on arbitrary attributes using Scan, but it’s not efficient.
            Recall: 
                - DynamoDB is a key-value store (with some document store capabilities), 
                - yes this is counter to a a regular key-value db
            Caution:
                - can use up all your provisioned throughput in 1 scan
                
            CLI  
                $ aws dynamodb scan \
                     --table-name Thread \
                     --filter-expression "LastPostedBy = :name" \
                     --expression-attribute-values '{":name":{"S":"User A"}}'

Apache Cassandra 
    - is a NoSQL distributed database 
    - designed for: 
        large amounts of data across many servers 
        high availability 
        no single point of failure.
    - not a relational DB
    - not a document DB
    - not a key-value store
    - not OLAP like Redshift/apache Spark
    - yes is a wide-column NoSQL database.
    Wide-Column DB
        - its SQL-like but its not.
        - In a wide-column DB, each row could have different columns
            - Rows can have different columns - there’s no fixed schema.
            - but still stores data in tables
        - Highly efficient for writing and reading large volumes of data across distributed systems.
        - Wide-Column DBs are designed for high scalability and performance, 
            for large volumes of data across many machines.

    Wide-Column & Columnar Store
        - Different!
        - "columnar store" and "wide-column store" are not the same thing.
        - wide-column: 
            Rows stored by row key, and each row contains many columns.
            writen and read BY ROW, not column
            optimized for fast OLTP workloads (insert, look up)
        - columnar store
            stores data BY COLUMNs (not row)
            analytics, designed for -> scanning billions of rows of columns doing aggregation (SUM, AVG, ect)
            - apache Parquet, Amazon Redhift, ClickHouse
            
    - Uses CQL (Cassandra Query Language)
        - like SQL
            
        
Amazon Keyspaces
    - Apache Cassandra in AWS
    
    cluster = collection of nodes
            - whole Cassandara DB system
    nodes = a single machine
            - a DB instance
            - all ndoes are equal (no master-slave setup)
            - hold 2-4 TB of data
    ring = distribution style of data
        - cassandra uses hashes to divide the data across nodes in a ring like structure
        - each node has a portion of the hash range (aka 'token range')
        - allows even distibution and scalabilty
        - k-connect graph
        - data sharing model
    keyspace = like a database in a RDBMS
        - is the top-level container for your data
        - defines replication settings for the data 
        A keyspace holds:
            - 1+ tables
            - settings for those tables, like Replication 

    table = where the data lives
        - tables are flexible schema but define columns primary keys and partitioning keys
        - is stored in a keyspace

    - you can Query Cassandra from the AWS console
    
Graph Database 
    - uses a graph structures to store, map, and query relationships. 
    - Instead of using tables, it uses:
        Nodes - entities (like people, products, or concepts)
        Edges - relationships between entities (like "FRIENDS_WITH" or "PURCHASED")
        Properties - key-value pairs attached to nodes and edges (like a user’s age or the timestamp of a transaction)
    - Unlike RD that use joins to connect data, graph databases can traverse relationships, 
        making them efficient for complex, interconnected data.
    - Flexible schema: You can add new types of nodes or relationships without altering a rigid schema.
    - Intuitive modeling: They often resemble real-world networks like social graphs, 
        recommendation engines, fraud detection systems, etc.
    Popular Graph Databases:
        Neo4j
        Amazon Neptune
        OrientDB
        ArangoDB
        Microsoft Azure Cosmos DB (Gremlin API)
    Common Use Cases:
        Knowledge graphs
        Recommendation engines
        Fraud detection
        Social networking apps
        Life sciences
    - "Find all users two hops away from user A who liked the same posts as user B."

Amazon Neptune
    - a graph databsae
        - fully managed 
    
    Neptune DB
        - built for OLTP (fast transactional queries)
        Provision Deployment
            - you choose instances
        Serverless
            - you set min/max Neptune Capacity Units
        - offers Multi-AZ
        - Storage can be either: "I/O Optimized" (fast but $), or "Standand" (25% speed of I/O)
    Neptune Analytics
        - designed for running analytics
        - built for OLAP
        - does copmlex graph anlytics on large datasets
    Neptun ML
        - integrates ML into graph data workflows, speificually using "graph neural networks" (GNNs)
        - to find patterns and make predictions
            - powered by Deep Graph Library (DGL)
            - LangChain
        predicts:
            - links (will user A follow user B)
            - node properties ( what category is this product)
            - edge lables/weights (how strong is this connection)
        - good for
            recommendations
            fraud detection

    - To interact with it, you can use:
        - SPARQL
        - OpenCypher
        - Gremlin query language
        Gremlin
            - a graph traversal languaged for Apache TinerPop
            - write once run anywhere (WORA)
            - traversals can be evaluated as either real-time querys (OLTP) or batch anlytics query (OLAP)
            - can work in nearly any language (python, js, php, groovy, ect)
        SPARQL 
            is a RDF (Resource Description Framework) query language
        OpenCypher can also query shit
    
    
Amazon Elastic Container Registry (ECR) 
    - a container image registry 
    - "store, manage, and deploy" container images (like Docker images)
    - Private registries 
        - accessible to thos in the AWS Account 
        - and accessible vai (IAM), controll access via Register Polcies and Repo Policy
    - Public registries
        - anyone
    - can scan container images for vulnerabilities using Amazon Inspector or the basic ECR image scanning feature.
    Registry - 1 or many repo(s) repositories
    repo = 1 or many images
    image = conterized app
    tag = specific image version
    
    Image Tag Mutability
        - a feature to prevent image tags from being overwritten
        - when on, ALL tags in a repo cannot be overwritten
            - cannot make some mutable and others not in same repo
    ECR Lifecycle Policy 
        - automatically manage and clean up your container images
        - JSON policy
            - ECR evaluates sequentially (from top to bottom). 
            - rules like:
                Image age (older than X days)
                Tag status (tagged, untagged, or any)
                Tag prefixes ( prod-*, dev-*)
                Image count (keep only the latest 5)
            - Once a rule matches an image, it’s either retained or marked for deletion (depending on your rule).

            
AWS ECS (Elastic Container Service)
    - container orchestration service 
    - run Docker containers on a cluster of VMs. 
    - a simpler alternative to Kubernetes 
    - supports two launch types: 
        Fargate (serverless) 
        EC2 (self-managed instances)
    (Fargate launch is next section)
    - Ec2 launch:
        Auto Scaling Group
        +------------------------------+
        | ECS Cluster                  |
        | +-----------+ +-----------+  |
        | |EC2 Containr | EC2 Cntnr |  |
        | | task 1    | | task 3    |  |
        | | task 2    | | Service 1 |  |
        | | ....      | | ...       |  |
        | +-----------+ +-----------+  |
        +------------------------------+
    
    1. Clusters
        group of resources (EC2 instances or Fargate tasks) to run containers.
    2. Task Definitions
        Blueprint for your application. 
        JSON configs
        Specifies:
            Docker image(s)
            CPU & memory
            Networking mode
            IAM roles
            Environment variables
            Volumes
        Can define one or multiple containers per task.
    3. Tasks
        is a running instance of a task definition.
        can run them manually or as part of a service.
    4. Services
        Manage long-running tasks.
        Ensures taks remain running (web apps)
        Supports:
            Load balancing (via ALB/NLB)
            Auto Scaling
            Blue/Green deployments (with CodeDeploy)
    5. Container Agent
        a thing (binary) that monitors each instance, and starts/stops
        Runs on each EC2 instance (in EC2 launch mode).
        Communicates with ECS to manage containers.
    6. ECS Scheduler / Controller
        schedules and deploys your containers
            - replaces unhealthy containers, ect
        places tasks across your cluster based on constraints and strategies (strats: binpack, random, spread).



AWS Fargate 
    - serverless "compute engine" for containers. 
    - is a serverless orchestration container service
    - run containers without managing the underlying EC2 instances.
    - you define the container specs (CPU, memory, etc.), and Fargate handles provisioning, scaling, and infrastructure management. 
    - It works with ECS and EKS. 
    - you can have an empty ECS cluster (no EC2s or containers) then launch Tasks
    - charged base on duration and consuptions  
        - charged for at least 1 min, then its by the second
    
    Auto Scaling Group
    +------------------------------+
    | ECS Cluster                  |
    | +----------------+           |
    | | Service 1      |           |
    | | task 1         |           |
    | | task 2         |           |
    | | ....           |           |   <------ No EC2 Containers
    | +----------------+           |
    +------------------------------+
    
    Task:
        - define memory and CPU
        - *you'll apply a Security Group to a task*
        - run in a VPC
        - you'll also apply a IAM role to the task
        
    Fargate Execution Role 
        - is an IAM role used to prepare or manage the container
        - used to pull container images and fetch secrets or logs.
        - Used by ECS agent.
        - permissions often include:
            - Pull images from Amazon ECR.
            - Read secrets from AWS Secrets Manager or Parameter Store.
            - Write logs to CloudWatch.

    Fargate Task Role 
        - is an IAM role assumed by the container(s) running inside your ECS task. 
        - It allows your application code to interact with AWS services securely.
        - permissions like:
            S3 (e.g., uploading files)
            DynamoDB
            SQS, SNS, etc.
        note:
            Execution Role = used by AWS/ECS to set up the task (pull image, fetch secrets).
            Task Role = for your app's code, inside the container

    Capacity Providers 
        - define how and where your tasks run. 
        - manage the compute capacity backing your ECS services or standalone tasks.
        - (uses an Auto Scaling Group)
        Types:
            Fargate – Serverless compute.
            Fargate Spot – Cheaper, interruptible version of Fargate.
            or custom EC2 capacity

    ECS Task Lifecycle 
        Main States:
            PROVISIONING
                - ECS is setting up resources (eg networking).
            PENDING
                - Resources are ready; waiting for the container to start.
            ACTIVATING
                - transition after laucnhed but before running
            DEACTIVATING
                - transition before stopped but after running
            RUNNING
                - Container(s) are active and executing.
            STOPPING
                - ECS is cleaning up and stopping the task.
            DEPROVISIONING (Fargate only)
                - Fargate is tearing down infrastructure (e.g., ENIs).
            STOPPED
                - Task has finished or failed. No resources running.
            DELETED
                - This is a transition state when a task stops. 
                - This state is not displayed in the console, but is displayed in describe-tasks.

        - updates visible via CloudWatch Events or ECS APIs.
        
    Task Definition JSON file 
        is a blueprint for running ECS tasks. It defines how containers should run, including settings like:
            family
            containerDefinitions: List of containers with: lable/name, Image, CPU, memory, Ports, env variables, health check, logging, ect
            taskRoleArn
            executionRoleArn
            networkMode
            requiresCompatibilities (FARGATE or EC2)
        Note:
            family = logical group for versions (revisions) of the same task. 
                Each time you push a new version, ECS creates a new version
                    ex json-file: `"family": "my-app-task"`
                ECS creates: my-app-task:1, my-app-task:2, ...
            networkMode
                - Fargate can only use AWSVPC mode
                    - AWSVPC: creates an ENI (elastic network interface) in your VPC with a private IP and full netowrking stack, just like a VM
                    - "bridge" or "host" are not supported on Fargate

    ECS Exec
        - like: $ docker exec
        - you connect into a running container in ECS without needing to ssh in the host, deal with keys, ports ect
        - prequists: 
            aws cli, task role permissions for ssm (system manager), and other things

    ECS Log Configurations 
        - define how container logs are collected and where they’re sent.
        Common Log Drivers:
            awslogs (Sends logs to CloudWatch Logs (most used)).
            fluentd
            gelf
            json-file
            journald
            logentries
            syslo,
            splunk
            awsfirelens
        ex)
            "logConfiguration": {
              "logDriver": "awslogs",
              "options": {
                "awslogs-group": "/ecs/my-app",
                "awslogs-region": "us-east-1",
                "awslogs-stream-prefix": "ecs"
              }

    ECS Service Connect 
        - like a Service Discovery
        - simplifies service-to-service communication 
        - has built-in service discovery, traffic routing, and observability—no 
        - makes it easy to setup a service mesh
        - "need to manage load balancers or DNS manually".
        - uses CloudMap
        - You define services with logical names (like users, orders), 
            and ECS Service Connect handles discovery and communication automatically.
            
    ECS-Optimized AMI 
        - an Amazon Machine Image preconfigured to run ECS tasks on EC2 instances efficiently.
        - Comes with the ECS agent pre-installed.
            - docker installed
        - other stuff

    ECS-Optimized Bottlerocket AMI 
        - a minimal, container-focused OS to run containers on VMs or bare metal
        - no package manager 
        - misses some features like ECS Anwywhere, Service Connect


    ECS Anywhere 
        - run ECS tasks on on-premises servers, not just in AWS.
        - extend ECS to non-AWS envs
        - uses a SSM Agent (at on-premise) for secure communction
        - misses some features like Service load balance, service discovery, ECS capacity, EFS volumns, 
            - windows is supported

AWS EKS (Elastic Kubernetes Service) 
    - run Kubernetes clusters on AWS without managing the control plane infrastructure.
    Control Plane:
        - the part of Kubernetes that manages the cluster. 
        - controls the cluster
        - handles: scheduling, scaling, networking, and overall orchestration of workloads. 
        - Key components include the API server, scheduler, controller manager, and etcd (cluster state database).

    Add-ons 
        - can be installed through the EKS console, CLI, or API.
        - helps integration, security, and lifecycle management within AWS infrastructure.
        - no, these add-ons themselves do not have a standalone UI to click on or directly interact with.
        - Once installed, these add-ons show up as pods in your cluster, usually in the kube-system namespace:
            $ kubectl get pods -n kube-system   
            output:
            aws-node-xxxxx          Running // ✅ aws-node-xxxxx = VPC CNI add-on
            coredns-xxxxx           Running
            kube-proxy-xxxxx        Running

        Common AWS EKS addons:
            - VPC CNI 
                – Manages pod networking.
            - CoreDNS 
                – Handles internal DNS resolution.
            - Kube-proxy 
                – Maintains network rules for services.
            - Amazon EBS CSI Driver 
                – Enables dynamic volume provisioning with EBS.
            - Amazon EFS CSI Driver 
                – Supports mounting EFS volumes to pods.
            - AWS Load Balancer Controller 
                – Manages ALB/NLB resources for Kubernetes services.
        Third party addons like:
            dynatrace, datadog, HA Proxy, Kong, New Relic, Splunk, NetApp, ect...

    EKS Connector 
        - you manage external Kubernetes clusters (on-prem or other clouds) inside the AWS EKS Console. 
        - It provides visibility but not full control 
            - you can view workloads, namespaces, and cluster health, 
            - can NOT deploy, update, ect from AWS
        ** Bring your own K8 cluster to EKS**
        
    EKS CTL
        - easy to create/delete clusters, install, ect
        - eksctl is a CLI tool for managing Amazon EKS clusters
        - uses YAML or CLI commands

    EKS Distro (EKS-D) 
        - is the open-source distribution of Kubernetes used by Amazon EKS. 
        - It provides the same Kubernetes components EKS uses, with security patches and updates.
        - You can run EKS-D on your own infrastructure (on-prem or cloud) 
        - for hybrid or custom environments needing EKS but without using AWS-managed control planes.

    
    EKS Anywhere 
        - deploy/manage EKS clusters on your own infrastructure (VMware, bare metal, AWS Snowball Edge, Apache CloudStack, and Nutanix.)
        - uses EKS Distro under the hood 
        - AWS Managment Console, you can view & manage your cluster after deployed here.

Traces & Spans
    - tracking and visualization of a request as it traverses multiple services
    - is a method to observe and troubleshoot requests as they flow through various services.
    Trace:
        - represents the entire lifecycle of a single request as it travels through your system.
        - includes all the services that the request touches.
        - as a trace ID
        - Trace = 1 or more spans
        -> Think of it as the story of one request.
    Span:
        - is a unit of work within a Trace.
        - Each Span has:
            A start time and end time
            A name ("HTTP GET /users")
            A unique Span ID
            A parent ID (if it’s part of a bigger operation)
            Metadata/tags (like status codes, DB queries, etc.)

OpenTelemetry 
    - an open-source framework for collecting, processing, and exporting telemetry data
        - traces, metrics, and logs from applications 
        - for distributed systems
    - It standardizes observability across services.
    - has APIs, and SDKs for you
    - Before OTel, companies used lots of fragmented tools 
        OpenTelemetry unifies how data is gathered and sent, 
        regardless of which backend you use (Datadog, New Relic, Prometheus, Jaeger, etc.).
        
    Instrumentation 
        - means adding code or libraries to capture telemetry data (traces, metrics, logs) from your application.
        - to capture what your app is doing so it can be observed.
        It can be:
            Automatic: via SDKs or agents that hook into frameworks.
            Manual: using OpenTelemetry APIs to explicitly create spans, metrics, etc.



    Collector
        The Open Telemetry collector is:
        - is an agent installed on the machine (or as a dedicated server)
        - a vendor-neutral service that receives, processes, and exports telemetry data (traces, metrics, logs) from apps.
        - centralizes telemetry handling
        - reduces overhead on apps.

        Key roles:
            Receiver: accepts data from apps.
            Processor: transforms or batches data.
            Exporter: sends data to backends (e.g., Prometheus, Jaeger, etc.).

    AWS Distro for OpenTelemetry (ADOT) 
        - AWS's production-ready version of OpenTelemetry,
        - vendor-neutral, open-source 
        - for collecting telemetry data from apps running on AWS or on-premises
            - EC2, ECS, Fargate, EKS, App Runner, Lambda,
        - Send telemetry data to (or from) observabilty backens
            - AWS Managed Service for Prometheus (AMP)
            - Amazon Cloudwatch
            - AWs X-ray
            - more
        - Supports multiple languages (Java, Python, .NET, etc.).
        - Integrates with AWS services like CloudWatch, X-Ray, and OpenSearch.


NOTE
Time-Series database (TSDB) 
    - a database optimized for storing and querying data indexed by time. 
        Each data point:
            A timestamp
            A metric name
            Labels/tags (optional, for metadata)
            A value
    - designed for high write throughput, efficient storage, and fast queries
    - Examples include Prometheus, InfluxDB, and TimescaleDB.


Prometheus 
    - an open-source monitoring and alerting toolkit 
    - designed for distributed systems. 
    - collects metrics as time series data
    - It "scraps" metrics at set intervals
    - then stores them in its "time-series database".
        - query database with PromQL (Prometheus Query Language)
   ** Time-series database **
    - YES prometheus is both 
        -> a time-series database AND
        -> a agent/data-scraper 
            - It actively pulls metrics from endpoints 
            - you'll install "exports" like the Prometheus-compatible/agent, named `node_exporter`
            
            node_exporter:
                - A lightweight binary you run on the VM.
                - It exposes metrics about the host system (CPU, memory, disk, network, etc.).
                - It serves these metrics at an HTTP endpoint, eg http://<vm-ip>:9100/metrics


    - is widely used in cloud-native environments, often alongside Kubernetes.
    Key features:
        - Multi-dimensional data model with time series identified by metric name and key/value labels
        - Flexible query language (PromQL) for extracting and analyzing metrics
        - Pull-based model via HTTP, simplifying service discovery and metrics collection
        - Built-in alerting via Alertmanager
        - No external dependencies, making it easy to deploy

Amazon Managed Service for Prometheus (AMP) 
    +---------------------+                +-------------+
    | Grafana cloud agent |                |             |
    | Prometheus server   |--- Ingest  --->|   AMP       |
    | OpenTelmetry        |    metrics     |             |
    | Item 4              |                +-------------+
    +---------------------+

    - runs Prometheus at scale. 
    - "is a Prom. compatible monitoring service for container infra and app metrics"
    - It automatically handles setup, scaling, and maintenance. You no infrastructure 
    - can handle millions of active metrics
    - doenst auto create dashboards
    How It Works:
        1. Ingest Metrics: Applications or services (like EKS, ECS, EC2) push metrics to AMP using Prometheus remote write.
        2. Store Metrics: AMP stores them in a time-series database.
        3. Query Metrics: You use Amazon Managed Grafana or Prometheus-compatible tools to query metrics with PromQL.

Grafana 
    - an analytics and visualization platform used to monitor metrics
    - metrics from data sources like Prometheus, InfluxDB, and Elasticsearch. 
    - often used with timeseries databases
    - provides interactive dashboards, alerts, and real-time data visualization.
    - is an open-source
    
Amazon Managed Grafana (AMG)
    - manages/scales/ect Grafana 
    - dashboards and infrastructure. 
    - It integrates with AWS data sources and services, 
    -> eliminates the need for self-hosting Grafana.



Apache Kafka 
    - like Kinesis (but open source, not serverless, ect)
    - "Distributed Messaging System"
    - for real-time data pipelines and streaming applications.
    - a message broker in a pub-sub system (like RabbitMQ)
        - Producer / Consumer / Topic
        - Kafka decouples producers and consumers
    - not for streaming movies, music, or game graphics to end users
    - uses Scala and Java
    - open-source, by LinkedIn
    Use Case:
        - Millions of events per second
            - IoT data, clickstream logs, telemetry, payments, orders
        - Decoupled microservices
        - Fraud detection, anomaly detection, monitoring
        - Immutable log of what happened in your system
    ✅ Kafka is the broker - not the producer nor the consumer.
    - comes with piles of scrips:
        `/bin/kafka-console-consumer.sh`
        `/bin/kafka-console-producer.sh`
        `/bin/kafka-delete-records.sh`
        `/bin/kafka-topics.sh`
        `/bin/kafka-server-start.sh`
        `/bin/kafka-server-stop.sh`
        ect...
    - uses Zookeepr
    
 
Apache ZooKeeper 
    - a centralized service for distributed systems. 
    - exposes common services like naming, config managment, synchroinizaiton, ect
        in a simple interface, so you dont writ from scratch
    - It's commonly used to coordinate and manage distributed applications.
    - It manages config data, helps services find each other, and makes sure things stay in sync 
        when multiple machines are involved. 
    In distributed systems, you often have:
        - Multiple servers
        - Running the same application
        - Trying to stay in sync
    -> But... these servers can crash,
         You need a reliable way to coordinate things like:
            Who is in charge? (leader election)
            What configuration should we all use?
            Has something changed that everyone needs to know about?
            
    - ZooKeeper is for coordination inside distributed apps (like leader election, config sharing, locking).
    - Kubernetes is for orchestrating and managing containerized applications (deploying, scaling, networking).
    
    - ZooKeeper is used for distributed cloud applications
    - Open source
    Used by:
        - Apache Hadoop
        - Apache Kafka
        - Apache Solr
        - Apache Hbase
        - Apache Accumulo
        ...
        
        
        
Amazon MSK (Managed Streaming for Apache Kafka) 
    - Apache Kafka on AWS.
    - Full managed:
        - provisioning, configuration, patching, and maintenance, availablity
    - needs your VPC
    - integration with S3 and EventBridge Pipes

    Bootstrap Brokers
        - "brokers that Kafka can use as a starting point when connecting to the cluster"
    
    MSK Connect 
        - easly for developers to stream data to and from their clusters
        - can connect to external systems (DBs)
     


AWS KMS (Key Managed Service)
    - create, control, rotate cryptographic keys.
    - encrypt/decrypt data or other keys (envelope encryption)
    - is a multi-tenant Harware Security Module (HSM), but in the cloud
        HSM (Hardware Security Module):
            - a physcal device designed to store crypto keys
            - used by high security. (Banks, governments, ect)
            - hardware designed to store encryption keys.
                - stored in memory 
                - never on disk
                - if power goes out, goodbye keys (security feature)
            - HSM server = expensive $$$
            - 
        Multi-Tenant:
            - data is isolated even though many customers share the same infrastrucutre
            - multiple customers are using the same peice of hardware
            - virtually isolated environments
            - if Single-Tenant then that means you're using the ENTIRE piece of hardware. (dedicated)
    Features
        - rotate keys automatically
        - IAM access control
        - log all key usage via CloudTrail
        - Compliance: FIPS 140-2
        - replicate keys across region for disaster recovery
        - Often, you just click a checkbox to turn on Encryption
    - Integrates with many services (like everything)
        - RDS, S3, Galacier, SNS, SQS, DynamoDB, EC2, X-Ray, ElastiCache, CodeBuild, CodeDeploy, CodeCommit,

    Note:
        CloudHSM
            - is single-tenant HSM
                - single customer, dedicated HSM
            - full control, you have
            - LEVEL 3 FIPs 140-2 if dedicated/full-control
        KMS 
            - IS MULTI-TENANT
                - multi customers on one HSM, but virtually isolated
            - LEVEL 2 FIPS 140-2 if shared/KMS
        cyprotgraphic keys = data keys 
            -> the string of 1s and 0s, the data and contents of a key
    CloudHSM vs KMS
    
        Management:
            CloudHSM: You manage the HSMs. AWS handles hardware, but you control users, key lifecycle, backups.
            KMS: Fully managed by AWS.
        Tenancy:
            CloudHSM: Single-tenant
            KMS: Multi-tenant
        Customization:
            CloudHSM:  Full control. You can custom cryptographic operations, protocols, or PKI setups.
            KMS: Limited to what AWS provides. No custom crypto, no raw access to keys.

        Integration with AWS Services:
            CloudHSM: No native integration. You have to build the logic yourself (unless using as a custom key store for KMS).
            KMS: Built-in for S3, EBS, RDS, Lambda, and more. Just check a box.

        Best for:
            CloudHSM: High-security and regulation.
            KMS: easy

    KMS Customer Master Key (CMK) 
        - A key you manage in AWS KMS.
        - stored in secure hardware
        - a key that encrypts other keys
        - protects other keys (typically data keys) via envelope encryption.
        - called a "Logical Key"
        - you interact with via APIs but you dont see or handle the contents of the key (the 1s and 0s used to encrypt)
        
        - AWS Stores it securely, typically in HSM
        - You see metadata about the key
        metadata:
            - key ID, policy, creation date, description
        - AWS doesnt encrypt your data with this CMK
            - AWS generates another key (symmetic) and encrypts yoru data with that, then AWS-KMS uses the CMK to encrypt that key
                -> envelope

    - Customer Master Key (CMK) !=  Customer Managed Key
        - A Customer Managed Key is one that you create and manage. 
        - You set the permissions, enable rotation, control deletion, etc.
        - All customer-managed keys are CMKs, but not all CMKs are customer-managed.

AWS Compute Optimizer 
    - gives recommendations to optimize the performance and cost of your AWS resources.
    recommends for:
        - EC2 instances
        - Auto Scaling groups
        - EBS volumes
        - Lambda functions
        - ECS services on Fargate. 
    - uses machine learning to analyze usage patterns and suggest the best resource configurations.
    - collects resource data from AWS services (ie, EC2 CloudWatch metrics)
    - straight forward, easy
        
        
AWS Audit Manager 
    - continuously audit your AWS usage 
    - watches over your AWS account and automatically gathers proof that 
        you're following specific rules (like HIPAA, GDPR, or SOC 2).
    - It collects evidence and maps it to control frameworks like GDPR, HIPAA, or PCI DSS.
    Features:
        "Automated Evidence Collection:"
        - "Prebuilt Frameworks": Offers built-in compliance frameworks (SOC 2, ISO 27001, HIPAA, PCI DSS)
    Use Cases:
        - Preparing for third-party audits.
        - Proving compliance to customers or regulators.
        - Reducing manual evidence collection effort and errors.
        
    Control Libraries 
        - A set of "controls" grouped together
        - a collection of indiviauals controls for specific security compliance, or operational requirements
        - controls define requirments for a specific standard
    
    
AWS Certificate Manager (ACM) 
    - SSL/TLS certificates 
    - It handles certificate renewal and simplifies HTTPS setup.
    - "provisioning, management, and deployment of certs"
    - public certificates
        - provided by ACM (free)
    - private certificates 
        - certificates you import
    - handle multiple subdomains and wildcards
    - **ACM can be attached to:** <-- 
        - Elastic Load Balacner
        - CloudFront    
        - API Gateway
        - (Elastic Beanstalk through ELB)
    
    
    
Amazon Cognito 
    - helps developers add user sign-up, sign-in, and access control to applications. 
        - Federated, CIAM, MFA, password, passwordless, ect
    - built to scale to millions.
    - 
    - Manage user directories using Cognito User Pools.
    - Supports MFA, Federate Identites, and fine-grained access control.
    - good at securing apis
    - good at temp access to your aws resorcesf
    - can do stuff with sign on trigger
    - integrates with other AWS services.
    Identity Pools
        - short term access
    User Pool 
        - authentication
        - you find this UI in Console
        - a user directory that manages user sign-up and sign-in. 
        - does authentication; verifying the identity of users.
        - User registration
        - User login
        - Password policies, account recovery, email/phone verification
        - MFA
    Identity Pool 
        - authorization
        - Grants temporary AWS IAM credentials using STS (Security Token Service).
    1. User signs in via User Pool.
    2. You get a JWT token.
    3. That token is passed to the Identity Pool, which:
        Validates it
        Maps it to an IAM role
        Issues temporary AWS credentials
    4. User uses those credentials to access AWS services securely.
    
    Amazon Cognito Sync 
        - a legacy service, replaced by AWS AppSync and Amazon Cognito Identity Pools
        - sync user data across devices. 
        - for mobile apps to keep user data, like: preferences, game state, or settings in sync between devices.
        
Amazon Detective 
    - a security service 
    - investigate potential security issues in your AWS resources. 
    - "quickly idenity root cuase of security findings or sus activity"
    - It collects data and creates visualizations
        - AWS CloudTrail, Amazon VPC Flow Logs, and Amazon GuardDuty 
    - "collects log data from your AWS resources and uses machine learning (ML), statistical analysis, 
        and graph theory to build a dataset that you can use to conduct more efficient security investigations."
    
    
Directory Service 
    - a system that keeps track of users, devices, and permissions in a network. 
    - It helps manage who can access what. 
    - "maps the names of network resourrecs to their network address"
    - "is shared information infrastructure for locating, manaing, and administering resources"
    - resources like: volumnes, folders, files, printers, users, groups, devices, telphone numbers.
    Key Features:
        Hierarchical structure
            - tree-like (folders and subfolders).
        Centralized management
        Authentication and authorization
    - Examples
        - DNS (Domain Name Service)
        - Microsoft Active Directory
        - Apache Directory Service
    
    
Active Directory
    - Microsoft thing
    - orgs can manage on-premise infrastructure using a single identity per user
    - (Forest, Domains, Trees)
    
LDAP (Lightweight Directory Access Protocol) 
    - a protocol (vendor neutral, industry standard)
    - accessing "directory services"
        - directory = a DB optimized for reading & searching, rather than writing
    - manage directory services over a network. 
    - authentication and authorization
    - query user credentials data from a central directory service like Microsoft Active Directory or OpenLDAP.
    - good for SSO
    
AWS Directory Service
    - is Microsoft Active Directory (AD)
    - 3 options:
    1. AWS Managed Microsoft AD: 
        - Fully managed Microsoft AD in the cloud.
    2. AD Connector: 
        - A proxy to your on-premises AD.
        - For enterprises wanting to use existing AD credentials in AWS without replicating data.
    3. Simple AD: 
        - Lightweight, Samba-based directory.
        - often small-scale or dev
    4. Amazon Cognito
        - Integrate sign up/in into web app

    
Hardening
    - the act of eliminating as many security risks possible
    - common for VMs where you run a collection of security checks
    
AWS Inspector
    - runs security benchmarks against EC2s and ECRs
    - scans for vulnerabilities 
    - CIS check is popular
    
Amazon Macie
    - monitors S3 data access
    - data security and privacy service. 
    - find sensitive data, such as PII - stored in Amazon S3.
        - passwords, api-keys, oauth tokens, credit card #, bank #, Helaht Info, social security #, passport ID, Drivers 
    - Detects anomalies, such as unencrypted data or publicly accessible buckets with PII
    - Dashboards 
    - uses ML
    
AWS Security Hub 
    - gives you a "security score"
        - cool UI in Console
    - is a "Cloud Security Posture Management" (CSPM) 
        -> tools that help you continuously assess and improve the security
            - Detect misconfigurations
            - Aggregate security findings 
            - Check compliance
    - gathers security findings into one place from your AWS accounts and services (like GuardDuty, Inspector, etc.) 
    - It helps you see what’s wrong, prioritize issues, and stay compliant with best practices. 
    - Think of it as a central dashboard for your cloud security.
    
    
    
    
AWS Secrets Manager
    - stores secrets (enviornment variables)
    - auto rotates secrets 
    - securely stores, manages, and retrieves secrets like API keys, passwords, and database credentials.
        - stores credentials
    - often used to store and rotate DB credentials
    - enforces encryption-at-rest via KMS
    - $0.40 / secret / month
    - 0.05 per 10,000 API calls
    
    $ aws secretsmanager describe-secret --secret-id enterprise/ShipDatabase
        - It retrieves metadata (not the secret value) about the secret named enterprise/ShipDatabase
        - output = json with lots metadata.: ARN, Name of secret, descriptions, DmsKeyId, isRotationEnabled, LastRoaatedDat, Tags, CreationDate

    $ aws secretsmanager get-secret-value --secret-id enterprise/ShipDatabase --version-stage AWSCURRENT
         - Retrieves the contents of the encrypted fields  (enterprise/ShipDatabas @ AWSCURRENT stage)
         - ouput = json with some metadata + value, eg `"SecretString": "{\"username\":\"admin\",\"password\":\"P@ssw0rd123\"}",`

Amazon Q
    - chatbot on aws
    - annoying shit popup
    - Amazon Q is not something you embed on your public website like a typical chatbot widge
    Where You Can Use Amazon Q:
        - AWS Console
    Amazon Q Business 
        - Enterprise AI assistant
        -  Microsoft 365, Google Drive, Slack, Salesforce, Jira, etc.
        - Uses RAG (retrieval-augmented generation) to ground responses in your company’s internal documents.
    Amazon Q Developer
        - AI coding
        - IDEs (VS Code or CLoud9)
    Amazon Q in Amazon QuickSight
        - AI-powered data insights for business intelligence (BI)
        - Auto-generates visualization
    Amazon Q in Amazon Connect
        - phone calls
    Amazon Q in AWS Supply Chain
        - "get answeresa bout what's happening in your supply chain"
    
Amazon CodeWhisper
    - integrate with your IDEs
    - Focused just on code generation
    - Auto-complete
    - can security scan
    - Individual & Pro versions
    NOTE
        - Amazon Q Developer you can prompt in natural language
        - CodeWhisper is not a chatbot, Not conversational
    
AWS Shield
    - is DDos protection
        - just that
        
        Internet -> R53        -> Shield -> Server A
        Internet -> CloudFront -> Shield -> Server B
        
    - protect you from Layer 3,4, and 7
    - if you dont have WAF then you wont get Shield on layer 7
    - is managed = service is automatically handled by the provider
    - free tier & paid tier
    Free
        - protect agaisnt most common ddos attacks
        - auto available on all AWS services
    Advanced (3,000 / year)
        - additional protection agasint larger sophisicated attacks
        Available on:
            R53, CloudFront, Elastic Load Balancing, AWS Globabl Accelerator, elastic IP
    - integrates with WAF

WAF (Web Application Firewall) 

AWS WAF (Web Application Firewall):
    - firewall for web apps from common threats ( SQL injection, XSS).
    
    - protects against attacks like SQL injection, cross-site scripting (XSS), and other OWASP Top 10 threats
    - a security system that watches HTTP/S traffic to and from a web apps 
    - Application, Layer 7
    
    Key Components:
        - Rules 
            - Define match conditions (eg, IP address, geo location, string match, SQLi, XSS).
        - Rule action
            - Each rule can be set to:
            Allow – let the request through.
            Block – deny the request.
            Count – log the request for monitoring without taking action.
        - Web ACL (Access Control List)
            - a set of Rules
                -> applied to a resource like CloudFront, ALB, or API Gateway.
            - like a firewall policy for your AWS resources. 
            - where you organize and apply rules (like allow/block/count) 
    - WAF can be attached to:
        CloudFront, Application Load Balancers (ALB), or API Gateway endpoints
    - Protects from OWASP Top 10
    
AWS Firewall Manager:
    - "central WAF"
    - centralizes and automates the deployment of AWS WAF rules, AWS Shield Advanced, ect
        -  across multiple accounts
    - Operates at the organization level VS single instance of WAF, single account

        
        
Amazon GuardDuty
    - threat detection service
    - IDS / IPS
        - Intrusion Detestion System 
        - Intrusion Protection System
    - a device or app thas monitors sus activity
        - continuously
        - malicous
        - sus
        - unauthorized
    - eg, RootUser doing stuff
    - uses machine leanring, anomly detection
    - Reads logs:
        - Analyzes CloudTrail Logs
        - VPC Flow Logs
        - DNS logs
    - cool UI in Console
        - list of threats
        eg) 76.72.169.18 is performing SSH brute force attacks against i-04fae5b8df570e6ce.
        
Service Health Dashboard
    - Shows the general status of AWS services
        - NA, South A, EU, Asia, ...
        - Alex for Business, API Gateway (Montreal), API Gateway (N. Cali) ect
        eg) NA: ✅ Alexa for Business (N. Virginia). Service is operating normally
        
AWS Personal Health Dashboard
        - alerts and guidance for AWS events that might affect your environment
        - proactive notifications
        
AWS Artifact 
    - a catalog of "AWS compliance reports"
        - self-serve portal, on-demand
        -> you go there and click on stuff
    - provides on-demand access to AWS compliance reports, such as ISO, SOC, and PCI documents. 
    - helps customers demonstrate compliance for audits and assessments.
    
    
    
AWS Storage Gateway 
    - a hybrid cloud storage service 
    - connects on-premises environments with AWS cloud storage. 
    - provides access to virtually unlimited cloud storage for backup, archiving, and disaster recovery. 
    Three types:

        1 File Gateway 
            - Shared File Storage
            – for storing and retrieving files as objects in Amazon S3.
            - in s3
            - has many S3 storage options 
            - Acts like a network file share (think shared drive).
            - protocols: Network File System (NFS) or SMB mount point
            FSx
                - is rip. October 28, 2024 ☠ 
                - for Windows File Servers (WFS)
                    -> new customers can't create a new FSx File Gateway
        2 Volume Gateway 
            - Block Storage
            – for block storage with cloud-backed volumes.
            - data stored in AWS and cached locally.
            - You store blocks, raw data like a disk, not files directly.
            - mount via iSCSI protocol
                (Internet Small Computer Systems Interface)
                How to:
                    1. Connect to the AWS Volume data via iSCI
                        -> You run AWS Storage Gateway on-prem (as a VM or hardware appliance).
                        -> the server is connecting via iSCI, over LAN (ie not over the internet)
                    2. Read the block storage via file system protocal like: XFS, ext4, NTFS, ZFS, ect
            - Two modes
                A. Stored Volumes
                    - store data locally on-premises AND backs it up to AWS
                    - then asynchronously backs it up in AWS
                B. Cached Volumes
                    - offload data to AWS but keeping frequently accessed data locally.
                    - All data goes to S3,
                        -> frequently accessed data blocks are pulled down to your local storage (to cache)
                    - frequently accessed data is cached locally, on-premises.
                    - and full data is stored in S3 
                    - AWS does magic alogrithm that determines which data to cache
                    Example) 
                        -> app or server accesses a volume:
                        -> The Storage Gateway serves data from the local cache if available.
                        -> If not, fetches FROM 3
                        -> When cache gets full, least-used blocks are automatically evicted... auto prune
        3 Tape Gateway 
            – for virtual tape backups to S3 and Glacier. (Emulates a physical tape library)
            - a compatibility bridge, not a literal tape replacement.
            - for legacy systems
        
    -Backup:
        File Gateway:
            - You can point your backup software (like Veeam, Commvault, etc.) to a File Gateway share.
            - It writes backup files (e.g., .bak, .zip, etc.) to the share.
            - Gateway stores them in Amazon S3.
        Volume Gateway
            -Instead of saving files, you back up entire virtual disks
            -Storage Gateway can take snapshots of volumes automatically or on-demand.
            -Those snapshots go to S3 as EBS snapshots.
    
    
    
Dedicate instance:
    - to meet regualtory requirements
    - or server bound licensing is very strict, and wont support multi-tenancy
    Multi-tenant
        - many people on same hardware, but virtual isolation
    Single-tenant
        - 1 customer on dedicated hardware
        - one person physically isolated on hardware
    
    
see line 888 Reservered isntances in AWS-cert-1
    
    
